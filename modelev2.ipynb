{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f6982f",
   "metadata": {},
   "source": [
    "## 1. Imports & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d28e686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T18:23:31.308886Z",
     "start_time": "2025-05-18T18:23:31.268908Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- CELLULE 1 : IMPORTS ET CONFIGURATION ---\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Pour éviter les erreurs tkinter sous Jupyter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, when, desc, avg, min, udf, percent_rank, countDistinct, stddev, lit, create_map, coalesce, expr, collect_list\n",
    ")\n",
    "from pyspark.sql.types import (StringType, FloatType, LongType, DoubleType, TimestampType, StructField, StructType)\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, StandardScaler, StringIndexer, Bucketizer, Tokenizer, HashingTF, IDF\n",
    ")\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator, RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"modelisation.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b32ea8",
   "metadata": {},
   "source": [
    "## 2. Variables globales & création des dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6634e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELLULE 2 : VARIABLES GLOBALES ET DOSSIERS ---\n",
    "DATA_DIR = \"./data/processed/parquet\"\n",
    "MODELS_DIR = \"./models\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "CHECKPOINTS_DIR = \"./data/checkpoints\"\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512a519",
   "metadata": {},
   "source": [
    "## 3. Classe utilitaire MemoryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853bdf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELLULE 3 : CLASSE MEMORYMANAGER ---\n",
    "class MemoryManager:\n",
    "    \"\"\"Gestionnaire de mémoire pour les DataFrames Spark\"\"\"\n",
    "    def __init__(self):\n",
    "        self.cached_dfs = []\n",
    "    def cache_df(self, df, name=\"unnamed\"):\n",
    "        df.cache()\n",
    "        self.cached_dfs.append((df, name))\n",
    "        logger.info(f\"DataFrame '{name}' mis en cache\")\n",
    "        return df\n",
    "    def unpersist_all(self):\n",
    "        for df, name in self.cached_dfs:\n",
    "            try:\n",
    "                df.unpersist()\n",
    "                logger.info(f\"DataFrame '{name}' libéré du cache\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Erreur lors de la libération de '{name}': {e}\")\n",
    "        self.cached_dfs.clear()\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.unpersist_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c72354",
   "metadata": {},
   "source": [
    "## 4. Fonctions Spark & prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54993088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELLULE 4 : FONCTIONS SPARK & PRÉTRAITEMENT ---\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Enhanced E-commerce Analytics\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"1g\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"24\") \\\n",
    "        .config(\"spark.default.parallelism\", \"8\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "        .config(\"spark.python.worker.reuse\", \"true\") \\\n",
    "        .config(\"spark.python.worker.memory\", \"2g\") \\\n",
    "        .config(\"spark.network.timeout\", \"800s\") \\\n",
    "        .config(\"spark.locality.wait\", \"10s\") \\\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\") \\\n",
    "        .config(\"spark.sql.streaming.checkpointLocation\", CHECKPOINTS_DIR) \\\n",
    "        .config(\"spark.sql.streaming.stateStore.providerClass\", \n",
    "                \"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\") \\\n",
    "        .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "        .config(\"spark.sql.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "        .master(\"local[8]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def find_latest_file(directory, pattern):\n",
    "    files = [f for f in os.listdir(directory) if pattern in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    latest_file = None\n",
    "    latest_time = 0\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        file_time = os.path.getmtime(file_path)\n",
    "        if file_time > latest_time:\n",
    "            latest_time = file_time\n",
    "            latest_file = file\n",
    "    return latest_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143ccc2",
   "metadata": {},
   "source": [
    "## 5. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb6f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_file(directory, pattern):\n",
    "    \"\"\"Trouve le fichier le plus récent dans le répertoire correspondant au pattern\"\"\"\n",
    "    files = [f for f in os.listdir(directory) if pattern in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    \n",
    "    latest_file = None\n",
    "    latest_time = 0\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        file_time = os.path.getmtime(file_path)\n",
    "        if file_time > latest_time:\n",
    "            latest_time = file_time\n",
    "            latest_file = file\n",
    "            \n",
    "    return latest_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865c834",
   "metadata": {},
   "source": [
    "## 6. Segmentation RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d26935f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rfm_segmentation(user_df):\n",
    "    \"\"\"Prépare les données pour la segmentation RFM avec améliorations\"\"\"\n",
    "    logger.info(\"Préparation des données pour segmentation RFM\")\n",
    "    \n",
    "    # Filtrer les utilisateurs avec au moins une interaction et mettre en cache\n",
    "    rfm_df = user_df.filter(col(\"nb_events\") > 0).cache()\n",
    "    \n",
    "    # Convertir les valeurs manquantes/nulles en 0 pour les métriques RFM\n",
    "    rfm_df = rfm_df.fillna({\n",
    "        \"recency\": 30,  # Valeur max si jamais vu\n",
    "        \"frequency\": 0,  # Pas d'achats\n",
    "        \"monetary\": 0    # Pas de dépenses\n",
    "    })\n",
    "    \n",
    "    # Créer des buckets pour les métriques RFM - version simplifiée pour éviter les erreurs\n",
    "    # Utilisation de quantiles fixes pour plus de stabilité\n",
    "    \n",
    "    # Récence (inversée: plus petit = meilleur)\n",
    "    recency_splits = [0, 10, 20, 30, float('inf')]\n",
    "    \n",
    "    # Fréquence et Monétaire - calcul dynamique sécurisé\n",
    "    freq_stats = rfm_df.select(\n",
    "        avg(\"frequency\").alias(\"avg_freq\"),\n",
    "        stddev(\"frequency\").alias(\"std_freq\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    # Si pas de variance, utiliser des splits fixes\n",
    "    if freq_stats[\"std_freq\"] is None or freq_stats[\"std_freq\"] == 0:\n",
    "        frequency_splits = [0, 0.5, 1.5, 2.5, float('inf')]\n",
    "    else:\n",
    "        avg_freq = freq_stats[\"avg_freq\"] or 0\n",
    "        frequency_splits = [0, avg_freq * 0.5, avg_freq, avg_freq * 1.5, float('inf')]\n",
    "    \n",
    "    # Même logique pour monetary\n",
    "    monetary_stats = rfm_df.select(\n",
    "        avg(\"monetary\").alias(\"avg_monetary\"),\n",
    "        stddev(\"monetary\").alias(\"std_monetary\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    if monetary_stats[\"std_monetary\"] is None or monetary_stats[\"std_monetary\"] == 0:\n",
    "        monetary_splits = [0, 100, 500, 1000, float('inf')]\n",
    "    else:\n",
    "        avg_monetary = monetary_stats[\"avg_monetary\"] or 0\n",
    "        monetary_splits = [0, avg_monetary * 0.5, avg_monetary, avg_monetary * 1.5, float('inf')]\n",
    "    \n",
    "    logger.info(f\"Splits récence: {recency_splits}\")\n",
    "    logger.info(f\"Splits fréquence: {frequency_splits}\")\n",
    "    logger.info(f\"Splits monétaire: {monetary_splits}\")\n",
    "    \n",
    "    # Création des bucketizers\n",
    "    recency_bucketizer = Bucketizer(\n",
    "        splits=recency_splits, \n",
    "        inputCol=\"recency\", \n",
    "        outputCol=\"recency_score\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    frequency_bucketizer = Bucketizer(\n",
    "        splits=frequency_splits, \n",
    "        inputCol=\"frequency\", \n",
    "        outputCol=\"frequency_score\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    monetary_bucketizer = Bucketizer(\n",
    "        splits=monetary_splits, \n",
    "        inputCol=\"monetary\", \n",
    "        outputCol=\"monetary_score\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    # Appliquer les bucketizers\n",
    "    rfm_df = recency_bucketizer.transform(rfm_df)\n",
    "    rfm_df = frequency_bucketizer.transform(rfm_df)\n",
    "    rfm_df = monetary_bucketizer.transform(rfm_df)\n",
    "    \n",
    "    # Inverser le score de récence et normaliser tous les scores\n",
    "    rfm_df = rfm_df.withColumn(\"recency_score\", 5.0 - col(\"recency_score\"))\n",
    "    rfm_df = rfm_df.withColumn(\"recency_score\", \n",
    "                              when(col(\"recency_score\") < 1, 1)\n",
    "                              .when(col(\"recency_score\") > 5, 5)\n",
    "                              .otherwise(col(\"recency_score\")))\n",
    "    \n",
    "    # Normaliser frequency et monetary scores\n",
    "    for score_col in [\"frequency_score\", \"monetary_score\"]:\n",
    "        rfm_df = rfm_df.withColumn(score_col, col(score_col) + 1)\n",
    "        rfm_df = rfm_df.withColumn(score_col, \n",
    "                                  when(col(score_col) > 5, 5)\n",
    "                                  .otherwise(col(score_col)))\n",
    "    \n",
    "    # Calculer le statut actif\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"is_active\",\n",
    "        when(col(\"frequency\") > 0, 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Calcul du score RFM global\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"rfm_score\", \n",
    "        col(\"recency_score\") * 100 + col(\"frequency_score\") * 10 + col(\"monetary_score\")\n",
    "    )\n",
    "    \n",
    "    # Segmentation RFM robuste\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"rfm_segment\",\n",
    "        when(col(\"is_active\") == 0, \"Inactif\")\n",
    "        .when((col(\"recency_score\") >= 4) & (col(\"frequency_score\") >= 4) & (col(\"monetary_score\") >= 4), \"Champions\")\n",
    "        .when((col(\"recency_score\") >= 4) & (col(\"frequency_score\") >= 4), \"Loyal Customers\")\n",
    "        .when((col(\"recency_score\") >= 3) & (col(\"monetary_score\") >= 4), \"Big Spenders\")\n",
    "        .when((col(\"recency_score\") >= 3) & (col(\"frequency_score\") >= 3), \"Potential Loyalists\")\n",
    "        .when((col(\"recency_score\") <= 2) & (col(\"frequency_score\") >= 3), \"At Risk\")\n",
    "        .when((col(\"recency_score\") <= 2) & (col(\"frequency_score\") <= 2), \"Hibernating\")\n",
    "        .when((col(\"recency_score\") >= 4) & (col(\"frequency_score\") <= 2), \"New Customers\")\n",
    "        .when((col(\"recency_score\") >= 3) & (col(\"frequency_score\") <= 2), \"Need Attention\")\n",
    "        .otherwise(\"Others\")\n",
    "    )\n",
    "    \n",
    "    # Afficher la distribution des segments - utilisation collect() pour éviter les erreurs UDF\n",
    "    segment_distribution = rfm_df.groupBy(\"rfm_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "    \n",
    "    logger.info(\"Distribution des segments RFM:\")\n",
    "    for row in segment_distribution:\n",
    "        logger.info(f\"{row['rfm_segment']}: {row['count']}\")\n",
    "    \n",
    "    return rfm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6aba1",
   "metadata": {},
   "source": [
    "## 7. Clustering comportemental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5337b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_behavioral_clustering(user_df):\n",
    "    \"\"\"Prépare les données pour le clustering comportemental avec optimisations\"\"\"\n",
    "    logger.info(\"Préparation des données pour clustering comportemental\")\n",
    "    \n",
    "    # Sélection des features comportementales pertinentes\n",
    "    behavior_features = [\n",
    "        \"nb_events\", \"nb_views\", \"nb_carts\", \"nb_purchases\", \"nb_removes\",\n",
    "        \"avg_price_viewed\", \"avg_price_purchased\", \"nb_sessions\",\n",
    "        \"conversion_rate\", \"cart_abandonment\", \"engagement_days\"\n",
    "    ]\n",
    "    \n",
    "    # Filtrer et mettre en cache\n",
    "    clustering_df = user_df.filter(col(\"nb_events\") >= 2).cache()\n",
    "    logger.info(f\"Utilisateurs avec au moins 2 événements: {clustering_df.count()}\")\n",
    "    \n",
    "    # Remplacer les valeurs nulles par des zéros\n",
    "    clustering_df = clustering_df.na.fill({\n",
    "        feature: 0 for feature in behavior_features\n",
    "    })\n",
    "    \n",
    "    # Assembler les features en vecteurs\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=behavior_features,\n",
    "        outputCol=\"features_raw\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    clustering_df = assembler.transform(clustering_df)\n",
    "    \n",
    "    # Standardiser les features\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_raw\", \n",
    "        outputCol=\"features\",\n",
    "        withStd=True, \n",
    "        withMean=True\n",
    "    )\n",
    "    \n",
    "    # Pipeline pour le preprocessing\n",
    "    preprocessing_pipeline = Pipeline(stages=[scaler])\n",
    "    preprocessing_model = preprocessing_pipeline.fit(clustering_df)\n",
    "    clustering_df = preprocessing_model.transform(clustering_df)\n",
    "    \n",
    "    return clustering_df, behavior_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e732d",
   "metadata": {},
   "source": [
    "## 8. Entraînement KMeans et analyse des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9f4c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kmeans_model(df, feature_col=\"features\", k_values=range(2, 11)):\n",
    "    \"\"\"Entraîne et évalue plusieurs modèles K-means avec différentes valeurs de k\"\"\"\n",
    "    logger.info(\"Entraînement des modèles K-means\")\n",
    "    \n",
    "    # Liste pour stocker les résultats\n",
    "    silhouette_scores = []\n",
    "    models = {}\n",
    "    \n",
    "    # Évaluateur pour le clustering\n",
    "    evaluator = ClusteringEvaluator(\n",
    "        predictionCol=\"prediction\", \n",
    "        featuresCol=feature_col,\n",
    "        metricName=\"silhouette\"\n",
    "    )\n",
    "    \n",
    "    # Tester différentes valeurs de k\n",
    "    for k in k_values:\n",
    "        logger.info(f\"Essai avec k={k}\")\n",
    "        \n",
    "        try:\n",
    "            # Créer et entraîner le modèle\n",
    "            kmeans = KMeans(\n",
    "                k=k, \n",
    "                seed=42, \n",
    "                featuresCol=feature_col,\n",
    "                maxIter=20,\n",
    "                tol=1e-4\n",
    "            )\n",
    "            model = kmeans.fit(df)\n",
    "            \n",
    "            # Faire des prédictions\n",
    "            predictions = model.transform(df)\n",
    "            \n",
    "            # Évaluer le modèle\n",
    "            silhouette = evaluator.evaluate(predictions)\n",
    "            logger.info(f\"Silhouette pour k={k}: {silhouette}\")\n",
    "            \n",
    "            # Stocker les résultats\n",
    "            silhouette_scores.append(silhouette)\n",
    "            models[k] = model\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur avec k={k}: {str(e)}\")\n",
    "            silhouette_scores.append(-1)  # Score invalide\n",
    "            models[k] = None\n",
    "    \n",
    "    # Trouver la meilleure valeur de k en utilisant numpy\n",
    "    silhouette_array = np.array(silhouette_scores)\n",
    "    valid_indices = silhouette_array > -1  # Filtrer les scores invalides\n",
    "    \n",
    "    if not np.any(valid_indices):\n",
    "        raise ValueError(\"Aucun modèle valide trouvé\")\n",
    "    \n",
    "    # Trouver l'indice du meilleur score parmi les valides\n",
    "    best_idx = np.argmax(silhouette_array[valid_indices])\n",
    "    # Convertir l'indice local (parmi les valides) en indice global\n",
    "    global_indices = np.where(valid_indices)[0]\n",
    "    global_best_idx = global_indices[best_idx]\n",
    "    \n",
    "    best_k = list(k_values)[global_best_idx]\n",
    "    best_score = silhouette_scores[global_best_idx]\n",
    "    best_model = models[best_k]\n",
    "    \n",
    "    logger.info(f\"Meilleur modèle: k={best_k} avec silhouette={best_score}\")\n",
    "    \n",
    "    # Générer les prédictions avec le meilleur modèle\n",
    "    results = best_model.transform(df)\n",
    "    \n",
    "    # Afficher la distribution des clusters\n",
    "    logger.info(\"Distribution des clusters:\")\n",
    "    results.groupBy(\"prediction\").count().orderBy(\"prediction\").show()\n",
    "    \n",
    "    # Ajout de visualisation des scores silhouette\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(list(k_values), silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
    "        plt.xlabel('Nombre de clusters', fontsize=12)\n",
    "        plt.ylabel('Score Silhouette', fontsize=12)\n",
    "        plt.title('Optimisation du nombre de clusters - Score Silhouette', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Créer le timestamp pour le nom de fichier\n",
    "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plt.savefig(f\"{RESULTS_DIR}/silhouette_scores_{timestamp_str}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logger.info(\"Graphique des scores silhouette sauvegardé\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Erreur lors de la sauvegarde du graphique: {str(e)}\")\n",
    "    \n",
    "    return best_model, results, best_k, silhouette_scores    \n",
    "\n",
    "def analyze_clusters(df, cluster_col=\"prediction\", feature_cols=None):\n",
    "    \"\"\"Analyse les caractéristiques des clusters avec gestion robuste\"\"\"\n",
    "    logger.info(\"Analyse des caractéristiques des clusters\")\n",
    "    \n",
    "    # Calculer les moyennes par cluster\n",
    "    agg_exprs = [count(\"*\").alias(\"cluster_size\")]\n",
    "    for col_name in feature_cols:\n",
    "        agg_exprs.append(avg(col(col_name)).alias(f\"avg_{col_name}\"))\n",
    "\n",
    "    cluster_stats = df.groupBy(cluster_col).agg(*agg_exprs).orderBy(cluster_col).collect()\n",
    "    \n",
    "    # Afficher les statistiques par cluster via logging\n",
    "    logger.info(\"Statistiques par cluster:\")\n",
    "    for row in cluster_stats:\n",
    "        logger.info(f\"Cluster {row[cluster_col]}: {dict(row.asDict())}\")\n",
    "    \n",
    "    return cluster_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe7eec",
   "metadata": {},
   "source": [
    "## 9. Combinaison des segmentations RFM et comportementale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a807ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_segmentations(cluster_df, rfm_df):\n",
    "    \"\"\"Combine les segmentations RFM et clustering comportemental - VERSION CORRIGÉE SANS UDF\"\"\"\n",
    "    logger.info(\"Combinaison des segmentations RFM et clustering\")\n",
    "    \n",
    "    # Debug : Vérifier les données d'entrée\n",
    "    logger.info(f\"Cluster DF count: {cluster_df.count()}\")\n",
    "    logger.info(f\"RFM DF count: {rfm_df.count()}\")\n",
    "    \n",
    "    # Jointure sécurisée avec repartitioning\n",
    "    cluster_df = cluster_df.repartition(20, \"user_id\")\n",
    "    rfm_df = rfm_df.repartition(20, \"user_id\").select(\"user_id\", \"rfm_segment\", \"rfm_score\")\n",
    "    \n",
    "    combined_df = cluster_df.join(rfm_df, on=\"user_id\", how=\"inner\")\n",
    "    \n",
    "    # Renommer les colonnes pour plus de clarté\n",
    "    combined_df = combined_df.withColumnRenamed(\"prediction\", \"behavior_cluster\")\n",
    "    \n",
    "    # SOLUTION: Remplacer l'UDF par une expression CASE WHEN native Spark\n",
    "    # Créer les étiquettes des clusters avec une expression conditionnelle\n",
    "    combined_df = combined_df.withColumn(\n",
    "        \"behavior_segment\",\n",
    "        when(col(\"behavior_cluster\") == 0, \"Explorateurs Occasionnels\")\n",
    "        .when(col(\"behavior_cluster\") == 1, \"Acheteurs Fidèles\")\n",
    "        .when(col(\"behavior_cluster\") == 2, \"Visiteurs Fréquents\")\n",
    "        .when(col(\"behavior_cluster\") == 3, \"Acheteurs à Fort Panier\")\n",
    "        .when(col(\"behavior_cluster\") == 4, \"Visiteurs Uniques\")\n",
    "        .when(col(\"behavior_cluster\") == 5, \"Convertisseurs Efficaces\")\n",
    "        .when(col(\"behavior_cluster\") == 6, \"Indécis (Abandon Panier)\")\n",
    "        .when(col(\"behavior_cluster\") == 7, \"Browsers Passifs\")\n",
    "        .when(col(\"behavior_cluster\") == 8, \"Acheteurs Impulsifs\")\n",
    "        .otherwise(\"Segment Non Défini\")\n",
    "    )\n",
    "    \n",
    "    # Afficher les distributions via collect() pour éviter les erreurs\n",
    "    logger.info(\"Distribution des segments comportementaux:\")\n",
    "    behavior_distribution = combined_df.groupBy(\"behavior_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "    for row in behavior_distribution:\n",
    "        logger.info(f\"{row['behavior_segment']}: {row['count']}\")\n",
    "    \n",
    "    # Analyser l'affinité entre segments\n",
    "    affinity_results = combined_df.groupBy(\"behavior_segment\", \"rfm_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "    logger.info(\"Top 10 affinités entre segments RFM et comportementaux:\")\n",
    "    for i, row in enumerate(affinity_results[:10]):\n",
    "        logger.info(f\"{i+1}. {row['behavior_segment']} + {row['rfm_segment']}: {row['count']}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230d24a",
   "metadata": {},
   "source": [
    "## 10. Fonctions de préparation et d'entraînement pour la recommandation produit (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3baa741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_interaction_data(df):\n",
    "    \"\"\"Prépare les données d'interaction pour le système de recommandation\"\"\"\n",
    "    logger.info(\"Préparation des données d'interaction pour recommandations\")\n",
    "    \n",
    "    # Créer des ratings implicites basés sur les événements\n",
    "    rating_weights = {\n",
    "        'view': 1.0,\n",
    "        'cart': 2.0,\n",
    "        'purchase': 5.0,\n",
    "        'remove_from_cart': -1.0\n",
    "    }\n",
    "    \n",
    "    # Convertir en expression Spark SQL\n",
    "    rating_expr = \" + \".join([\n",
    "        f\"CASE WHEN event_type = '{event}' THEN {weight} ELSE 0 END\"\n",
    "        for event, weight in rating_weights.items()\n",
    "    ])\n",
    "    \n",
    "    interactions = df.groupBy(\"user_id\", \"product_id\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"interaction_count\"),\n",
    "            expr(f\"sum({rating_expr})\").alias(\"rating_score\"),\n",
    "            collect_list(\"event_type\").alias(\"event_types\"),\n",
    "            avg(\"price\").alias(\"avg_price\")\n",
    "        ) \\\n",
    "        .filter(col(\"rating_score\") > 0) \\\n",
    "        .withColumn(\"rating\", \n",
    "                   when(col(\"rating_score\") > 10, 5.0)\n",
    "                   .when(col(\"rating_score\") > 5, 4.0)\n",
    "                   .when(col(\"rating_score\") > 2, 3.0)\n",
    "                   .when(col(\"rating_score\") > 1, 2.0)\n",
    "                   .otherwise(1.0))\n",
    "    \n",
    "    logger.info(f\"Interactions préparées: {interactions.count()} paires user-product\")\n",
    "    return interactions\n",
    "\n",
    "def train_recommendation_models(interactions_df):\n",
    "    \"\"\"Entraîne les modèles de recommandation ALS et content-based\"\"\"\n",
    "    logger.info(\"Entraînement du modèle ALS pour recommandations collaboratives\")\n",
    "    \n",
    "    with MemoryManager() as mm:\n",
    "        # Préparation des données pour ALS\n",
    "        interactions_cached = mm.cache_df(interactions_df, \"interactions\")\n",
    "        \n",
    "        # Correction : cast des IDs en Long\n",
    "        from pyspark.sql.types import LongType\n",
    "        recommendation_df = interactions_cached \\\n",
    "            .withColumn(\"user_id\", col(\"user_id\").cast(LongType())) \\\n",
    "            .withColumn(\"product_id\", col(\"product_id\").cast(LongType()))\n",
    "        \n",
    "        # Division train/test\n",
    "        train_data, test_data = recommendation_df.randomSplit([0.8, 0.2], seed=42)\n",
    "        train_data = mm.cache_df(train_data, \"train_data\")\n",
    "        test_data = mm.cache_df(test_data, \"test_data\")\n",
    "        \n",
    "        # Configuration ALS avec validation croisée\n",
    "        als = ALS(\n",
    "            userCol=\"user_id\",\n",
    "            itemCol=\"product_id\",\n",
    "            ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Grid search pour optimiser les hyperparamètres\n",
    "        param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 20, 30]) \\\n",
    "            .addGrid(als.regParam, [0.01, 0.1, 1.0]) \\\n",
    "            .addGrid(als.alpha, [1.0, 10.0]) \\\n",
    "            .build()\n",
    "        \n",
    "        evaluator = RegressionEvaluator(\n",
    "            metricName=\"rmse\",\n",
    "            labelCol=\"rating\",\n",
    "            predictionCol=\"prediction\"\n",
    "        )\n",
    "        \n",
    "        crossval = CrossValidator(\n",
    "            estimator=als,\n",
    "            estimatorParamMaps=param_grid,\n",
    "            evaluator=evaluator,\n",
    "            numFolds=3,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Entraînement du modèle\n",
    "        logger.info(\"Recherche des meilleurs hyperparamètres ALS...\")\n",
    "        cv_model = crossval.fit(train_data)\n",
    "        best_model = cv_model.bestModel\n",
    "        \n",
    "        # Évaluation\n",
    "        predictions = best_model.transform(test_data)\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        logger.info(f\"RMSE du modèle ALS: {rmse:.4f}\")\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"{MODELS_DIR}/als_model_{timestamp_str}\"\n",
    "        best_model.save(model_path)\n",
    "        logger.info(f\"Modèle ALS sauvegardé: {model_path}\")\n",
    "        \n",
    "        return best_model, rmse\n",
    "\n",
    "def generate_recommendations(als_model, user_id, spark, content_vectors=None, num_recommendations=10):\n",
    "    \"\"\"Génère des recommandations pour un utilisateur spécifique\"\"\"\n",
    "    try:\n",
    "        # Recommandations collaboratives via ALS\n",
    "        user_df = spark.createDataFrame([(user_id,)], [\"user_id\"])\n",
    "        user_recommendations = als_model.recommendForUserSubset(user_df, num_recommendations)\n",
    "        \n",
    "        if user_recommendations.count() > 0:\n",
    "            recs = user_recommendations.select(\"recommendations\").collect()[0][\"recommendations\"]\n",
    "            collaborative_recs = [(rec[\"product_id\"], rec[\"rating\"]) for rec in recs]\n",
    "            logger.info(f\"Généré {len(collaborative_recs)} recommandations collaboratives pour l'utilisateur {user_id}\")\n",
    "            return collaborative_recs\n",
    "        else:\n",
    "            logger.warning(f\"Aucune recommandation collaborative pour l'utilisateur {user_id}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la génération de recommandations pour {user_id}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc3cc3",
   "metadata": {},
   "source": [
    "## 11 execution du pipeline d'enrainement des modeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0739d249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 19:59:18,599 - INFO - Session Spark initialisée\n",
      "2025-05-24 19:59:18,601 - INFO - Chargement des données prétraitées\n",
      "2025-05-24 19:59:22,995 - INFO - Comportements utilisateurs chargés: 163024 lignes\n",
      "2025-05-24 19:59:22,996 - INFO - Préparation des données pour segmentation RFM\n",
      "2025-05-24 19:59:25,349 - INFO - Splits récence: [0, 10, 20, 30, inf]\n",
      "2025-05-24 19:59:25,350 - INFO - Splits fréquence: [0, 0.03819069584846403, 0.07638139169692806, 0.1145720875453921, inf]\n",
      "2025-05-24 19:59:25,353 - INFO - Splits monétaire: [0, 16.671183476052605, 33.34236695210521, 50.013550428157814, inf]\n",
      "2025-05-24 19:59:27,672 - INFO - Distribution des segments RFM:\n",
      "2025-05-24 19:59:27,673 - INFO - Inactif: 150572\n",
      "2025-05-24 19:59:27,674 - INFO - At Risk: 12452\n",
      "2025-05-24 19:59:27,675 - INFO - Préparation des données pour clustering comportemental\n",
      "2025-05-24 19:59:28,542 - INFO - Utilisateurs avec au moins 2 événements: 117529\n",
      "2025-05-24 19:59:29,701 - INFO - Entraînement des modèles K-means\n",
      "2025-05-24 19:59:29,708 - INFO - Essai avec k=2\n",
      "2025-05-24 19:59:36,071 - INFO - Silhouette pour k=2: 0.7839887201925793\n",
      "2025-05-24 19:59:36,072 - INFO - Essai avec k=3\n",
      "2025-05-24 19:59:39,542 - INFO - Silhouette pour k=3: 0.7234835367892903\n",
      "2025-05-24 19:59:39,544 - INFO - Essai avec k=4\n",
      "2025-05-24 19:59:42,642 - INFO - Silhouette pour k=4: 0.729655373579742\n",
      "2025-05-24 19:59:42,643 - INFO - Essai avec k=5\n",
      "2025-05-24 19:59:45,463 - INFO - Silhouette pour k=5: 0.7333949850155613\n",
      "2025-05-24 19:59:45,465 - INFO - Essai avec k=6\n",
      "2025-05-24 19:59:48,323 - INFO - Silhouette pour k=6: 0.5498959436136374\n",
      "2025-05-24 19:59:48,325 - INFO - Essai avec k=7\n",
      "2025-05-24 19:59:51,178 - INFO - Silhouette pour k=7: 0.7517621699920589\n",
      "2025-05-24 19:59:51,179 - INFO - Meilleur modèle: k=2 avec silhouette=0.7839887201925793\n",
      "2025-05-24 19:59:51,204 - INFO - Distribution des clusters:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|         0|107723|\n",
      "|         1|  9806|\n",
      "+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 19:59:51,796 - INFO - Graphique des scores silhouette sauvegardé\n",
      "2025-05-24 19:59:51,813 - INFO - Analyse des caractéristiques des clusters\n",
      "2025-05-24 19:59:52,665 - INFO - Statistiques par cluster:\n",
      "2025-05-24 19:59:52,666 - INFO - Cluster 0: {'prediction': 0, 'cluster_size': 107723, 'avg_nb_events': 7.845093434085571, 'avg_nb_views': 7.777057824234379, 'avg_nb_carts': 0.04321268438495029, 'avg_nb_purchases': 0.02482292546624212, 'avg_nb_removes': 0.0, 'avg_avg_price_viewed': 320.8869577016758, 'avg_avg_price_purchased': 2.1138300084475916, 'avg_nb_sessions': 1.501072194424589, 'avg_conversion_rate': 0.002730616324765064, 'avg_cart_abandonment': 0.027586185556164114, 'avg_engagement_days': 1.0}\n",
      "2025-05-24 19:59:52,667 - INFO - Cluster 1: {'prediction': 1, 'cluster_size': 9806, 'avg_nb_events': 11.157250662859473, 'avg_nb_views': 8.6962064042423, 'avg_nb_carts': 1.018050173363247, 'avg_nb_purchases': 1.4429940852539263, 'avg_nb_removes': 0.0, 'avg_avg_price_viewed': 372.51225273436495, 'avg_avg_price_purchased': 372.28669310859163, 'avg_nb_sessions': 1.9568631450132572, 'avg_conversion_rate': 0.3734051640774437, 'avg_cart_abandonment': 0.022791749825061942, 'avg_engagement_days': 1.0}\n",
      "2025-05-24 19:59:53,172 - INFO - Modèle K-means sauvegardé: ./models/kmeans_behavioral_2_clusters_20250524_195952\n",
      "2025-05-24 19:59:53,178 - INFO - Combinaison des segmentations RFM et clustering\n",
      "2025-05-24 19:59:53,330 - INFO - Cluster DF count: 117529\n",
      "2025-05-24 19:59:53,427 - INFO - RFM DF count: 163024\n",
      "2025-05-24 19:59:53,496 - INFO - Distribution des segments comportementaux:\n",
      "2025-05-24 19:59:55,044 - INFO - Explorateurs Occasionnels: 107723\n",
      "2025-05-24 19:59:55,045 - INFO - Acheteurs Fidèles: 9806\n",
      "2025-05-24 19:59:56,139 - INFO - Top 10 affinités entre segments RFM et comportementaux:\n",
      "2025-05-24 19:59:56,140 - INFO - 1. Explorateurs Occasionnels + Inactif: 105049\n",
      "2025-05-24 19:59:56,141 - INFO - 2. Acheteurs Fidèles + At Risk: 9754\n",
      "2025-05-24 19:59:56,142 - INFO - 3. Explorateurs Occasionnels + At Risk: 2674\n",
      "2025-05-24 19:59:56,143 - INFO - 4. Acheteurs Fidèles + Inactif: 52\n",
      "2025-05-24 19:59:57,565 - INFO - Segmentations combinées sauvegardées: ./results/combined_segmentation_20250524_195952.parquet\n",
      "2025-05-24 19:59:58,664 - INFO - Profils utilisateurs sauvegardés: ./results/user_profiles_20250524_195952.parquet\n",
      "2025-05-24 19:59:58,665 - INFO - === DÉMARRAGE DE LA PHASE RECOMMANDATION DE PRODUIT ===\n",
      "2025-05-24 19:59:58,753 - INFO - Préparation des données d'interaction pour recommandations\n",
      "2025-05-24 20:00:00,212 - INFO - Interactions préparées: 631528 paires user-product\n",
      "2025-05-24 20:00:00,212 - INFO - Entraînement du modèle ALS pour recommandations collaboratives\n",
      "2025-05-24 20:00:00,226 - INFO - DataFrame 'interactions' mis en cache\n",
      "2025-05-24 20:00:00,269 - INFO - DataFrame 'train_data' mis en cache\n",
      "2025-05-24 20:00:00,278 - INFO - DataFrame 'test_data' mis en cache\n",
      "2025-05-24 20:00:00,291 - INFO - Recherche des meilleurs hyperparamètres ALS...\n",
      "2025-05-24 20:09:19,597 - INFO - Closing down clientserver connection\n",
      "2025-05-24 20:09:19,597 - INFO - Closing down clientserver connection\n",
      "2025-05-24 20:09:20,769 - INFO - RMSE du modèle ALS: 0.9083\n",
      "2025-05-24 20:09:22,457 - INFO - Modèle ALS sauvegardé: ./models/als_model_20250524_200920\n",
      "2025-05-24 20:09:22,459 - INFO - DataFrame 'interactions' libéré du cache\n",
      "2025-05-24 20:09:22,460 - INFO - DataFrame 'train_data' libéré du cache\n",
      "2025-05-24 20:09:22,461 - INFO - DataFrame 'test_data' libéré du cache\n",
      "2025-05-24 20:09:22,462 - INFO - Modèle ALS entraîné. RMSE: {rmse:.4f}\n",
      "2025-05-24 20:09:34,497 - ERROR - Erreur lors de la génération de recommandations pour 477500496: An error occurred while calling o13306.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 7348.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7348.0 (TID 21606) (Desktop-g9s1re6 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "2025-05-24 20:09:34,498 - INFO - Recommandations pour l'utilisateur {user_id}: {recommandations}\n",
      "2025-05-24 20:10:54,691 - ERROR - Erreur lors de la génération de recommandations pour 512370871: An error occurred while calling o13334.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 7374.0 failed 1 times, most recent failure: Lost task 6.0 in stage 7374.0 (TID 21628) (Desktop-g9s1re6 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "2025-05-24 20:10:54,692 - INFO - Recommandations pour l'utilisateur {user_id}: {recommandations}\n",
      "2025-05-24 20:12:04,969 - ERROR - Erreur lors de la génération de recommandations pour 512412945: An error occurred while calling o13362.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 7400.0 failed 1 times, most recent failure: Lost task 5.0 in stage 7400.0 (TID 21645) (Desktop-g9s1re6 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "2025-05-24 20:12:04,970 - INFO - Recommandations pour l'utilisateur {user_id}: {recommandations}\n",
      "2025-05-24 20:13:15,249 - ERROR - Erreur lors de la génération de recommandations pour 512421058: An error occurred while calling o13390.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7426.0 failed 1 times, most recent failure: Lost task 4.0 in stage 7426.0 (TID 21662) (Desktop-g9s1re6 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "2025-05-24 20:13:15,250 - INFO - Recommandations pour l'utilisateur {user_id}: {recommandations}\n",
      "2025-05-24 20:14:25,575 - ERROR - Erreur lors de la génération de recommandations pour 512442248: An error occurred while calling o13418.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7452.0 failed 1 times, most recent failure: Lost task 4.0 in stage 7452.0 (TID 21679) (Desktop-g9s1re6 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\n",
      "\t... 34 more\n",
      "\n",
      "2025-05-24 20:14:25,576 - INFO - Recommandations pour l'utilisateur {user_id}: {recommandations}\n",
      "2025-05-24 20:14:25,873 - INFO - Graphique final des scores silhouette sauvegardé\n",
      "2025-05-24 20:14:25,874 - INFO - === RÉSUMÉ DE LA SEGMENTATION ===\n",
      "2025-05-24 20:14:35,726 - INFO - Nombre d'utilisateurs total: 163024\n",
      "2025-05-24 20:14:35,944 - INFO - Utilisateurs pour clustering: 117529\n",
      "2025-05-24 20:14:35,945 - INFO - Nombre optimal de clusters: 2\n",
      "2025-05-24 20:14:35,946 - INFO - Score silhouette optimal: 0.7840\n",
      "2025-05-24 20:14:37,362 - INFO - Utilisateurs segmentés: 117529\n",
      "2025-05-24 20:14:37,363 - INFO - Distribution finale des segments RFM:\n",
      "2025-05-24 20:14:38,917 - INFO - Inactif: 105101\n",
      "2025-05-24 20:14:38,918 - INFO - At Risk: 12428\n",
      "2025-05-24 20:14:38,918 - INFO - Distribution finale des segments comportementaux:\n",
      "2025-05-24 20:14:40,626 - INFO - Explorateurs Occasionnels: 107723\n",
      "2025-05-24 20:14:40,626 - INFO - Acheteurs Fidèles: 9806\n",
      "2025-05-24 20:14:41,643 - INFO - Session Spark fermée\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialisation de la session Spark\n",
    "    spark = create_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")  # Réduire les logs Spark\n",
    "    logger.info(\"Session Spark initialisée\")\n",
    "    \n",
    "    try:\n",
    "        # Trouver les fichiers les plus récents\n",
    "        latest_cleaned = find_latest_file(DATA_DIR, \"cleaned_data\")\n",
    "        latest_user_behavior = find_latest_file(DATA_DIR, \"user_behavior\")\n",
    "        latest_recommendation = find_latest_file(DATA_DIR, \"recommendation_data\")\n",
    "        latest_product = find_latest_file(DATA_DIR, \"product_data\")\n",
    "        latest_time_series = find_latest_file(DATA_DIR, \"time_series_data\")\n",
    "        \n",
    "        # Vérifier que les fichiers existent\n",
    "        if not latest_user_behavior:\n",
    "            raise FileNotFoundError(\"Fichier user_behavior non trouvé\")\n",
    "        \n",
    "        # Chargement des données principales\n",
    "        logger.info(\"Chargement des données prétraitées\")\n",
    "        \n",
    "        user_behavior_df = spark.read.parquet(os.path.join(DATA_DIR, latest_user_behavior))\n",
    "        logger.info(f\"Comportements utilisateurs chargés: {user_behavior_df.count()} lignes\")\n",
    "        \n",
    "        # Exécuter la segmentation RFM\n",
    "        rfm_segmentation = prepare_rfm_segmentation(user_behavior_df)\n",
    "        \n",
    "        # Exécuter la préparation pour le clustering\n",
    "        behavior_clustering_df, behavior_features = prepare_behavioral_clustering(user_behavior_df)\n",
    "        \n",
    "        # Entraîner le modèle K-means\n",
    "        kmeans_model, cluster_results, best_k, silhouette_scores = train_kmeans_model(\n",
    "            behavior_clustering_df, feature_col=\"features\", k_values=range(2, 8)\n",
    "        )\n",
    "        \n",
    "        # Analyser les clusters obtenus\n",
    "        cluster_stats = analyze_clusters(\n",
    "            cluster_results.select(\"user_id\", \"prediction\", *behavior_features),\n",
    "            cluster_col=\"prediction\", \n",
    "            feature_cols=behavior_features\n",
    "        )\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"{MODELS_DIR}/kmeans_behavioral_{best_k}_clusters_{timestamp_str}\"\n",
    "        kmeans_model.save(model_path)\n",
    "        logger.info(f\"Modèle K-means sauvegardé: {model_path}\")\n",
    "        \n",
    "        # Combiner les segmentations\n",
    "        user_clusters = cluster_results.select(\"user_id\", \"prediction\")\n",
    "        combined_segments = combine_segmentations(user_clusters, rfm_segmentation)\n",
    "        \n",
    "        # Sauvegarder les segmentations combinées\n",
    "        combined_output_path = f\"{RESULTS_DIR}/combined_segmentation_{timestamp_str}.parquet\"\n",
    "        combined_segments.write.mode(\"overwrite\").format(\"parquet\").save(combined_output_path)\n",
    "        logger.info(f\"Segmentations combinées sauvegardées: {combined_output_path}\")\n",
    "        \n",
    "        # Créer un dataframe de profils utilisateurs pour les recommandations\n",
    "        user_profiles = combined_segments.select(\n",
    "            \"user_id\", \"behavior_cluster\", \"behavior_segment\", \"rfm_segment\", \"rfm_score\"\n",
    "        )\n",
    "        \n",
    "        # Sauvegarder les profils utilisateurs\n",
    "        profiles_output_path = f\"{RESULTS_DIR}/user_profiles_{timestamp_str}.parquet\"\n",
    "        user_profiles.write.mode(\"overwrite\").format(\"parquet\").save(profiles_output_path)\n",
    "        logger.info(f\"Profils utilisateurs sauvegardés: {profiles_output_path}\")\n",
    "        \n",
    "        # === PARTIE RECOMMANDATION DE PRODUIT ===\n",
    "        try:\n",
    "            logger.info(\"=== DÉMARRAGE DE LA PHASE RECOMMANDATION DE PRODUIT ===\")\n",
    "            # 1. Préparer les interactions utilisateur-produit\n",
    "            recommendation_df = spark.read.parquet(os.path.join(DATA_DIR, latest_recommendation))\n",
    "            \n",
    "            # Correction : cast des IDs en Long\n",
    "            recommendation_df = recommendation_df \\\n",
    "                .withColumn(\"user_id\", col(\"user_id\").cast(LongType())) \\\n",
    "                .withColumn(\"product_id\", col(\"product_id\").cast(LongType()))\n",
    "            \n",
    "            interactions = prepare_interaction_data(recommendation_df)\n",
    "            \n",
    "            # 2. Entraîner le modèle ALS (filtrage collaboratif)\n",
    "            als_model, rmse = train_recommendation_models(interactions)\n",
    "            logger.info(\"Modèle ALS entraîné. RMSE: {rmse:.4f}\")\n",
    "            \n",
    "            # 3. Charger les données produit pour affichage (optionnel)\n",
    "            if latest_product:\n",
    "                product_data = spark.read.parquet(os.path.join(DATA_DIR, latest_product))\n",
    "            else:\n",
    "                product_data = None\n",
    "            \n",
    "            # 4. Générer des recommandations pour quelques utilisateurs (exemple: 5 premiers)\n",
    "            user_ids = [row[\"user_id\"] for row in user_behavior_df.select(\"user_id\").distinct().limit(5).collect()]\n",
    "            for user_id in user_ids:\n",
    "                recommandations = generate_recommendations(als_model, user_id, spark, num_recommendations=5)\n",
    "                logger.info(\"Recommandations pour l'utilisateur {user_id}: {recommandations}\")\n",
    "                if product_data and recommandations:\n",
    "                    rec_ids = [pid for pid, _ in recommandations]\n",
    "                    produits = product_data.filter(col(\"product_id\").isin(rec_ids)).toPandas()\n",
    "                    for pid, score in recommandations:\n",
    "                        details = produits[produits[\"product_id\"] == pid]\n",
    "                        if not details.empty:\n",
    "                            logger.info(f\"Produit recommandé: {details.iloc[0]['category_code']} - {details.iloc[0]['brand']} | Score: {score:.2f}\")\n",
    "                        else:\n",
    "                            logger.info(f\"Produit ID: {pid} | Score: {score:.2f}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la phase recommandation: {e}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "        \n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(2, 8), silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
    "            plt.xlabel('Nombre de clusters', fontsize=12)\n",
    "            plt.ylabel('Score Silhouette', fontsize=12)\n",
    "            plt.title('Optimisation du nombre de clusters - Score Silhouette', fontsize=14)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/silhouette_scores_final_{timestamp_str}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logger.info(f\"Graphique final des scores silhouette sauvegardé\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Erreur lors de la sauvegarde du graphique final: {str(e)}\")\n",
    "        \n",
    "        # Afficher un résumé final\n",
    "        logger.info(\"=== RÉSUMÉ DE LA SEGMENTATION ===\")\n",
    "        logger.info(f\"Nombre d'utilisateurs total: {user_behavior_df.count()}\")\n",
    "        logger.info(f\"Utilisateurs pour clustering: {behavior_clustering_df.count()}\")\n",
    "        logger.info(f\"Nombre optimal de clusters: {best_k}\")\n",
    "        \n",
    "        # Calculer le meilleur score silhouette de manière sécurisée\n",
    "        valid_scores = [score for score in silhouette_scores if score > -1]\n",
    "        best_silhouette = np.max(valid_scores) if valid_scores else 0\n",
    "        logger.info(f\"Score silhouette optimal: {best_silhouette:.4f}\")\n",
    "        logger.info(f\"Utilisateurs segmentés: {combined_segments.count()}\")\n",
    "        \n",
    "        # Afficher la distribution finale des segments\n",
    "        logger.info(\"Distribution finale des segments RFM:\")\n",
    "        rfm_final_distribution = combined_segments.groupBy(\"rfm_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "        for row in rfm_final_distribution[:10]:\n",
    "            logger.info(f\"{row['rfm_segment']}: {row['count']}\")\n",
    "        \n",
    "        logger.info(\"Distribution finale des segments comportementaux:\")\n",
    "        behavior_final_distribution = combined_segments.groupBy(\"behavior_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "        for row in behavior_final_distribution[:10]:\n",
    "            logger.info(f\"{row['behavior_segment']}: {row['count']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'exécution: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "        logger.info(\"Session Spark fermée\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
