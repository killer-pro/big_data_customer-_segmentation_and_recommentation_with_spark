# Rapport projet : Analyse Comportementale et Recommandation Produit sur un Site E-commerce avec Spark 

## Pr√©sent√© par :
- mouhamadou diouf ciss√©
- ndeye fatou niassy

## plan du projet

1. Introduction
2. Contexte du projet
3. Objectifs du projet
4. Analyse exploratoire des donn√©es
5. Pr√©traitement des donn√©es


# 1. Pr√©sentation du Projet

## Contexte

Le pr√©sent projet vise √† concevoir une solution d‚Äôanalyse comportementale des utilisateurs d‚Äôune plateforme e-commerce, avec pour finalit√© l‚Äô√©laboration d‚Äôun syst√®me de recommandation intelligent. Ce projet, men√© dans un cadre acad√©mique, s‚Äôappuie sur des technologies Big Data et des approches d‚Äôintelligence artificielle afin de traiter efficacement des volumes importants de donn√©es g√©n√©r√©s par les utilisateurs.

## Objectifs du Projet

Le projet vise √† concevoir une solution compl√®te d‚Äôanalyse comportementale et de recommandation intelligente dans un contexte e-commerce. Pour cela, plusieurs objectifs principaux ont √©t√© d√©finis :

### ‚Ä¢ Compr√©hension et mod√©lisation des comportements utilisateurs

L‚Äôobjectif initial est d‚Äôanalyser en profondeur les comportements de navigation et d‚Äôachat des utilisateurs √† partir des donn√©es de logs collect√©es sur une plateforme e-commerce. Il s‚Äôagit notamment d‚Äôidentifier les actions r√©alis√©es par les utilisateurs (consultations, ajouts au panier, achats), d‚Äô√©tudier la chronologie et la fr√©quence de ces actions, et d‚Äôen extraire des tendances comportementales significatives. Ces analyses doivent permettre d‚Äô√©tablir un mod√®le repr√©sentatif des interactions typiques entre un utilisateur et la plateforme.

### ‚Ä¢ Identification des patterns d‚Äôinteractions et des profils types

√Ä partir des donn√©es collect√©es, il est essentiel d‚Äôidentifier des sch√©mas d‚Äôinteractions r√©currents, appel√©s patterns, permettant de regrouper les utilisateurs selon des comportements similaires. Cette √©tape vise √† faire √©merger des profils types de clients (par exemple : acheteurs impulsifs, visiteurs r√©guliers non acheteurs, utilisateurs sensibles au prix, etc.), facilitant ainsi une segmentation fine de la client√®le et une personnalisation plus pertinente des actions marketing ou commerciales.

### ‚Ä¢ Mise en place d‚Äôun syst√®me de recommandation dynamique et personnalis√©

Sur la base des pr√©f√©rences observ√©es (historique de navigation, fr√©quence d‚Äôachat, cat√©gories pr√©f√©r√©es, marques consult√©es), un syst√®me de recommandation doit √™tre d√©velopp√©. Ce syst√®me devra √™tre capable de proposer de mani√®re dynamique des produits pertinents √† chaque utilisateur, en tenant compte √† la fois de ses habitudes pass√©es et de son profil comportemental. Les recommandations devront s‚Äôappuyer sur des techniques hybrides combinant filtrage collaboratif, analyse de contenu, et segmentation client.

### ‚Ä¢ Simulation d‚Äôun environnement temps r√©el pour √©valuation des performances

Enfin, une infrastructure de simulation en temps r√©el sera mise en place afin de tester la robustesse et la r√©activit√© de l‚Äôensemble du syst√®me. Cette simulation permettra de reproduire l‚Äôarriv√©e continue des donn√©es, de d√©clencher le traitement de ces flux par les modules analytiques et de mesurer les performances en conditions quasi-r√©elles.

## Donn√©es
Les jeux de donn√©es fournis sont au format CSV et couvrent les √©v√©nements de navigation et d‚Äôachat sur les mois d‚Äôoctobre et novembre. Chaque enregistrement contient des informations sur le type d‚Äô√©v√©nement, les produits concern√©s, les cat√©gories, la marque, le prix, l‚Äôutilisateur et sa session. Le volume de donn√©es est estim√© √† plusieurs dizaines de millions de lignes, ce qui justifie l‚Äôadoption d‚Äôune architecture Big Data.

# 2. Analyse exploratoire des donn√©es
## üîç Aper√ßu des donn√©es

### üìä √âchantillon des premi√®res lignes du jeu de donn√©es :

| event_time         | event_type | product_id | category_id         | category_code             | brand    | price  | user_id  | user_session                      |
|--------------------|------------|------------|----------------------|----------------------------|----------|--------|----------|-----------------------------------|
| 2019-10-01 00:00:00| view       | 44600062   | 2103807459595387724 | NULL                       | shiseido | 35.79  | 541312140| 72d76fde-8bb3-4e0...              |
| 2019-10-01 00:00:00| view       | 3900821    | 2053013552326770905 | appliances.environment    | aqua     | 33.20  | 554748717| 9333dfbd-b87a-470...              |
| 2019-10-01 00:00:01| view       | 17200506   | 2053013559792632471 | furniture.living_room     | NULL     | 543.10 | 519107250| 566511c2-e2e3-422...              |
| 2019-10-01 00:00:01| view       | 1307067    | 2053013558920217191 | computers.notebook        | lenovo   | 251.74 | 550050854| 7c90fc70-0e80-459...              |
| 2019-10-01 00:00:04| view       | 1004237    | 2053013555631882655 | electronics.smartphone    | apple    |1081.98 | 535871217| c6bd7419-2748-4c5...              |

> *Remarque* : seules les 5 premi√®res lignes sont affich√©es.

---

### üìà R√©partition des types d'√©v√©nements :

| event_type | count     |
|------------|-----------|
| view       | 40,779,399|
| cart       |   926,516 |
| purchase   |   742,849 |

Le jeu de donn√©es contient plusieurs colonnes cl√©s relatives au comportement des utilisateurs sur une plateforme e-commerce :

- *event_time* : Date et heure pr√©cises de l‚Äô√©v√©nement.
- *event_type* : Type d‚Äôaction effectu√©e par l‚Äôutilisateur (view, cart, purchase).
- *product_id* : Identifiant unique du produit concern√©.
- *category_id* et *category_code* : Informations sur la cat√©gorie du produit.
- *brand* : Marque du produit (des valeurs peuvent √™tre manquantes).
- *price* : Prix du produit.
- *user_id* : Identifiant de l‚Äôutilisateur.
- *user_session* : Identifiant unique de la session utilisateur (utile pour reconstituer les parcours utilisateurs).

 üìä Analyse Exploratoire des Donn√©es d'√âv√©nements eCommerce

## üîç Aper√ßu des Donn√©es

| event_time         | event_type | product_id | category_id        | category_code              | brand    | price   | user_id   | user_session                   |
|--------------------|------------|------------|---------------------|-----------------------------|----------|---------|-----------|--------------------------------|
| 2019-10-01 00:00:00| view       | 44600062   | 2103807459595387724| NULL                        | shiseido | 35.79   | 541312140 | 72d76fde-8bb3-4e0...           |
| 2019-10-01 00:00:00| view       | 3900821    | 2053013552326770905| appliances.environment      | aqua     | 33.2    | 554748717 | 9333dfbd-b87a-470...           |
| 2019-10-01 00:00:01| view       | 17200506   | 2053013559792632471| furniture.living_room       | NULL     | 543.1   | 519107250 | 566511c2-e2e3-422...           |
| 2019-10-01 00:00:01| view       | 1307067    | 2053013558920217191| computers.notebook          | lenovo   | 251.74  | 550050854 | 7c90fc70-0e80-459...           |
| 2019-10-01 00:00:04| view       | 1004237    | 2053013555631882655| electronics.smartphone      | apple    | 1081.98 | 535871217 | c6bd7419-2748-4c5...           |

> Affichage des 5 premi√®res lignes du dataset.

---

### üß† Analyse

- *View* est de loin l‚Äô√©v√©nement le plus fr√©quent, repr√©sentant environ *96.1%* de toutes les interactions.  
- Les √©v√©nements *cart* (ajouts au panier) sont environ *2.2%, tandis que les **purchases* (achats) ne repr√©sentent que *1.7%*.  
- Ce d√©s√©quilibre est *typique dans les donn√©es eCommerce* : la majorit√© des utilisateurs consultent les produits sans n√©cessairement acheter.

---

## üìä Visualisation de la Distribution des √âv√©nements

### Histogramme et Camembert
![image.jpg](images%2Fimage.jpg)

*Figure : Diagramme en barres et camembert repr√©sentant la distribution et la proportion des √©v√©nements (view, cart, purchase).*

---

## üîç Interpr√©tation

- *Taux de conversion faible* : Seuls une faible proportion des utilisateurs passent du view au purchase.
- *Potentiel d'optimisation* :
  - Travailler sur la r√©tention des paniers (cart abandonment).
  - Analyser les produits les plus vus vs. ceux les plus achet√©s.
  - Am√©liorer le parcours utilisateur pour favoriser l‚Äôachat

## üßπ Analyse des Valeurs Manquantes

L'examen des valeurs manquantes permet de mieux comprendre la qualit√© et la compl√©tude des donn√©es avant tout traitement ou mod√©lisation.

### üìã Tableau des Valeurs Manquantes

| Colonne         | Valeurs Manquantes | Pourcentage (%) |
|-----------------|--------------------|------------------|
| category_code | 13,515,609         | 31.84%           |
| brand         | 6,113,008          | 14.40%           |
| user_session  | 2                  | 0.000005%        |
| event_time    | 0                  | 0.00%            |
| event_type    | 0                  | 0.00%            |
| product_id    | 0                  | 0.00%            |
| category_id   | 0                  | 0.00%            |
| price         | 0                  | 0.00%            |
| user_id       | 0                  | 0.00%            |

### üîç Interpr√©tation

- ‚úÖ *Colonnes sans valeurs manquantes* : event_time, event_type, product_id, category_id, price, et user_id. Ces colonnes sont enti√®rement compl√®tes.
- ‚ö† *Colonnes avec valeurs manquantes notables* :
  - category_code : *31.84%* de valeurs manquantes, ce qui est √©lev√©. Cela peut poser probl√®me pour la cat√©gorisation ou les analyses li√©es √† la classification de produits.
  - brand : *14.40%* de valeurs manquantes. Cela pourrait affecter les analyses de fid√©lit√© √† la marque ou les √©tudes de pr√©f√©rences.
- üü¢ *Colonne avec peu de valeurs manquantes* : user_session ne contient que 2 valeurs manquantes, ce qui est n√©gligeable et facilement nettoyable.

## ‚è∞ Analyse Temporelle

L‚Äôanalyse temporelle permet de comprendre quand les utilisateurs interagissent le plus avec la plateforme, en examinant les tendances par *jour* et par *heure*.

### üìÖ Distribution des √âv√©nements par Jour du Mois (Octobre 2019)

| Jour du Mois | Nombre d'√âv√©nements |
|--------------|---------------------|
| 1            | 1 244 245           |
| 2            | 1 191 328           |
| 3            | 1 127 303           |
| 4            | 1 417 190           |
| 5            | 1 330 339           |
| 6            | 1 318 379           |
| 7            | 1 200 531           |
| 8            | 1 370 633           |
| 9            | 1 347 543           |
| 10           | 1 284 077           |

> üîç *Observation* : Les √©v√©nements sont globalement bien r√©partis sur le mois d‚Äôoctobre, avec un pic autour du *15 au 20 octobre*. Il n‚Äôy a pas de chute brutale, ce qui sugg√®re une activit√© r√©guli√®re des utilisateurs.

### üïí Distribution des √âv√©nements par Heure de la Journ√©e

| Heure | Nombre d'√âv√©nements |
|-------|---------------------|
| 0     | 306 805             |
| 1     | 559 027             |
| 2     | 1 069 047           |
| 3     | 1 550 285           |
| 4     | 1 915 643           |
| 5     | 2 125 633           |
| 6     | 2 269 092           |
| 7     | 2 335 718           |
| 8     | 2 390 127           |
| 9     | 2 351 683           |

> üîç *Observation* :  
- L'activit√© commence √† monter √† partir de *3h du matin, atteint son pic entre **8h et 15h*, puis redescend progressivement.
- Le *pic horaire* se situe vers *15h*, sugg√©rant un comportement d‚Äôachat ou de navigation intensif en journ√©e.

## üí∞ Analyse des Prix

L‚Äôanalyse des prix permet de comparer les comportements des utilisateurs selon le type d‚Äô√©v√©nement (vue, ajout au panier, achat), en se basant sur les statistiques suivantes : prix moyen, minimum, maximum et le nombre de valeurs non nulles.

### üî¢ Statistiques des Prix par Type d'√âv√©nement

| Type d'√âv√©nement | Prix Moyen (en $) | Prix Min | Prix Max      | Valeurs Non Nulles |
|------------------|-------------------|----------|---------------|---------------------|
| Purchase         | 309.56            | 0.77     | 2 574.07      | 742 849             |
| View             | 288.98            | 0.00     | 2 574.07      | 40 779 399          |
| Cart             | 333.83            | 0.00     | 2 574.04      | 926 516             |

### üßê Observations

- üìå *Prix Moyen* :
  - Les produits ajout√©s au panier (*cart) ont un prix moyen plus √©lev√© (333,83 $) que ceux seulement consult√©s (288,98 $) ou achet√©s (309,56 $*).
  - Cela peut indiquer une intention d‚Äôachat pour des produits de plus grande valeur qui ne sont pas toujours concr√©tis√©e.

- üìâ *Prix Min = 0.00* :
  - Le prix minimal est nul pour les √©v√©nements view et cart, ce qui peut signaler la pr√©sence d‚Äôarticles gratuits ou une donn√©e erron√©e.

- üìà *Prix Max similaire* :
  - Le prix maximal est tr√®s proche pour tous les types d‚Äô√©v√©nements (‚âà 2 574 $), ce qui montre qu‚Äôil existe des produits haut de gamme qui suscitent √† la fois de la consultation, de l‚Äôajout au panier et des achats.

- üßÆ *Volume de donn√©es* :
  - Les √©v√©nements de type view sont les plus nombreux, ce qui est logique car les utilisateurs consultent bien plus qu‚Äôils n‚Äôach√®tent ou ajoutent au panier.

---

## üîó Analyse des Sessions

L‚Äôanalyse des sessions permet de mieux comprendre le comportement utilisateur sur la plateforme, en mesurant le nombre d‚Äô√©v√©nements par session et la dur√©e des sessions.

### üìä Distribution du Nombre d‚Äô√âv√©nements par Session

| Statistique        | Valeur        |
|--------------------|---------------|
| Nombre total       | 9 244 422     |
| Moyenne            | 4.59 √©v√©nements/session |
| √âcart-type         | 6.77          |
| Minimum            | 1 √©v√©nement   |
| 25e percentile     | 1 √©v√©nement   |
| M√©diane (50e)      | 2 √©v√©nements  |
| 75e percentile     | 5 √©v√©nements  |
| Maximum            | 1 159 √©v√©nements |

### ‚è± Dur√©e des Sessions

- *Dur√©e moyenne* : 17,37 minutes  
- *Dur√©e m√©diane* : 1,05 minute

### üßê Observations

- üìå *Sessions courtes* :
  - La m√©diane tr√®s basse (1,05 minute) indique que la majorit√© des sessions sont *tr√®s br√®ves*, avec peu d‚Äôinteractions.
  - Cela sugg√®re une navigation rapide ou peu engageante pour de nombreux utilisateurs.

- üîù *Sessions longues mais rares* :
  - La valeur maximale de *1 159 √©v√©nements* montre qu‚Äôil existe des sessions tr√®s actives, mais ce sont des *cas exceptionnels*.

- üéØ *Comportement utilisateur* :
  - 75 % des sessions contiennent *5 √©v√©nements ou moins*, ce qui renforce l‚Äôid√©e que la plateforme est souvent utilis√©e de mani√®re ponctuelle.
  - Ces statistiques peuvent √™tre utiles pour *segmenter les utilisateurs* selon leur engagement ou ajuster l‚Äôinterface utilisateur (UX).

---

# 3. Pr√©traitement des donn√©es

# T√¢ches de pr√©traitement r√©alis√©es

La fonction preprocess_data effectue plusieurs op√©rations de pr√©traitement sur les donn√©es brutes. Voici les √©tapes principales :

## 1. Extraction des caract√©ristiques temporelles et nettoyage des donn√©e

√Ä partir de la colonne event_time, plusieurs nouvelles colonnes sont cr√©√©es pour enrichir les donn√©es temporelles :

- hour : heure de l'√©v√©nement
- minute : minute de l'√©v√©nement
- second : seconde de l'√©v√©nement
- day : jour du mois
- month : mois
- dayofweek : jour de la semaine
- date : date au format yyyy-MM-dd
- hour_bucket : regroupement horaire (date et heure arrondie √† l'heure, format yyyy-MM-dd HH:00:00)

| event_time          | event_type | product_id | category_id          | category_code       | brand    | price  | user_id   | user_session                        | hour | minute | second | day | month | dayofweek | date       | hour_bucket           |
|---------------------|------------|------------|---------------------|---------------------|----------|--------|-----------|-----------------------------------|------|--------|--------|-----|-------|-----------|------------|-----------------------|
| 2019-10-01 00:00:00 | view       | 44600062   | 2103807459595387724 | unknown             | shiseido | 35.79  | 541312140 | 72d76fde-8bb3-4e0...              | 0    | 0      | 0      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |
| 2019-10-01 00:00:00 | view       | 3900821    | 2053013552326770905 | appliances.enviro... | aqua     | 33.2   | 554748717 | 9333dfbd-b87a-470...              | 0    | 0      | 0      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |
| 2019-10-01 00:00:01 | view       | 17200506   | 2053013559792632471 | furniture.living_... | unknown  | 543.1  | 519107250 | 566511c2-e2e3-422...              | 0    | 0      | 1      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |
| 2019-10-01 00:00:01 | view       | 1307067    | 2053013558920217191 | computers.notebook   | lenovo   | 251.74 | 550050854 | 7c90fc70-0e80-459...              | 0    | 0      | 1      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |
| 2019-10-01 00:00:04 | view       | 1004237    | 2053013555631882655 | electronics.smart... | apple    | 1081.98| 535871217 | c6bd7419-2748-4c5...              | 0    | 0      | 4      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |

---

## 2. Traitement des valeurs manquantes

Pour garantir la qualit√© des donn√©es, certains champs cat√©goriels sont nettoy√©s :

- category_code :  
  - Remplacement des valeurs null ou "NaN" par "unknown"
- brand :  
  - Remplacement des valeurs null par "unknown"



## 3. Nettoyage des prix

- La colonne price est corrig√©e en rempla√ßant les valeurs null ou inf√©rieures ou √©gales √† 0 par null (valeur manquante)
# Entra√Ænement des Mod√®les d'IA avec Apache Spark

## Vue d'ensemble

Cette section pr√©sente l'impl√©mentation de l'entra√Ænement des mod√®les d'intelligence artificielle pour le syst√®me de recommandation e-commerce utilisant Apache Spark. Le processus comprend deux composants principaux : un mod√®le de segmentation client bas√© sur l'approche RFM (Recency, Frequency, Monetary) et un syst√®me de recommandation collaborative utilisant l'algorithme ALS (Alternating Least Squares).

## Architecture du syst√®me d'entra√Ænement

### Classe principale : EcommerceModelTrainer

Le syst√®me d'entra√Ænement est encapsul√© dans la classe `EcommerceModelTrainer` qui orchestre l'ensemble du processus. Cette classe centralise la gestion de la session Spark, la pr√©paration des donn√©es et l'entra√Ænement des mod√®les.

### Configuration Spark

La m√©thode `_create_spark_session()` initialise la session Spark avec une configuration optimis√©e :

```python
SparkSession.builder \
    .appName("E-commerce Model Training") \
    .config("spark.driver.memory", "8g") \
    .config("spark.executor.memory", "8g") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .master("local[*]")
```

Cette configuration alloue 8 Go de m√©moire aux processus driver et executor, active l'optimisation adaptative des requ√™tes SQL et utilise la s√©rialisation Kryo pour am√©liorer les performances.

## Fonctions de pr√©paration des donn√©es

### prepare_user_features()
**But** : Calculer les caract√©ristiques comportementales et RFM pour chaque utilisateur

Cette fonction transforme les donn√©es brutes d'√©v√©nements en m√©triques utilisateur exploitables :

```python
# Calcul de la date de r√©f√©rence (date maximale dans les donn√©es)
max_date = cleaned_df.select(max("event_time")).first()[0]

# Calcul des m√©triques RFM et comportementales
user_features = cleaned_df.groupBy("user_id").agg(
    # Recency: nombre de jours depuis la derni√®re activit√©
    datediff(lit(max_date), max("event_time")).alias("recency"),
    
    # Frequency: nombre de jours distincts d'activit√©
    countDistinct(col("event_time").cast("date")).alias("frequency"),
    
    # Monetary: montant total des achats
    sum(when(col("event_type") == "purchase", col("price")).otherwise(0)).alias("monetary"),
    
    # M√©triques comportementales
    count("*").alias("total_events"),
    sum(when(col("event_type") == "view", 1).otherwise(0)).alias("views"),
    sum(when(col("event_type") == "cart", 1).otherwise(0)).alias("carts"),
    sum(when(col("event_type") == "purchase", 1).otherwise(0)).alias("purchases")
)

# Calcul des taux de conversion
user_features = user_features.withColumn(
    "conversion_rate",
    when(col("views") > 0, col("purchases") / col("views")).otherwise(0)
).withColumn(
    "cart_abandonment",
    when(col("carts") > 0, (col("carts") - col("purchases")) / col("carts")).otherwise(0)
)
```

**Sortie** : DataFrame avec 15+ caract√©ristiques par utilisateur

### prepare_product_features()  
**But** : Agr√©ger les m√©triques de performance et popularit√© par produit

Cette fonction analyse l'engagement des utilisateurs avec chaque produit :

```python
# Calcul des m√©triques agr√©g√©es par produit
product_features = cleaned_df.groupBy("product_id").agg(
    count("*").alias("total_events"),
    sum(when(col("event_type") == "view", 1).otherwise(0)).alias("total_views"),
    sum(when(col("event_type") == "cart", 1).otherwise(0)).alias("total_carts"),
    sum(when(col("event_type") == "purchase", 1).otherwise(0)).alias("total_purchases"),
    countDistinct("user_id").alias("unique_users"),
    avg("price").alias("avg_price"),
    first("category_code").alias("category_code"),
    first("brand").alias("brand")
)

# Nettoyage et extraction de la cat√©gorie principale
product_features = product_features.withColumn(
    "category",
    when(col("category_code").isNull(), "unknown")
    .otherwise(split(col("category_code"), r"\\.")[0])
).drop("category_code")

# Calcul du score de popularit√© am√©lior√©
product_features = product_features.withColumn(
    "enhanced_popularity_score",
    (col("total_purchases") * 3 + col("total_carts") * 2 + col("total_views")) / col("unique_users")
)
```

**Sortie** : DataFrame avec m√©triques d'engagement et m√©tadonn√©es par produit

### prepare_recommendation_data()
**But** : Pr√©parer et indexer les donn√©es pour l'algorithme ALS

Cette fonction transforme les donn√©es pour le syst√®me de recommandation :

```python
# Indexation des utilisateurs et produits
user_indexer = StringIndexer(
    inputCol="user_id",
    outputCol="user_idx",
    handleInvalid="keep"
)

product_indexer = StringIndexer(
    inputCol="product_id",
    outputCol="product_idx",
    handleInvalid="keep"
)

# Pipeline d'indexation
indexer_pipeline = Pipeline(stages=[user_indexer, product_indexer])
indexer_model = indexer_pipeline.fit(cleaned_df)
indexed_df = indexer_model.transform(cleaned_df)

# Pr√©paration des donn√©es d'interaction avec scores implicites
interaction_data = indexed_df.filter(
    col("event_type").isin(["view", "cart", "purchase"])
).withColumn(
    "rating",
    when(col("event_type") == "view", 1.0)
    .when(col("event_type") == "cart", 3.0)
    .when(col("event_type") == "purchase", 5.0)
    .otherwise(0.0)
).groupBy("user_idx", "product_idx").agg(
    sum("rating").alias("rating"),
    count("*").alias("interaction_count")
).withColumn(
    "rating",
    # Normalisation du rating bas√©e sur le nombre d'interactions
    when(col("rating") > 10, 10.0).otherwise(col("rating"))
)
```

**Sortie** : Donn√©es d'interaction index√©es et pipeline de transformation

## Mod√®le de segmentation client (K-Means)

### Pr√©paration des caract√©ristiques utilisateur

Le syst√®me calcule automatiquement les m√©triques RFM et comportementales pour chaque utilisateur :

- **Recency** : Nombre de jours depuis la derni√®re activit√©
- **Frequency** : Nombre de jours distincts d'activit√© 
- **Monetary** : Montant total des achats
- **M√©triques comportementales** : Vues, ajouts au panier, achats, suppressions
- **Diversit√©** : Nombre de produits, cat√©gories et marques uniques consult√©s
- **Taux de conversion** : Ratio achats/vues
- **Taux d'abandon panier** : Ratio (paniers - achats)/paniers

### train_user_segmentation_model()
**But** : Entra√Æner le mod√®le de clustering K-Means pour la segmentation client

Cette fonction impl√©mente un pipeline complet de segmentation :

```python
# S√©lection des caract√©ristiques pour le clustering
feature_cols = [
    "recency", "frequency", "monetary", "conversion_rate",
    "cart_abandonment", "unique_products", "unique_categories", "avg_price"
]

# Remplacement des valeurs nulles par 0
for col_name in feature_cols:
    user_features = user_features.withColumn(col_name,
                                           when(col(col_name).isNull(), 0.0).otherwise(
                                               col(col_name).cast(DoubleType())))

# Assemblage des caract√©ristiques
assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features_raw"
)

# Normalisation des caract√©ristiques
scaler = StandardScaler(
    inputCol="features_raw",
    outputCol="features",
    withStd=True,
    withMean=True
)

# Mod√®le K-means
kmeans = KMeans(
    featuresCol="features",
    predictionCol="segment_id",
    k=5,  # 5 segments utilisateur
    seed=42,
    maxIter=20
)

# Pipeline de preprocessing et clustering
pipeline = Pipeline(stages=[assembler, scaler, kmeans])
model = pipeline.fit(user_features)

# √âvaluation du clustering
evaluator = ClusteringEvaluator(
    featuresCol="features",
    predictionCol="segment_id",
    metricName="silhouette"
)
silhouette_score = evaluator.evaluate(segmented_users)
```

**Sortie** : Mod√®le K-Means entra√Æn√© et utilisateurs segment√©s avec noms descriptifs

### _assign_segment_names()
**But** : Attribuer automatiquement des noms m√©tier aux segments num√©riques

Cette fonction utilitaire analyse les caract√©ristiques moyennes de chaque segment et applique une logique m√©tier :

```python
# Calcul des moyennes par segment
segment_stats = segmented_users.groupBy("segment_id").agg(
    avg("recency").alias("avg_recency"),
    avg("frequency").alias("avg_frequency"),
    avg("monetary").alias("avg_monetary"),
    avg("conversion_rate").alias("avg_conversion"),
    count("*").alias("segment_size")
).collect()

# Logique de nommage bas√©e sur les caract√©ristiques RFM
for row in segment_stats:
    segment_id = row.segment_id
    recency = row.avg_recency
    frequency = row.avg_frequency
    monetary = row.avg_monetary
    conversion = row.avg_conversion

    if monetary > 500 and frequency > 10 and recency < 30:
        segment_name = "VIP_Customers"
    elif frequency > 5 and recency < 60:
        segment_name = "Loyal_Customers"
    elif recency < 30 and conversion > 0.1:
        segment_name = "Active_Buyers"
    elif recency > 60 and frequency < 3:
        segment_name = "Dormant_Users"
    else:
        segment_name = "Casual_Browsers"

# Application du mapping
mapping_expr = create_map([lit(x) for x in chain.from_iterable(segment_mapping.items())])
segmented_users = segmented_users.withColumn(
    "segment",
    mapping_expr[col("segment_id")]
)
```

- **VIP_Customers** : Monetary > 500‚Ç¨, Frequency > 10 jours, Recency < 30 jours
- **Loyal_Customers** : Frequency > 5 jours, Recency < 60 jours  
- **Active_Buyers** : Recency < 30 jours, Conversion > 10%
- **Dormant_Users** : Recency > 60 jours, Frequency < 3 jours
- **Casual_Browsers** : Autres profils

### √âvaluation du clustering

La qualit√© du clustering est mesur√©e par le score de silhouette, qui √©value la coh√©sion intra-cluster et la s√©paration inter-cluster.

## Syst√®me de recommandation collaborative (ALS)

### Pr√©paration des donn√©es d'interaction

Le syst√®me transforme les √©v√©nements utilisateur en scores d'interaction implicites :

- **Vues** : Score de 1.0
- **Ajouts au panier** : Score de 3.0  
- **Achats** : Score de 5.0

Les interactions multiples sont agr√©g√©es et normalis√©es avec un plafond √† 10.0 pour √©viter les biais.

### Indexation des entit√©s

Un pipeline d'indexation convertit les identifiants string en indices num√©riques requis par ALS :

```python
user_indexer = StringIndexer(inputCol="user_id", outputCol="user_idx")
product_indexer = StringIndexer(inputCol="product_id", outputCol="product_idx")
```

### train_als_model()
**But** : Entra√Æner le mod√®le de recommandation collaborative ALS

Cette fonction impl√©mente l'algorithme de factorisation matricielle :

```python
# Division train/test
train_data, test_data = interaction_data.randomSplit([0.8, 0.2], seed=42)

# Configuration du mod√®le ALS
als = ALS(
    userCol="user_idx",
    itemCol="product_idx",
    ratingCol="rating",
    nonnegative=True,
    implicitPrefs=True,  # Donn√©es implicites
    rank=50,  # Nombre de facteurs latents
    maxIter=15,  # Nombre d'it√©rations
    regParam=0.1,  # R√©gularisation
    alpha=1.0,  # Param√®tre de confiance pour les donn√©es implicites
    coldStartStrategy="drop",
    seed=42
)

# Entra√Ænement
als_model = als.fit(train_data)

# √âvaluation sur les donn√©es de test
predictions = als_model.transform(test_data)
evaluator = RegressionEvaluator(
    metricName="rmse",
    labelCol="rating",
    predictionCol="prediction"
)
rmse = evaluator.evaluate(predictions.filter(col("prediction").isNotNull()))

# G√©n√©ration de recommandations pour tous les utilisateurs (cache)
all_users = interaction_data.select("user_idx").distinct()
user_recommendations = als_model.recommendForUserSubset(all_users, 10)
```

**Sortie** : Mod√®le ALS entra√Æn√© et recommandations pr√©-calcul√©es

## Fonctions utilitaires et de gestion

### _find_latest_parquet_file()
**But** : Identifier automatiquement le fichier de donn√©es le plus r√©cent

Cette fonction utilitaire parcourt un r√©pertoire pour trouver le fichier avec le timestamp le plus r√©cent :

```python
def _find_latest_parquet_file(self, directory_path, prefix):
    full_directory_path = os.path.abspath(directory_path).replace("\\", "/")
    latest_file = None
    latest_timestamp = None
    
    try:
        entries = os.listdir(full_directory_path)
        # Filtrer les entr√©es qui sont des r√©pertoires et qui commencent par le pr√©fixe
        matching_entries = [entry for entry in entries 
                          if os.path.isdir(os.path.join(full_directory_path, entry)) 
                          and entry.startswith(prefix)]

        for entry_name in matching_entries:
            # Extraire le timestamp du nom du r√©pertoire (format YYYYMMDD_HHMMSS)
            timestamp_match = re.search(r'\d{8}_\d{6}', entry_name)
            if timestamp_match:
                timestamp_str = timestamp_match.group(0)
                timestamp = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
                if latest_timestamp is None or timestamp > latest_timestamp:
                    latest_timestamp = timestamp
                    latest_file = os.path.join(full_directory_path, entry_name)
    except Exception as e:
        logger.error(f"Erreur lors de la recherche du fichier {prefix}: {e}")
    
    return latest_file
```

**Utilit√©** : Permet la reprise automatique d'entra√Ænement sur les donn√©es les plus fra√Æches

### train_all_models()
**But** : Orchestrer l'ensemble du processus d'entra√Ænement

Cette fonction ma√Ætresse coordonne tout le pipeline d'entra√Ænement :

```python
def train_all_models(self, cleaned_data_path, processed_data_path="./data/processed/parquet/"):
    logger.info("D√©but de l'entra√Ænement de tous les mod√®les...")

    # Chargement des donn√©es nettoy√©es
    cleaned_df = self.spark.read.parquet(cleaned_data_path)
    
    # 1. Chargement intelligent des caract√©ristiques utilisateur
    user_behavior_path = self._find_latest_parquet_file(processed_data_path, "user_behavior_")
    if user_behavior_path:
        user_features = self.spark.read.parquet(user_behavior_path)
    else:
        # Fallback vers la pr√©paration si le fichier est manquant
        user_features = self.prepare_user_features(cleaned_df)

    # 2. Entra√Ænement du mod√®le de segmentation
    segmentation_model, segmented_users = self.train_user_segmentation_model(user_features)

    # 3. Pr√©paration des donn√©es pour recommandation (Indexation)
    interaction_data, indexer_model = self.prepare_recommendation_data(cleaned_df)

    # 4. Chargement des caract√©ristiques produits avec fallback
    product_features_path = self._find_latest_parquet_file(processed_data_path, "product_data_")
    if product_features_path:
        product_features = self.spark.read.parquet(product_features_path)
    else:
        product_features = self.prepare_product_features(cleaned_df)

    # 5. Entra√Ænement du mod√®le ALS
    als_model = self.train_als_model(interaction_data)

    # 6. Sauvegarde des m√©tadonn√©es
    metadata = {
        "training_date": datetime.now().isoformat(),
        "total_users": cleaned_df.select("user_id").distinct().count(),
        "total_products": cleaned_df.select("product_id").distinct().count(),
        "total_interactions": cleaned_df.count(),
        "segments_count": segmented_users.select("segment_id").distinct().count()
    }
```

**Flux d'ex√©cution** :
```
Donn√©es nettoy√©es ‚Üí Caract√©ristiques utilisateur ‚Üí Segmentation K-Means
                 ‚Üì
Indexation ‚Üí Donn√©es d'interaction ‚Üí Mod√®le ALS ‚Üí Recommandations
                 ‚Üì
Sauvegarde des mod√®les et m√©tadonn√©es
```

**Sortie** : Dictionnaire contenant tous les mod√®les entra√Æn√©s et leurs m√©tadonn√©es

## Fonction principale et point d'entr√©e

### main()
**But** : Point d'entr√©e principal pour l'ex√©cution via spark-submit

Cette fonction configure l'environnement d'ex√©cution et lance le processus complet :

```python
def main():
    # Configuration des chemins
    CLEANED_DATA_PATH = "./data/processed/parquet/cleaned_data_*.parquet" 
    MODELS_OUTPUT_PATH = "./models"
    PROCESSED_DATA_PATH = "./data/processed/parquet/" 
    
    # Cr√©ation du trainer
    trainer = EcommerceModelTrainer(models_output_path=MODELS_OUTPUT_PATH)

    try:
        # Entra√Ænement de tous les mod√®les
        models = trainer.train_all_models(CLEANED_DATA_PATH, PROCESSED_DATA_PATH)
        logger.info("Tous les mod√®les ont √©t√© entra√Æn√©s et sauvegard√©s avec succ√®s!")

    except Exception as e:
        logger.error(f"Erreur lors de l'entra√Ænement des mod√®les: {str(e)}")
        raise
    finally:
        trainer.stop()

if __name__ == "__main__":
    main()
```

**Usage** : `spark-submit model_training.py` pour lancement en production

### stop()
**But** : Fermer proprement la session Spark et lib√©rer les ressources

```python
def stop(self):
    """Arr√™te la session Spark"""
    self.spark.stop()
```

Cette m√©thode garantit la lib√©ration correcte des ressources Spark, √©vitant les fuites m√©moire et les processus orphelins.

## Gestion de la persistance et m√©tadonn√©es

### Sauvegarde des mod√®les

Tous les mod√®les entra√Æn√©s sont sauvegard√©s de mani√®re persistante :

- **Mod√®le K-Means** : Pipeline complet incluant le pr√©processing
- **Mod√®le ALS** : Mod√®le de factorisation matricielle
- **Pipeline d'indexation** : Mappings utilisateur/produit vers indices
- **Segments utilisateur** : Assignations de segments avec m√©triques RFM
- **Recommandations pr√©-calcul√©es** : Top 10 recommandations par utilisateur

### M√©tadonn√©es d'entra√Ænement

Le syst√®me sauvegarde automatiquement les m√©tadonn√©es d'entra√Ænement :

```json
{
  "training_date": "2024-12-XX",
  "total_users": 123456,
  "total_products": 7890,
  "total_interactions": 9876543,
  "segments_count": 5
}
```

## Optimisations et robustesse

### Gestion des donn√©es manquantes

Le syst√®me impl√©mente plusieurs strat√©gies de robustesse :

- Remplacement des valeurs nulles par 0.0 dans les caract√©ristiques num√©riques
- Gestion des cat√©gories manquantes avec une valeur "unknown"
- Strat√©gie "handleInvalid=keep" pour les nouveaux identifiants

### Chargement intelligent des donn√©es

La fonction `_find_latest_parquet_file()` identifie automatiquement les fichiers de donn√©es les plus r√©cents bas√©s sur des timestamps, permettant une reprise d'entra√Ænement sur les donn√©es les plus √† jour.

### Fallback automatique

En cas d'√©chec du chargement des donn√©es pr√©trait√©es, le syst√®me bascule automatiquement vers un calcul √† partir des donn√©es nettoy√©es de base, assurant la continuit√© du processus d'entra√Ænement.

## M√©triques de performance

### Clustering
- **Score de silhouette** : Mesure la qualit√© de la s√©paration des segments
- **Distribution des segments** : Taille et caract√©ristiques moyennes par segment

### Recommandation
- **RMSE** : Erreur quadratique moyenne sur l'ensemble de test
- **Couverture** : Pourcentage de produits recommand√©s
- **Temps d'entra√Ænement** : Performance du processus d'apprentissage

## Architecture de d√©ploiement

Le syst√®me d'entra√Ænement est con√ßu pour √™tre ex√©cut√© via `spark-submit`, permettant une scalabilit√© horizontale sur un cluster Spark en production. La structure modulaire facilite la maintenance et l'extension avec de nouveaux algorithmes.
# Syst√®me de Recommandation en Temps R√©el avec Spark Streaming

## Objectif

Cette section pr√©sente l'impl√©mentation d'un syst√®me de recommandation capable de traiter des donn√©es clients en temps r√©el et de g√©n√©rer des recommandations instantan√©es. Le syst√®me utilise Apache Spark Structured Streaming pour consommer un flux de donn√©es d'interactions utilisateur-produit et applique un algorithme de recommandation hybride combinant filtrage collaboratif et popularit√©.

## Architecture du Streaming

### Configuration Spark
```python
def _create_spark_session(self):
    return SparkSession.builder \
        .appName("Streaming Recommendation System") \
        .config("spark.driver.memory", "8g") \
        .config("spark.executor.memory", "8g") \
        .config("spark.sql.streaming.checkpointLocation", "./checkpoint") \
        .master("local[*]") \
        .getOrCreate()
```

La session Spark est configur√©e avec :
- **Checkpointing** : Pour la tol√©rance aux pannes et la reprise apr√®s incident
- **M√©moire optimis√©e** : 8GB allou√©s au driver et aux executors pour traiter les donn√©es en m√©moire
- **Mode local** : Utilisation de tous les c≈ìurs disponibles

### Lecture du Flux de Donn√©es

```python
stream_df = self.spark.readStream \
    .format("csv") \
    .option("header", "true") \
    .schema(schema) \
    .option("maxFilesPerTrigger", 1) \
    .load(input_path)
```

**Points cl√©s :**
- **Sch√©ma d√©fini** : √âvite l'inf√©rence automatique pour de meilleures performances
- **maxFilesPerTrigger** : Contr√¥le le d√©bit de traitement (1 fichier par batch)
- **Format CSV** : Simulation de donn√©es temps r√©el √† partir de fichiers

### Fen√™trage Temporel

```python
windowed_activity = stream_df \
    .withWatermark("event_time", "10 minutes") \
    .groupBy(
        window(col("event_time"), "5 minutes"),
        "user_id", "product_id", "event_type", "category_code", "brand", "price"
    ).agg(count("*").alias("event_count"))
```

**M√©canisme de fen√™trage :**
- **Watermark** : Tol√®re un retard maximum de 10 minutes pour les √©v√©nements
- **Fen√™tre glissante** : Agr√©gation sur des intervalles de 5 minutes
- **Agr√©gation** : Comptage des interactions par utilisateur/produit

## Algorithme de Recommandation

### Analyse des Interactions Utilisateur

```python
user_interactions = user_activity_df_processed.groupBy("user_id", "product_id") \
    .agg(
        count("*").alias("interaction_count"),
        spark_sum(when(col("event_type") == "view", 1).otherwise(0)).alias("views"),
        spark_sum(when(col("event_type") == "cart", 1).otherwise(0)).alias("carts"),
        spark_sum(when(col("event_type") == "purchase", 1).otherwise(0)).alias("purchases")
    )
```

**Agr√©gation des comportements :**
- Comptage total des interactions
- Distinction par type d'√©v√©nement (vue, ajout panier, achat)
- Base pour le calcul des pr√©f√©rences utilisateur

### Extraction des Pr√©f√©rences Cat√©gorielles

```python
user_categories = user_activity_df_processed.filter(col("category_code").isNotNull()) \
    .groupBy("user_id", "main_category") \
    .agg(count("*").alias("category_interest"))

top_categories = user_categories.withColumn(
    "rank", row_number().over(Window.partitionBy("user_id").orderBy(desc("category_interest")))
).filter(col("rank") <= 3)
```

**Identification des centres d'int√©r√™t :**
- Extraction de la cat√©gorie principale depuis le code complet
- Classement des cat√©gories par fr√©quence d'interaction
- S√©lection des 3 cat√©gories les plus populaires par utilisateur

### G√©n√©ration des Recommandations

```python
recommendations = top_categories.join(
    broadcast(self.product_popularity.filter(col("total_purchases") > 0)),
    on=top_categories.main_category == self.product_popularity.main_category_code, 
    how="inner"
).withColumn(
    "rec_score",
    col("category_interest") * col("total_purchases") / 100
).withColumn(
    "rank", row_number().over(Window.partitionBy("user_id").orderBy(desc("rec_score")))
)
```

**Algorithme hybride :**
- **Filtrage collaboratif** : Bas√© sur les pr√©f√©rences cat√©gorielles de l'utilisateur
- **Popularit√© globale** : Pond√©ration par le nombre total d'achats du produit
- **Score de recommandation** : Formule combinant int√©r√™t personnel et popularit√©
- **Broadcast join** : Optimisation pour les donn√©es de r√©f√©rence (produits populaires)

### M√©canisme de Fallback

```python
if recommendations.isEmpty():
    logger.warning("Collaborative recommendations are empty. Falling back to popular products.")
    unique_users_df = user_activity_df_processed.select("user_id").distinct()
    top_popular_products = self.product_popularity
    
    fallback_recommendations = unique_users_df.crossJoin(top_popular_products.limit(10))
    recommendations = fallback_recommendations.select(
        col(unique_users_df.user_id).alias("user_id"),
        col(top_popular_products.product_id).alias("product_id"),
        lit(0.1).alias("rec_score"),
        col(top_popular_products.main_category_code).alias("recommended_category_code"),
        col(top_popular_products.main_brand).alias("main_brand"),
        col(top_popular_products.avg_price).alias("avg_price")
    ).withColumn("recommendation_type", lit("streaming_popular_fallback"))
```

**Strat√©gie de repli :**
- **D√©tection automatique** : V√©rification si les recommandations collaboratives sont vides
- **Recommandations populaires** : Suggestion des 10 produits les plus vendus
- **Cross-join** : Attribution des m√™mes produits populaires √† tous les nouveaux utilisateurs

## Persistance et Monitoring

### √âcriture vers InfluxDB

```python
def _write_to_influxdb(self, df, measurement_name):
    def write_batch(batch_df, batch_id):
        try:
            client = InfluxDBClient(url=INFLUXDB_CONFIG["url"], 
                                  token=INFLUXDB_CONFIG["token"])
            write_api = client.write_api(write_options=SYNCHRONOUS)
            
            points = []
            for row in batch_df.collect():
                point = Point(measurement_name)
                # Ajout des champs et tags selon le type de donn√©es
                points.append(point)
            
            write_api.write(bucket=INFLUXDB_CONFIG["bucket"], record=points)
        except Exception as e:
            logger.error(f"Error writing to InfluxDB: {e}")
```

**Monitoring temps r√©el :**
- **M√©triques d'√©v√©nements** : Suivi des interactions utilisateur
- **M√©triques de recommandations** : Performance et qualit√© des suggestions
- **Base de donn√©es temporelle** : InfluxDB pour l'analyse des tendances

### Sauvegarde des R√©sultats

```python
# Sauvegarde incr√©mentale des recommandations
recommendations.write.mode("append").option("header", "true").csv(output_csv_path)
```

**Persistance des recommandations :**
- **Mode append** : Accumulation des r√©sultats de chaque batch
- **Format CSV** : Facilite l'analyse post-traitement
- **Horodatage implicite** : Tra√ßabilit√© temporelle des recommandations

## Configuration du Pipeline

### Traitement par Micro-Batch

```python
query = windowed_activity.writeStream \
    .outputMode("update") \
    .foreachBatch(process_batch) \
    .trigger(processingTime='60 seconds') \
    .start()
```

**Param√®tres de streaming :**
- **Mode update** : Seules les lignes modifi√©es sont transmises
- **Trigger de 60 secondes** : √âquilibre entre latence et throughput
- **ForeachBatch** : Traitement personnalis√© de chaque micro-batch

### Gestion d'Erreurs et Tol√©rance aux Pannes

```python
try:
    query.awaitTermination()
except KeyboardInterrupt:
    logger.info("Stopping streaming...")
    query.stop()
    self.spark.stop()
```

**Robustesse du syst√®me :**
- **Arr√™t gracieux** : Gestion des interruptions manuelles
- **Logging d√©taill√©** : Tra√ßabilit√© des erreurs et performances
- **Checkpointing automatique** : Reprise apr√®s panne

## R√©sultats et Performance

Le syst√®me traite les donn√©es en temps r√©el avec les caract√©ristiques suivantes :

- **Latence** : Recommandations g√©n√©r√©es en moins de 60 secondes apr√®s r√©ception des donn√©es
- **D√©bit** : Capable de traiter des milliers d'interactions par batch
- **Scalabilit√©** : Architecture distribu√©e pr√™te pour le passage √† l'√©chelle
- **Qualit√©** : Algorithme hybride combinant personnalisation et popularit√©

Cette impl√©mentation d√©montre la capacit√© de Spark Structured Streaming √† alimenter des syst√®mes de recommandation temps r√©el, essentiels pour les applications e-commerce modernes n√©cessitant une personnalisation instantan√©e.
# Visualisation des Recommandations √† Froid

Cette section pr√©sente l'analyse et la visualisation des recommandations g√©n√©r√©es par notre syst√®me de recommandation Spark. Les recommandations produites sont stock√©es dans deux formats compl√©mentaires pour faciliter l'analyse et le monitoring.

## Architecture de Stockage des Recommandations

Les recommandations g√©n√©r√©es par le mod√®le Spark sont distribu√©es via deux canaux :

- **Export CSV** : Sauvegarde dans un fichier `merged_recommendations.csv` pour analyse batch
- **Stockage InfluxDB** : Envoi en temps r√©el pour monitoring et analyse temporelle

### Structure de la Table InfluxDB

Les recommandations sont stock√©es dans InfluxDB avec la structure suivante :
![Capture d'√©cran 2025-05-26 182734.png](images%2FCapture%20d%27%C3%A9cran%202025-05-26%20182734.png)
![Capture d'√©cran 2025-05-26 195331.png](images%2FCapture%20d%27%C3%A9cran%202025-05-26%20195331.png)

*Capture d'√©cran √† ins√©rer : Structure de la table des recommandations dans InfluxDB*

Cette base de donn√©es temporelle permet un suivi en temps r√©el des performances du syst√®me de recommandation et facilite la cr√©ation de dashboards de monitoring.

## Analyse des Donn√©es de Recommandation

Le script `visualize_recommendations.py` charge et analyse les donn√©es depuis le fichier CSV merg√©. Les donn√©es analys√©es sont identiques √† celles stock√©es dans InfluxDB, garantissant la coh√©rence entre les analyses batch et temps r√©el.

### Chargement et Traitement des Donn√©es

```python
# Chargement du fichier CSV consolid√©
df = pd.read_csv("./results/merged_recommendations.csv")

# G√©n√©ration automatique de 8 visualisations principales
# - Distribution des scores
# - Top cat√©gories et marques
# - Relations prix/score
# - Analyse par utilisateur
```

## R√©sultats des Visualisations

### Distribution des Scores de Recommandation
![aed12cf4-5991-415d-9a32-867d28ad3301.png](images%2Faed12cf4-5991-415d-9a32-867d28ad3301.png)

La distribution des scores montre une concentration importante des recommandations autour de scores faibles (0-100), avec quelques pics isol√©s √† des scores plus √©lev√©s. Cette distribution sugg√®re que la majorit√© des recommandations sont de qualit√© standard, avec quelques recommandations exceptionnelles.

### Top 10 Cat√©gories Recommand√©es
![a94eb5fa-7842-40d8-85b0-b79360b4ea29.png](images%2Fa94eb5fa-7842-40d8-85b0-b79360b4ea29.png)
L'√©lectronique domine largement avec plus de 400K recommandations, suivie par les appareils √©lectrom√©nagers (~90K). Cette r√©partition refl√®te probablement les habitudes d'achat des utilisateurs et la disponibilit√© des produits dans ces cat√©gories.

### Top 10 Marques Recommand√©es

![8db3703b-6230-415c-a389-1a9c07a1798f.png](images%2F8db3703b-6230-415c-a389-1a9c07a1798f.png)

Apple et Samsung se positionnent comme les marques les plus recommand√©es, avec respectivement ~240K et ~200K recommandations. Cette dominance s'explique par leur pr√©sence forte dans les cat√©gories √©lectroniques populaires.

### Relation Score vs Prix Moyen
![980174d7-e8fa-413f-8903-020be2726a8b.png](images%2F980174d7-e8fa-413f-8903-020be2726a8b.png)

Le graphique de dispersion r√©v√®le une absence de corr√©lation claire entre le prix et le score de recommandation. Les produits √† tous niveaux de prix peuvent obtenir des scores √©lev√©s, sugg√©rant que l'algorithme privil√©gie la pertinence utilisateur plut√¥t que la valeur mon√©taire.

### Performance par Cat√©gorie et Marque

![f94a8e11-de91-4fbc-ab0e-b0a49bff4448.png](images%2Ff94a8e11-de91-4fbc-ab0e-b0a49bff4448.png)

L'√©lectronique obtient le score moyen le plus √©lev√© (~15), confirmant la qualit√© des recommandations dans cette cat√©gorie dominante.

![aa35d47d-6b94-49e3-8caa-308e4dca96f4.png](images%2Faa35d47d-6b94-49e3-8caa-308e4dca96f4.png)

Samsung pr√©sente le score moyen le plus √©lev√© (~18), suivi d'Apple (~10), sugg√©rant une forte ad√©quation entre ces marques et les pr√©f√©rences utilisateurs.

### Distribution des Prix
![26d3b002-d87b-4e3d-b9f2-ed718171487b.png](images%2F26d3b002-d87b-4e3d-b9f2-ed718171487b.png)
La distribution des prix montre plusieurs pics distincts, sugg√©rant des gammes de prix privil√©gi√©es par les utilisateurs. La pr√©sence de pics autour de 150‚Ç¨, 250‚Ç¨ et 450‚Ç¨ indique des segments de march√© bien d√©finis.

## Synth√®se

Les visualisations r√©v√®lent un syst√®me de recommandation √©quilibr√© avec :
- Une concentration sur l'√©lectronique et les grandes marques
- Des scores de recommandation ind√©pendants du prix
- Une distribution √©quitable entre utilisateurs
- Des segments de prix clairement identifi√©s

Ces analyses, disponibles en temps r√©el via InfluxDB et en batch via les exports CSV, permettent un monitoring continu de la qualit√© des recommandations.