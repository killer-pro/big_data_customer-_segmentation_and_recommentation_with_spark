{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3d28e686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T18:23:31.308886Z",
     "start_time": "2025-05-18T18:23:31.268908Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, when, desc, avg, min, udf, percent_rank,countDistinct,stddev,lit,create_map,coalesce\n",
    ")\n",
    "from pyspark.sql.types import ( StringType, FloatType\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, StandardScaler, StringIndexer, Bucketizer\n",
    ")\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator, RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.window import Window\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"modelisation.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paramètres globaux\n",
    "DATA_DIR = \"./data/processed/parquet\"\n",
    "MODELS_DIR = \"./models\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "\n",
    "# Création des répertoires de sortie s'ils n'existent pas\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Création de la session Spark\n",
    "def create_spark_session():\n",
    "    \"\"\"Crée et retourne une session Spark configurée avec optimisations\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"E-commerce Models\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"1g\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"24\") \\\n",
    "        .config(\"spark.default.parallelism\", \"8\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "        .config(\"spark.python.worker.reuse\", \"true\") \\\n",
    "        .config(\"spark.python.worker.memory\", \"2g\") \\\n",
    "        .config(\"spark.network.timeout\", \"800s\") \\\n",
    "        .config(\"spark.locality.wait\", \"10s\") \\\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\") \\\n",
    "        .master(\"local[8]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ceb6f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_file(directory, pattern):\n",
    "    \"\"\"Trouve le fichier le plus récent dans le répertoire correspondant au pattern\"\"\"\n",
    "    files = [f for f in os.listdir(directory) if pattern in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    \n",
    "    latest_file = None\n",
    "    latest_time = 0\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        file_time = os.path.getmtime(file_path)\n",
    "        if file_time > latest_time:\n",
    "            latest_time = file_time\n",
    "            latest_file = file\n",
    "            \n",
    "    return latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d26935f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rfm_segmentation(user_df):\n",
    "    \"\"\"Prépare les données pour la segmentation RFM avec améliorations\"\"\"\n",
    "    logger.info(\"Préparation des données pour segmentation RFM\")\n",
    "    \n",
    "    # Filtrer les utilisateurs avec au moins une interaction et mettre en cache\n",
    "    rfm_df = user_df.filter(col(\"nb_events\") > 0).cache()\n",
    "    \n",
    "    # Convertir les valeurs manquantes/nulles en 0 pour les métriques RFM\n",
    "    rfm_df = rfm_df.fillna({\n",
    "        \"recency\": 30,  # Valeur max si jamais vu\n",
    "        \"frequency\": 0,  # Pas d'achats\n",
    "        \"monetary\": 0    # Pas de dépenses\n",
    "    })\n",
    "    \n",
    "    # Créer des buckets pour les métriques RFM - version simplifiée pour éviter les erreurs\n",
    "    # Utilisation de quantiles fixes pour plus de stabilité\n",
    "    \n",
    "    # Récence (inversée: plus petit = meilleur)\n",
    "    recency_splits = [0, 10, 20, 30, float('inf')]\n",
    "    \n",
    "    # Fréquence et Monétaire - calcul dynamique sécurisé\n",
    "    freq_stats = rfm_df.select(\n",
    "        avg(\"frequency\").alias(\"avg_freq\"),\n",
    "        stddev(\"frequency\").alias(\"std_freq\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    # Si pas de variance, utiliser des splits fixes\n",
    "    if freq_stats[\"std_freq\"] is None or freq_stats[\"std_freq\"] == 0:\n",
    "        frequency_splits = [0, 0.5, 1.5, 2.5, float('inf')]\n",
    "    else:\n",
    "        avg_freq = freq_stats[\"avg_freq\"] or 0\n",
    "        frequency_splits = [0, avg_freq * 0.5, avg_freq, avg_freq * 1.5, float('inf')]\n",
    "    \n",
    "    # Même logique pour monetary\n",
    "    monetary_stats = rfm_df.select(\n",
    "        avg(\"monetary\").alias(\"avg_monetary\"),\n",
    "        stddev(\"monetary\").alias(\"std_monetary\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    if monetary_stats[\"std_monetary\"] is None or monetary_stats[\"std_monetary\"] == 0:\n",
    "        monetary_splits = [0, 100, 500, 1000, float('inf')]\n",
    "    else:\n",
    "        avg_monetary = monetary_stats[\"avg_monetary\"] or 0\n",
    "        monetary_splits = [0, avg_monetary * 0.5, avg_monetary, avg_monetary * 1.5, float('inf')]\n",
    "    \n",
    "    logger.info(f\"Splits récence: {recency_splits}\")\n",
    "    logger.info(f\"Splits fréquence: {frequency_splits}\")\n",
    "    logger.info(f\"Splits monétaire: {monetary_splits}\")\n",
    "    \n",
    "    # Création des bucketizers\n",
    "    recency_bucketizer = Bucketizer(\n",
    "        splits=recency_splits, \n",
    "        inputCol=\"recency\", \n",
    "        outputCol=\"recency_score\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    frequency_bucketizer = Bucketizer(\n",
    "        splits=frequency_splits, \n",
    "        inputCol=\"frequency\", \n",
    "        outputCol=\"frequency_score\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    monetary_bucketizer = Bucketizer(\n",
    "        splits=monetary_splits, \n",
    "        inputCol=\"monetary\", \n",
    "        outputCol=\"monetary_score\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    # Appliquer les bucketizers\n",
    "    rfm_df = recency_bucketizer.transform(rfm_df)\n",
    "    rfm_df = frequency_bucketizer.transform(rfm_df)\n",
    "    rfm_df = monetary_bucketizer.transform(rfm_df)\n",
    "    \n",
    "    # Inverser le score de récence et normaliser tous les scores\n",
    "    rfm_df = rfm_df.withColumn(\"recency_score\", 5.0 - col(\"recency_score\"))\n",
    "    rfm_df = rfm_df.withColumn(\"recency_score\", \n",
    "                              when(col(\"recency_score\") < 1, 1)\n",
    "                              .when(col(\"recency_score\") > 5, 5)\n",
    "                              .otherwise(col(\"recency_score\")))\n",
    "    \n",
    "    # Normaliser frequency et monetary scores\n",
    "    for score_col in [\"frequency_score\", \"monetary_score\"]:\n",
    "        rfm_df = rfm_df.withColumn(score_col, col(score_col) + 1)\n",
    "        rfm_df = rfm_df.withColumn(score_col, \n",
    "                                  when(col(score_col) > 5, 5)\n",
    "                                  .otherwise(col(score_col)))\n",
    "    \n",
    "    # Calculer le statut actif\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"is_active\",\n",
    "        when(col(\"frequency\") > 0, 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Calcul du score RFM global\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"rfm_score\", \n",
    "        col(\"recency_score\") * 100 + col(\"frequency_score\") * 10 + col(\"monetary_score\")\n",
    "    )\n",
    "    \n",
    "    # Segmentation RFM robuste\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"rfm_segment\",\n",
    "        when(col(\"is_active\") == 0, \"Inactif\")\n",
    "        .when((col(\"recency_score\") >= 4) & (col(\"frequency_score\") >= 4) & (col(\"monetary_score\") >= 4), \"Champions\")\n",
    "        .when((col(\"recency_score\") >= 4) & (col(\"frequency_score\") >= 4), \"Loyal Customers\")\n",
    "        .when((col(\"recency_score\") >= 3) & (col(\"monetary_score\") >= 4), \"Big Spenders\")\n",
    "        .when((col(\"recency_score\") >= 3) & (col(\"frequency_score\") >= 3), \"Potential Loyalists\")\n",
    "        .when((col(\"recency_score\") <= 2) & (col(\"frequency_score\") >= 3), \"At Risk\")\n",
    "        .when((col(\"recency_score\") <= 2) & (col(\"frequency_score\") <= 2), \"Hibernating\")\n",
    "        .when((col(\"recency_score\") >= 4) & (col(\"frequency_score\") <= 2), \"New Customers\")\n",
    "        .when((col(\"recency_score\") >= 3) & (col(\"frequency_score\") <= 2), \"Need Attention\")\n",
    "        .otherwise(\"Others\")\n",
    "    )\n",
    "    \n",
    "    # Afficher la distribution des segments - utilisation collect() pour éviter les erreurs UDF\n",
    "    segment_distribution = rfm_df.groupBy(\"rfm_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "    \n",
    "    logger.info(\"Distribution des segments RFM:\")\n",
    "    for row in segment_distribution:\n",
    "        logger.info(f\"{row['rfm_segment']}: {row['count']}\")\n",
    "    \n",
    "    return rfm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5337b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_behavioral_clustering(user_df):\n",
    "    \"\"\"Prépare les données pour le clustering comportemental avec optimisations\"\"\"\n",
    "    logger.info(\"Préparation des données pour clustering comportemental\")\n",
    "    \n",
    "    # Sélection des features comportementales pertinentes\n",
    "    behavior_features = [\n",
    "        \"nb_events\", \"nb_views\", \"nb_carts\", \"nb_purchases\", \"nb_removes\",\n",
    "        \"avg_price_viewed\", \"avg_price_purchased\", \"nb_sessions\",\n",
    "        \"conversion_rate\", \"cart_abandonment\", \"engagement_days\"\n",
    "    ]\n",
    "    \n",
    "    # Filtrer et mettre en cache\n",
    "    clustering_df = user_df.filter(col(\"nb_events\") >= 2).cache()\n",
    "    logger.info(f\"Utilisateurs avec au moins 2 événements: {clustering_df.count()}\")\n",
    "    \n",
    "    # Remplacer les valeurs nulles par des zéros\n",
    "    clustering_df = clustering_df.na.fill({\n",
    "        feature: 0 for feature in behavior_features\n",
    "    })\n",
    "    \n",
    "    # Assembler les features en vecteurs\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=behavior_features,\n",
    "        outputCol=\"features_raw\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    clustering_df = assembler.transform(clustering_df)\n",
    "    \n",
    "    # Standardiser les features\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_raw\", \n",
    "        outputCol=\"features\",\n",
    "        withStd=True, \n",
    "        withMean=True\n",
    "    )\n",
    "    \n",
    "    # Pipeline pour le preprocessing\n",
    "    preprocessing_pipeline = Pipeline(stages=[scaler])\n",
    "    preprocessing_model = preprocessing_pipeline.fit(clustering_df)\n",
    "    clustering_df = preprocessing_model.transform(clustering_df)\n",
    "    \n",
    "    return clustering_df, behavior_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d9f4c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kmeans_model(df, feature_col=\"features\", k_values=range(2, 11)):\n",
    "    \"\"\"Entraîne et évalue plusieurs modèles K-means avec différentes valeurs de k\"\"\"\n",
    "    logger.info(\"Entraînement des modèles K-means\")\n",
    "    \n",
    "    # Liste pour stocker les résultats\n",
    "    silhouette_scores = []\n",
    "    models = {}\n",
    "    \n",
    "    # Évaluateur pour le clustering\n",
    "    evaluator = ClusteringEvaluator(\n",
    "        predictionCol=\"prediction\", \n",
    "        featuresCol=feature_col,\n",
    "        metricName=\"silhouette\"\n",
    "    )\n",
    "    \n",
    "    # Tester différentes valeurs de k\n",
    "    for k in k_values:\n",
    "        logger.info(f\"Essai avec k={k}\")\n",
    "        \n",
    "        try:\n",
    "            # Créer et entraîner le modèle\n",
    "            kmeans = KMeans(\n",
    "                k=k, \n",
    "                seed=42, \n",
    "                featuresCol=feature_col,\n",
    "                maxIter=20,\n",
    "                tol=1e-4\n",
    "            )\n",
    "            model = kmeans.fit(df)\n",
    "            \n",
    "            # Faire des prédictions\n",
    "            predictions = model.transform(df)\n",
    "            \n",
    "            # Évaluer le modèle\n",
    "            silhouette = evaluator.evaluate(predictions)\n",
    "            logger.info(f\"Silhouette pour k={k}: {silhouette}\")\n",
    "            \n",
    "            # Stocker les résultats\n",
    "            silhouette_scores.append(silhouette)\n",
    "            models[k] = model\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur avec k={k}: {str(e)}\")\n",
    "            silhouette_scores.append(-1)  # Score invalide\n",
    "            models[k] = None\n",
    "    \n",
    "    # Trouver la meilleure valeur de k en utilisant numpy\n",
    "    silhouette_array = np.array(silhouette_scores)\n",
    "    valid_indices = silhouette_array > -1  # Filtrer les scores invalides\n",
    "    \n",
    "    if not np.any(valid_indices):\n",
    "        raise ValueError(\"Aucun modèle valide trouvé\")\n",
    "    \n",
    "    # Trouver l'indice du meilleur score parmi les valides\n",
    "    best_idx = np.argmax(silhouette_array[valid_indices])\n",
    "    # Convertir l'indice local (parmi les valides) en indice global\n",
    "    global_indices = np.where(valid_indices)[0]\n",
    "    global_best_idx = global_indices[best_idx]\n",
    "    \n",
    "    best_k = list(k_values)[global_best_idx]\n",
    "    best_score = silhouette_scores[global_best_idx]\n",
    "    best_model = models[best_k]\n",
    "    \n",
    "    logger.info(f\"Meilleur modèle: k={best_k} avec silhouette={best_score}\")\n",
    "    \n",
    "    # Générer les prédictions avec le meilleur modèle\n",
    "    results = best_model.transform(df)\n",
    "    \n",
    "    # Afficher la distribution des clusters\n",
    "    logger.info(\"Distribution des clusters:\")\n",
    "    results.groupBy(\"prediction\").count().orderBy(\"prediction\").show()\n",
    "    \n",
    "    # Ajout de visualisation des scores silhouette\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(list(k_values), silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
    "        plt.xlabel('Nombre de clusters', fontsize=12)\n",
    "        plt.ylabel('Score Silhouette', fontsize=12)\n",
    "        plt.title('Optimisation du nombre de clusters - Score Silhouette', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Créer le timestamp pour le nom de fichier\n",
    "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plt.savefig(f\"{RESULTS_DIR}/silhouette_scores_{timestamp_str}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logger.info(f\"Graphique des scores silhouette sauvegardé\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Erreur lors de la sauvegarde du graphique: {str(e)}\")\n",
    "    \n",
    "    return best_model, results, best_k, silhouette_scores    \n",
    "def analyze_clusters(df, cluster_col=\"prediction\", feature_cols=None):\n",
    "    \"\"\"Analyse les caractéristiques des clusters avec gestion robuste\"\"\"\n",
    "    logger.info(\"Analyse des caractéristiques des clusters\")\n",
    "    \n",
    "    # Calculer les moyennes par cluster\n",
    "    agg_exprs = [count(\"*\").alias(\"cluster_size\")]\n",
    "    for col_name in feature_cols:\n",
    "        agg_exprs.append(avg(col(col_name)).alias(f\"avg_{col_name}\"))\n",
    "\n",
    "    cluster_stats = df.groupBy(cluster_col).agg(*agg_exprs).orderBy(cluster_col).collect()\n",
    "    \n",
    "    # Afficher les statistiques par cluster via logging\n",
    "    logger.info(\"Statistiques par cluster:\")\n",
    "    for row in cluster_stats:\n",
    "        logger.info(f\"Cluster {row[cluster_col]}: {dict(row.asDict())}\")\n",
    "    \n",
    "    return cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c4a807ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters(df, cluster_col=\"prediction\", feature_cols=None):\n",
    "    \"\"\"Analyse les caractéristiques des clusters avec gestion robuste\"\"\"\n",
    "    logger.info(\"Analyse des caractéristiques des clusters\")\n",
    "    \n",
    "    # Calculer les moyennes par cluster\n",
    "    agg_exprs = [count(\"*\").alias(\"cluster_size\")]\n",
    "    for col_name in feature_cols:\n",
    "        agg_exprs.append(avg(col(col_name)).alias(f\"avg_{col_name}\"))\n",
    "\n",
    "    cluster_stats = df.groupBy(cluster_col).agg(*agg_exprs).orderBy(cluster_col).collect()\n",
    "    \n",
    "    # Afficher les statistiques par cluster via logging\n",
    "    logger.info(\"Statistiques par cluster:\")\n",
    "    for row in cluster_stats:\n",
    "        logger.info(f\"Cluster {row[cluster_col]}: {dict(row.asDict())}\")\n",
    "    \n",
    "    return cluster_stats\n",
    "def combine_segmentations(cluster_df, rfm_df):\n",
    "    \"\"\"Combine les segmentations RFM et clustering comportemental - VERSION CORRIGÉE SANS UDF\"\"\"\n",
    "    logger.info(\"Combinaison des segmentations RFM et clustering\")\n",
    "    \n",
    "    # Debug : Vérifier les données d'entrée\n",
    "    logger.info(f\"Cluster DF count: {cluster_df.count()}\")\n",
    "    logger.info(f\"RFM DF count: {rfm_df.count()}\")\n",
    "    \n",
    "    # Jointure sécurisée avec repartitioning\n",
    "    cluster_df = cluster_df.repartition(20, \"user_id\")\n",
    "    rfm_df = rfm_df.repartition(20, \"user_id\").select(\"user_id\", \"rfm_segment\", \"rfm_score\")\n",
    "    \n",
    "    combined_df = cluster_df.join(rfm_df, on=\"user_id\", how=\"inner\")\n",
    "    \n",
    "    # Renommer les colonnes pour plus de clarté\n",
    "    combined_df = combined_df.withColumnRenamed(\"prediction\", \"behavior_cluster\")\n",
    "    \n",
    "    # SOLUTION: Remplacer l'UDF par une expression CASE WHEN native Spark\n",
    "    # Créer les étiquettes des clusters avec une expression conditionnelle\n",
    "    combined_df = combined_df.withColumn(\n",
    "        \"behavior_segment\",\n",
    "        when(col(\"behavior_cluster\") == 0, \"Explorateurs Occasionnels\")\n",
    "        .when(col(\"behavior_cluster\") == 1, \"Acheteurs Fidèles\")\n",
    "        .when(col(\"behavior_cluster\") == 2, \"Visiteurs Fréquents\")\n",
    "        .when(col(\"behavior_cluster\") == 3, \"Acheteurs à Fort Panier\")\n",
    "        .when(col(\"behavior_cluster\") == 4, \"Visiteurs Uniques\")\n",
    "        .when(col(\"behavior_cluster\") == 5, \"Convertisseurs Efficaces\")\n",
    "        .when(col(\"behavior_cluster\") == 6, \"Indécis (Abandon Panier)\")\n",
    "        .when(col(\"behavior_cluster\") == 7, \"Browsers Passifs\")\n",
    "        .when(col(\"behavior_cluster\") == 8, \"Acheteurs Impulsifs\")\n",
    "        .otherwise(\"Segment Non Défini\")\n",
    "    )\n",
    "    \n",
    "    # Afficher les distributions via collect() pour éviter les erreurs\n",
    "    logger.info(\"Distribution des segments comportementaux:\")\n",
    "    behavior_distribution = combined_df.groupBy(\"behavior_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "    for row in behavior_distribution:\n",
    "        logger.info(f\"{row['behavior_segment']}: {row['count']}\")\n",
    "    \n",
    "    # Analyser l'affinité entre segments\n",
    "    affinity_results = combined_df.groupBy(\"behavior_segment\", \"rfm_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "    logger.info(\"Top 10 affinités entre segments RFM et comportementaux:\")\n",
    "    for i, row in enumerate(affinity_results[:10]):\n",
    "        logger.info(f\"{i+1}. {row['behavior_segment']} + {row['rfm_segment']}: {row['count']}\")\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0739d249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 23:59:03,008 - INFO - Session Spark initialisée\n",
      "2025-05-18 23:59:03,012 - INFO - Chargement des données prétraitées\n",
      "2025-05-18 23:59:03,427 - INFO - Comportements utilisateurs chargés: 163024 lignes\n",
      "2025-05-18 23:59:03,429 - INFO - Préparation des données pour segmentation RFM\n",
      "2025-05-18 23:59:05,229 - INFO - Splits récence: [0, 10, 20, 30, inf]\n",
      "2025-05-18 23:59:05,231 - INFO - Splits fréquence: [0, 0.03819069584846403, 0.07638139169692806, 0.1145720875453921, inf]\n",
      "2025-05-18 23:59:05,232 - INFO - Splits monétaire: [0, 16.671183476052605, 33.34236695210521, 50.013550428157814, inf]\n",
      "2025-05-18 23:59:06,403 - INFO - Distribution des segments RFM:\n",
      "2025-05-18 23:59:06,404 - INFO - Inactif: 150572\n",
      "2025-05-18 23:59:06,406 - INFO - At Risk: 12452\n",
      "2025-05-18 23:59:06,407 - INFO - Préparation des données pour clustering comportemental\n",
      "2025-05-18 23:59:07,262 - INFO - Utilisateurs avec au moins 2 événements: 117529\n",
      "2025-05-18 23:59:07,953 - INFO - Entraînement des modèles K-means\n",
      "2025-05-18 23:59:07,960 - INFO - Essai avec k=2\n",
      "2025-05-18 23:59:13,275 - INFO - Silhouette pour k=2: 0.7839887201925793\n",
      "2025-05-18 23:59:13,278 - INFO - Essai avec k=3\n",
      "2025-05-18 23:59:17,068 - INFO - Silhouette pour k=3: 0.7234835367892903\n",
      "2025-05-18 23:59:17,070 - INFO - Essai avec k=4\n",
      "2025-05-18 23:59:20,769 - INFO - Silhouette pour k=4: 0.729655373579742\n",
      "2025-05-18 23:59:20,770 - INFO - Essai avec k=5\n",
      "2025-05-18 23:59:23,875 - INFO - Silhouette pour k=5: 0.7333949850155613\n",
      "2025-05-18 23:59:23,876 - INFO - Essai avec k=6\n",
      "2025-05-18 23:59:28,255 - INFO - Silhouette pour k=6: 0.5498959436136374\n",
      "2025-05-18 23:59:28,256 - INFO - Essai avec k=7\n",
      "2025-05-18 23:59:31,431 - INFO - Silhouette pour k=7: 0.7517621699920589\n",
      "2025-05-18 23:59:31,432 - INFO - Meilleur modèle: k=2 avec silhouette=0.7839887201925793\n",
      "2025-05-18 23:59:31,451 - INFO - Distribution des clusters:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|         0|107723|\n",
      "|         1|  9806|\n",
      "+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 23:59:32,198 - INFO - Graphique des scores silhouette sauvegardé\n",
      "2025-05-18 23:59:32,231 - INFO - Analyse des caractéristiques des clusters\n",
      "2025-05-18 23:59:33,617 - INFO - Statistiques par cluster:\n",
      "2025-05-18 23:59:33,618 - INFO - Cluster 0: {'prediction': 0, 'cluster_size': 107723, 'avg_nb_events': 7.845093434085571, 'avg_nb_views': 7.777057824234379, 'avg_nb_carts': 0.04321268438495029, 'avg_nb_purchases': 0.02482292546624212, 'avg_nb_removes': 0.0, 'avg_avg_price_viewed': 320.8869577016758, 'avg_avg_price_purchased': 2.1138300084475916, 'avg_nb_sessions': 1.501072194424589, 'avg_conversion_rate': 0.002730616324765064, 'avg_cart_abandonment': 0.027586185556164114, 'avg_engagement_days': 1.0}\n",
      "2025-05-18 23:59:33,620 - INFO - Cluster 1: {'prediction': 1, 'cluster_size': 9806, 'avg_nb_events': 11.157250662859473, 'avg_nb_views': 8.6962064042423, 'avg_nb_carts': 1.018050173363247, 'avg_nb_purchases': 1.4429940852539263, 'avg_nb_removes': 0.0, 'avg_avg_price_viewed': 372.51225273436495, 'avg_avg_price_purchased': 372.28669310859163, 'avg_nb_sessions': 1.9568631450132572, 'avg_conversion_rate': 0.3734051640774437, 'avg_cart_abandonment': 0.022791749825061942, 'avg_engagement_days': 1.0}\n",
      "2025-05-18 23:59:33,957 - INFO - Modèle K-means sauvegardé: ./models/kmeans_behavioral_2_clusters_20250518_235933\n",
      "2025-05-18 23:59:33,964 - INFO - Combinaison des segmentations RFM et clustering\n",
      "2025-05-18 23:59:34,116 - INFO - Cluster DF count: 117529\n",
      "2025-05-18 23:59:34,244 - INFO - RFM DF count: 163024\n",
      "2025-05-18 23:59:34,320 - INFO - Distribution des segments comportementaux:\n",
      "2025-05-18 23:59:36,690 - INFO - Explorateurs Occasionnels: 107723\n",
      "2025-05-18 23:59:36,691 - INFO - Acheteurs Fidèles: 9806\n",
      "2025-05-18 23:59:38,541 - INFO - Top 10 affinités entre segments RFM et comportementaux:\n",
      "2025-05-18 23:59:38,543 - INFO - 1. Explorateurs Occasionnels + Inactif: 105049\n",
      "2025-05-18 23:59:38,544 - INFO - 2. Acheteurs Fidèles + At Risk: 9754\n",
      "2025-05-18 23:59:38,548 - INFO - 3. Explorateurs Occasionnels + At Risk: 2674\n",
      "2025-05-18 23:59:38,551 - INFO - 4. Acheteurs Fidèles + Inactif: 52\n",
      "2025-05-18 23:59:40,461 - INFO - Segmentations combinées sauvegardées: ./results/combined_segmentation_20250518_235933.parquet\n",
      "2025-05-18 23:59:41,845 - INFO - Profils utilisateurs sauvegardés: ./results/user_profiles_20250518_235933.parquet\n",
      "2025-05-18 23:59:42,281 - INFO - Graphique final des scores silhouette sauvegardé\n",
      "2025-05-18 23:59:42,283 - INFO - === RÉSUMÉ DE LA SEGMENTATION ===\n",
      "2025-05-18 23:59:42,621 - INFO - Nombre d'utilisateurs total: 163024\n",
      "2025-05-18 23:59:42,770 - INFO - Utilisateurs pour clustering: 117529\n",
      "2025-05-18 23:59:42,771 - INFO - Nombre optimal de clusters: 2\n",
      "2025-05-18 23:59:42,772 - INFO - Score silhouette optimal: 0.7840\n",
      "2025-05-18 23:59:43,944 - INFO - Utilisateurs segmentés: 117529\n",
      "2025-05-18 23:59:43,946 - INFO - Distribution finale des segments RFM:\n",
      "2025-05-18 23:59:45,434 - INFO - Inactif: 105101\n",
      "2025-05-18 23:59:45,435 - INFO - At Risk: 12428\n",
      "2025-05-18 23:59:45,436 - INFO - Distribution finale des segments comportementaux:\n",
      "2025-05-18 23:59:46,891 - INFO - Explorateurs Occasionnels: 107723\n",
      "2025-05-18 23:59:46,892 - INFO - Acheteurs Fidèles: 9806\n",
      "2025-05-18 23:59:47,550 - INFO - Session Spark fermée\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialisation de la session Spark\n",
    "    spark = create_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")  # Réduire les logs Spark\n",
    "    logger.info(\"Session Spark initialisée\")\n",
    "    \n",
    "    try:\n",
    "        # Trouver les fichiers les plus récents\n",
    "        latest_cleaned = find_latest_file(DATA_DIR, \"cleaned_data\")\n",
    "        latest_user_behavior = find_latest_file(DATA_DIR, \"user_behavior\")\n",
    "        latest_recommendation = find_latest_file(DATA_DIR, \"recommendation_data\")\n",
    "        latest_product = find_latest_file(DATA_DIR, \"product_data\")\n",
    "        latest_time_series = find_latest_file(DATA_DIR, \"time_series_data\")\n",
    "        \n",
    "        # Vérifier que les fichiers existent\n",
    "        if not latest_user_behavior:\n",
    "            raise FileNotFoundError(\"Fichier user_behavior non trouvé\")\n",
    "        \n",
    "        # Chargement des données principales\n",
    "        logger.info(\"Chargement des données prétraitées\")\n",
    "        \n",
    "        user_behavior_df = spark.read.parquet(os.path.join(DATA_DIR, latest_user_behavior))\n",
    "        logger.info(f\"Comportements utilisateurs chargés: {user_behavior_df.count()} lignes\")\n",
    "        \n",
    "        # Exécuter la segmentation RFM\n",
    "        rfm_segmentation = prepare_rfm_segmentation(user_behavior_df)\n",
    "        \n",
    "        # Exécuter la préparation pour le clustering\n",
    "        behavior_clustering_df, behavior_features = prepare_behavioral_clustering(user_behavior_df)\n",
    "        \n",
    "        # Entraîner le modèle K-means\n",
    "        kmeans_model, cluster_results, best_k, silhouette_scores = train_kmeans_model(\n",
    "            behavior_clustering_df, feature_col=\"features\", k_values=range(2, 8)\n",
    "        )\n",
    "        \n",
    "        # Analyser les clusters obtenus\n",
    "        cluster_stats = analyze_clusters(\n",
    "            cluster_results.select(\"user_id\", \"prediction\", *behavior_features),\n",
    "            cluster_col=\"prediction\", \n",
    "            feature_cols=behavior_features\n",
    "        )\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"{MODELS_DIR}/kmeans_behavioral_{best_k}_clusters_{timestamp_str}\"\n",
    "        kmeans_model.save(model_path)\n",
    "        logger.info(f\"Modèle K-means sauvegardé: {model_path}\")\n",
    "        \n",
    "        # Combiner les segmentations\n",
    "        user_clusters = cluster_results.select(\"user_id\", \"prediction\")\n",
    "        combined_segments = combine_segmentations(user_clusters, rfm_segmentation)\n",
    "        \n",
    "        # Sauvegarder les segmentations combinées\n",
    "        combined_output_path = f\"{RESULTS_DIR}/combined_segmentation_{timestamp_str}.parquet\"\n",
    "        combined_segments.write.mode(\"overwrite\").format(\"parquet\").save(combined_output_path)\n",
    "        logger.info(f\"Segmentations combinées sauvegardées: {combined_output_path}\")\n",
    "        \n",
    "        # Créer un dataframe de profils utilisateurs pour les recommandations\n",
    "        user_profiles = combined_segments.select(\n",
    "            \"user_id\", \"behavior_cluster\", \"behavior_segment\", \"rfm_segment\", \"rfm_score\"\n",
    "        )\n",
    "        \n",
    "        # Sauvegarder les profils utilisateurs\n",
    "        profiles_output_path = f\"{RESULTS_DIR}/user_profiles_{timestamp_str}.parquet\"\n",
    "        user_profiles.write.mode(\"overwrite\").format(\"parquet\").save(profiles_output_path)\n",
    "        logger.info(f\"Profils utilisateurs sauvegardés: {profiles_output_path}\")\n",
    "        \n",
    "        # Générer un graphique des scores silhouette (déjà fait dans train_kmeans_model, mais on peut le refaire ici si nécessaire)\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(2, 8), silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
    "            plt.xlabel('Nombre de clusters', fontsize=12)\n",
    "            plt.ylabel('Score Silhouette', fontsize=12)\n",
    "            plt.title('Optimisation du nombre de clusters - Score Silhouette', fontsize=14)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}/silhouette_scores_final_{timestamp_str}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logger.info(f\"Graphique final des scores silhouette sauvegardé\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Erreur lors de la sauvegarde du graphique final: {str(e)}\")\n",
    "        \n",
    "        # Afficher un résumé final\n",
    "        logger.info(\"=== RÉSUMÉ DE LA SEGMENTATION ===\")\n",
    "        logger.info(f\"Nombre d'utilisateurs total: {user_behavior_df.count()}\")\n",
    "        logger.info(f\"Utilisateurs pour clustering: {behavior_clustering_df.count()}\")\n",
    "        logger.info(f\"Nombre optimal de clusters: {best_k}\")\n",
    "        \n",
    "        # Calculer le meilleur score silhouette de manière sécurisée\n",
    "        valid_scores = [score for score in silhouette_scores if score > -1]\n",
    "        best_silhouette = np.max(valid_scores) if valid_scores else 0\n",
    "        logger.info(f\"Score silhouette optimal: {best_silhouette:.4f}\")\n",
    "        logger.info(f\"Utilisateurs segmentés: {combined_segments.count()}\")\n",
    "        \n",
    "        # Afficher la distribution finale des segments\n",
    "        logger.info(\"Distribution finale des segments RFM:\")\n",
    "        rfm_final_distribution = combined_segments.groupBy(\"rfm_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "        for row in rfm_final_distribution[:10]:\n",
    "            logger.info(f\"{row['rfm_segment']}: {row['count']}\")\n",
    "        \n",
    "        logger.info(\"Distribution finale des segments comportementaux:\")\n",
    "        behavior_final_distribution = combined_segments.groupBy(\"behavior_segment\").count().orderBy(desc(\"count\")).collect()\n",
    "        for row in behavior_final_distribution[:10]:\n",
    "            logger.info(f\"{row['behavior_segment']}: {row['count']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'exécution: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "        logger.info(\"Session Spark fermée\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7a6d1ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 23:59:47,584 - INFO - Préparation des données pour le système de recommandation\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_df, test_df, pipeline_model\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Préparer les données pour les recommandations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m als_train_df, als_test_df, als_pipeline = \u001b[43mprepare_recommendation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecommendation_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mprepare_recommendation_data\u001b[39m\u001b[34m(recom_df)\u001b[39m\n\u001b[32m      3\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mPréparation des données pour le système de recommandation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Indexer les utilisateurs et produits pour ALS\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m user_indexer = \u001b[43mStringIndexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputCol\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_idx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandleInvalid\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mskip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m product_indexer = StringIndexer(\n\u001b[32m     13\u001b[39m     inputCol=\u001b[33m\"\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     14\u001b[39m     outputCol=\u001b[33m\"\u001b[39m\u001b[33mproduct_idx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     handleInvalid=\u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Créer le pipeline de préparation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\__init__.py:139\u001b[39m, in \u001b[36mkeyword_only.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m forces keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m % func.\u001b[34m__name__\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m._input_kwargs = kwargs\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\ml\\feature.py:4650\u001b[39m, in \u001b[36mStringIndexer.__init__\u001b[39m\u001b[34m(self, inputCol, outputCol, inputCols, outputCols, handleInvalid, stringOrderType)\u001b[39m\n\u001b[32m   4645\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4646\u001b[39m \u001b[33;03m__init__(self, \\\\*, inputCol=None, outputCol=None, inputCols=None, outputCols=None, \\\u001b[39;00m\n\u001b[32m   4647\u001b[39m \u001b[33;03m         handleInvalid=\"error\", stringOrderType=\"frequencyDesc\")\u001b[39;00m\n\u001b[32m   4648\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4649\u001b[39m \u001b[38;5;28msuper\u001b[39m(StringIndexer, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m-> \u001b[39m\u001b[32m4650\u001b[39m \u001b[38;5;28mself\u001b[39m._java_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.ml.feature.StringIndexer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4651\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m._input_kwargs\n\u001b[32m   4652\u001b[39m \u001b[38;5;28mself\u001b[39m.setParams(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\ml\\wrapper.py:80\u001b[39m, in \u001b[36mJavaWrapper._new_java_obj\u001b[39m\u001b[34m(java_class, *args)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33;03mReturns a new Java object.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m sc = SparkContext._active_spark_context\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     82\u001b[39m java_obj = _jvm()\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def prepare_recommendation_data(recom_df):\n",
    "    \"\"\"Prépare les données pour le système de recommandation\"\"\"\n",
    "    logger.info(\"Préparation des données pour le système de recommandation\")\n",
    "    \n",
    "    # Indexer les utilisateurs et produits pour ALS\n",
    "    user_indexer = StringIndexer(\n",
    "        inputCol=\"user_id\", \n",
    "        outputCol=\"user_idx\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    product_indexer = StringIndexer(\n",
    "        inputCol=\"product_id\", \n",
    "        outputCol=\"product_idx\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    # Créer le pipeline de préparation\n",
    "    pipeline = Pipeline(stages=[user_indexer, product_indexer])\n",
    "    pipeline_model = pipeline.fit(recom_df)\n",
    "    als_df = pipeline_model.transform(recom_df)\n",
    "    \n",
    "    # Afficher un aperçu des données préparées\n",
    "    logger.info(\"Aperçu des données préparées pour ALS:\")\n",
    "    als_df.select(\"user_id\", \"user_idx\", \"product_id\", \"product_idx\", \n",
    "                 \"event_type\", \"interaction_score\").show(5)\n",
    "    \n",
    "    # Calculer quelques statistiques utiles\n",
    "    unique_users = als_df.select(\"user_id\").distinct().count()\n",
    "    unique_products = als_df.select(\"product_id\").distinct().count()\n",
    "    total_interactions = als_df.count()\n",
    "    \n",
    "    logger.info(f\"Statistiques du dataset de recommandation:\")\n",
    "    logger.info(f\"Utilisateurs uniques: {unique_users}\")\n",
    "    logger.info(f\"Produits uniques: {unique_products}\")\n",
    "    logger.info(f\"Interactions totales: {total_interactions}\")\n",
    "    logger.info(f\"Densité: {total_interactions / (unique_users * unique_products) * 100:.6f}%\")\n",
    "    \n",
    "    # Diviser les données en ensembles d'entraînement et de test\n",
    "    train_df, test_df = als_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    logger.info(f\"Ensemble d'entraînement: {train_df.count()} lignes\")\n",
    "    logger.info(f\"Ensemble de test: {test_df.count()} lignes\")\n",
    "    \n",
    "    return train_df, test_df, pipeline_model\n",
    "\n",
    "# Préparer les données pour les recommandations\n",
    "als_train_df, als_test_df, als_pipeline = prepare_recommendation_data(recommendation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe34f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:58:57,717 - INFO - Entraînement du modèle ALS\n",
      "2025-05-18 17:58:57,719 - INFO - Essai avec rank=10, regParam=0.01\n",
      "2025-05-18 17:59:50,935 - INFO - RMSE pour rank=10, regParam=0.01: 1.687387446788512\n",
      "2025-05-18 17:59:50,936 - INFO - Essai avec rank=10, regParam=0.1\n",
      "2025-05-18 18:00:34,282 - INFO - RMSE pour rank=10, regParam=0.1: 1.6891501157680544\n",
      "2025-05-18 18:00:34,283 - INFO - Essai avec rank=10, regParam=1.0\n",
      "2025-05-18 18:01:21,609 - INFO - RMSE pour rank=10, regParam=1.0: 1.730507037766797\n",
      "2025-05-18 18:01:21,611 - INFO - Essai avec rank=20, regParam=0.01\n",
      "2025-05-18 18:02:13,881 - INFO - RMSE pour rank=20, regParam=0.01: 1.667358695414784\n",
      "2025-05-18 18:02:13,882 - INFO - Essai avec rank=20, regParam=0.1\n",
      "2025-05-18 18:03:04,332 - INFO - RMSE pour rank=20, regParam=0.1: 1.6690093297310653\n",
      "2025-05-18 18:03:04,333 - INFO - Essai avec rank=20, regParam=1.0\n",
      "2025-05-18 18:03:59,786 - INFO - RMSE pour rank=20, regParam=1.0: 1.7204041942327433\n",
      "2025-05-18 18:03:59,787 - INFO - Essai avec rank=30, regParam=0.01\n",
      "2025-05-18 18:05:05,610 - INFO - RMSE pour rank=30, regParam=0.01: 1.6531043709800564\n",
      "2025-05-18 18:05:05,612 - INFO - Essai avec rank=30, regParam=0.1\n",
      "2025-05-18 18:06:18,188 - INFO - RMSE pour rank=30, regParam=0.1: 1.6552328093070034\n",
      "2025-05-18 18:06:18,189 - INFO - Essai avec rank=30, regParam=1.0\n",
      "2025-05-18 18:07:22,089 - INFO - RMSE pour rank=30, regParam=1.0: 1.7145976336804531\n",
      "2025-05-18 18:07:22,091 - INFO - Meilleur modèle - RMSE: 1.6531043709800564\n",
      "2025-05-18 18:07:24,799 - INFO - Modèle ALS sauvegardé: ./models/als_model_20250518_180722\n"
     ]
    }
   ],
   "source": [
    "def train_als_model(train_df, test_df):\n",
    "    \"\"\"Entraîne et évalue le modèle ALS\"\"\"\n",
    "    logger.info(\"Entraînement du modèle ALS\")\n",
    "    \n",
    "    # Hyperparamètres à tester\n",
    "    als_models = {}\n",
    "    ranks = [10, 20, 30]\n",
    "    reg_params = [0.01, 0.1, 1.0]\n",
    "    \n",
    "    best_model = None\n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    for rank in ranks:\n",
    "        for reg_param in reg_params:\n",
    "            logger.info(f\"Essai avec rank={rank}, regParam={reg_param}\")\n",
    "            \n",
    "            als = ALS(\n",
    "                rank=rank,\n",
    "                maxIter=15,\n",
    "                regParam=reg_param,\n",
    "                userCol=\"user_idx\",\n",
    "                itemCol=\"product_idx\",\n",
    "                ratingCol=\"interaction_score\",\n",
    "                coldStartStrategy=\"drop\",\n",
    "                nonnegative=True,\n",
    "                implicitPrefs=True\n",
    "            )\n",
    "            \n",
    "            model = als.fit(train_df)\n",
    "            predictions = model.transform(test_df)\n",
    "            \n",
    "            # Évaluation\n",
    "            evaluator = RegressionEvaluator(\n",
    "                metricName=\"rmse\",\n",
    "                labelCol=\"interaction_score\",\n",
    "                predictionCol=\"prediction\"\n",
    "            )\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            \n",
    "            logger.info(f\"RMSE pour rank={rank}, regParam={reg_param}: {rmse}\")\n",
    "            \n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_model = model\n",
    "    \n",
    "    logger.info(f\"Meilleur modèle - RMSE: {best_rmse}\")\n",
    "    \n",
    "    # Sauvegarde du modèle\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = f\"{MODELS_DIR}/als_model_{timestamp_str}\"\n",
    "    best_model.save(model_path)\n",
    "    logger.info(f\"Modèle ALS sauvegardé: {model_path}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Exécuter l'entraînement ALS\n",
    "als_model = train_als_model(als_train_df, als_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eddbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:40:42,419 - INFO - Préparation des caractéristiques produits\n"
     ]
    }
   ],
   "source": [
    "def prepare_content_based_features(product_df):\n",
    "    \"\"\"Prépare les caractéristiques produits pour le content-based filtering\"\"\"\n",
    "    logger.info(\"Préparation des caractéristiques produits\")\n",
    "    \n",
    "    # Combiner les métadonnées\n",
    "    product_features = product_df.withColumn(\n",
    "        \"product_features\",\n",
    "        concat_ws(\" \", \n",
    "            col(\"category_code\"), \n",
    "            col(\"brand\"), \n",
    "            col(\"price\").cast(\"string\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # TF-IDF\n",
    "    tokenizer = Tokenizer(inputCol=\"product_features\", outputCol=\"tokens\")\n",
    "    hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=[tokenizer, hashing_tf, idf])\n",
    "    model = pipeline.fit(product_features)\n",
    "    product_features = model.transform(product_features)\n",
    "    \n",
    "    return product_features\n",
    "\n",
    "# Utilisation\n",
    "product_features = prepare_content_based_features(product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "311226f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_10488\\867523316.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m### 3. Intégration dans le flux principal ###\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Après l'entraînement du modèle ALS et la segmentation RFM initiale\u001b[39;00m\n\u001b[32m     99\u001b[39m \n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Segmentation RFM améliorée\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m enhanced_rfm = enhanced_rfm_segmentation(user_behavior_df)\n\u001b[32m    102\u001b[39m enhanced_rfm.select(\u001b[33m\"user_id\"\u001b[39m, \u001b[33m\"rfm_score\"\u001b[39m, \u001b[33m\"prediction\"\u001b[39m).show(\u001b[32m5\u001b[39m)\n\u001b[32m    103\u001b[39m \n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Exemple d'utilisation des recommandations hybrides\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_10488\\867523316.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(user_df)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m enhanced_rfm_segmentation(user_df):\n\u001b[32m     62\u001b[39m     \u001b[33m\"\"\"Amélioration de la segmentation RFM avec validation\"\"\"\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# Calcul des percentiles dynamiques\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     window_r = Window.orderBy(desc(\u001b[33m\"recency\"\u001b[39m))\n\u001b[32m     65\u001b[39m     window_fm = Window.orderBy(col(\u001b[33m\"frequency\"\u001b[39m), col(\u001b[33m\"monetary\"\u001b[39m))\n\u001b[32m     66\u001b[39m \n\u001b[32m     67\u001b[39m     rfm_df = user_df.withColumn(\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\sql\\utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m             \u001b[38;5;28;01mfrom\u001b[39;00m pyspark.sql.connect \u001b[38;5;28;01mimport\u001b[39;00m functions\n\u001b[32m    171\u001b[39m \n\u001b[32m    172\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m getattr(functions, f.__name__)(*args, **kwargs)\n\u001b[32m    173\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\sql\\functions.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m    313\u001b[39m     |  \u001b[32m1\u001b[39m|\n\u001b[32m    314\u001b[39m     |  \u001b[32m0\u001b[39m|\n\u001b[32m    315\u001b[39m     +---+\n\u001b[32m    316\u001b[39m     \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m col.desc() \u001b[38;5;28;01mif\u001b[39;00m isinstance(col, Column) \u001b[38;5;28;01melse\u001b[39;00m _invoke_function(\u001b[33m\"desc\"\u001b[39m, col)\n",
      "\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\sql\\functions.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(name, *args)\u001b[39m\n\u001b[32m     91\u001b[39m     \"\"\"\n\u001b[32m     92\u001b[39m     Invokes JVM function identified by name \u001b[38;5;28;01mwith\u001b[39;00m args\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mand\u001b[39;00m wraps the result \u001b[38;5;28;01mwith\u001b[39;00m :\u001b[38;5;28;01mclass\u001b[39;00m:`~pyspark.sql.Column`.\n\u001b[32m     94\u001b[39m     \"\"\"\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     96\u001b[39m     jf = _get_jvm_function(name, SparkContext._active_spark_context)\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jf(*args))\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def get_user_preferences(user_id, als_model):\n",
    "    \"\"\"Récupère le vecteur latent sous forme de liste\"\"\"\n",
    "    user_factors = als_model.userFactors.filter(col(\"id\") == user_id)\n",
    "    if user_factors.count() == 0:\n",
    "        return None\n",
    "    return user_factors.first().features.tolist()\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calcule la similarité cosinus entre deux vecteurs\"\"\"\n",
    "    return float(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "def combine_recommendations(als_recs, content_recs, alpha=0.7):\n",
    "    \"\"\"Combine les recommandations avec pondération\"\"\"\n",
    "    combined = als_recs.union(content_recs).groupBy(\"product_id\").agg(\n",
    "        (alpha * max(\"rating\")).alias(\"als_score\"),\n",
    "        ((1 - alpha) * max(\"similarity\")).alias(\"content_score\"),\n",
    "        (alpha * max(\"rating\") + (1 - alpha) * max(\"similarity\")).alias(\"combined_score\")\n",
    "    ).orderBy(desc(\"combined_score\"))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def hybrid_recommendations(user_id, als_model, product_features, num_recs=10):\n",
    "    \"\"\"Combine les recommandations collaboratives et basées sur le contenu\"\"\"\n",
    "    # Conversion préalable du user_id en DataFrame\n",
    "    user_df = spark.createDataFrame([(user_id,)], [\"user_id\"])\n",
    "    \n",
    "    # Récupération des recommandations ALS (avec gestion de la sérialisation)\n",
    "    als_recs = als_model.recommendForUserSubset(user_df, num_recs)\n",
    "    \n",
    "    # Extraction du vecteur utilisateur sous forme de liste Python\n",
    "    user_vector = get_user_preferences(user_id, als_model)\n",
    "    if user_vector is None:\n",
    "        return als_recs\n",
    "    \n",
    "    user_array = user_vector.tolist()  # Conversion en liste sérialisable\n",
    "    \n",
    "    # Définition de l'UDF avec capture de la liste Python\n",
    "    def similarity_udf_wrapper(features):\n",
    "        user_vec = np.array(user_array)\n",
    "        product_vec = np.array(features.toArray())\n",
    "        dot_product = np.dot(user_vec, product_vec)\n",
    "        norm_product = np.linalg.norm(product_vec)\n",
    "        return float(dot_product / (np.linalg.norm(user_vec) * norm_product))\n",
    "    \n",
    "    similarity_udf = udf(similarity_udf_wrapper, FloatType())\n",
    "    \n",
    "    # Calcul des similarités\n",
    "    content_recs = product_features.withColumn(\n",
    "        \"similarity\", similarity_udf(col(\"features\"))\n",
    "    ).select(\"product_id\", \"similarity\").orderBy(desc(\"similarity\")).limit(num_recs)\n",
    "    \n",
    "    # Combinaison des résultats\n",
    "    als_df = als_recs.selectExpr(\n",
    "        \"product_id\", \n",
    "        \"explode(recommendations).rating\"\n",
    "    )\n",
    "    \n",
    "    return combine_recommendations(als_df, content_recs)    \n",
    "\n",
    "### 2. Implémentation complète de enhanced_rfm_segmentation ###\n",
    "def enhanced_rfm_segmentation(user_df):\n",
    "    \"\"\"Amélioration de la segmentation RFM avec validation\"\"\"\n",
    "    # Calcul des percentiles dynamiques\n",
    "    window_r = Window.orderBy(desc(\"recency\"))\n",
    "    window_fm = Window.orderBy(col(\"frequency\"), col(\"monetary\"))\n",
    "    \n",
    "    rfm_df = user_df.withColumn(\n",
    "        \"r_percentile\", percent_rank().over(window_r)\n",
    "    ).withColumn(\n",
    "        \"f_percentile\", percent_rank().over(window_fm)\n",
    "    ).withColumn(\n",
    "        \"m_percentile\", percent_rank().over(window_fm)\n",
    "    )\n",
    "    \n",
    "    # Calcul du score pondéré\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"rfm_score\",\n",
    "        (col(\"r_percentile\") * 0.4 + \n",
    "         col(\"f_percentile\") * 0.3 + \n",
    "         col(\"m_percentile\") * 0.3)\n",
    "    )\n",
    "    \n",
    "    # Clustering avec validation\n",
    "    assembler = VectorAssembler(inputCols=[\"rfm_score\"], outputCol=\"features\")\n",
    "    kmeans = KMeans(k=5, seed=42)\n",
    "    pipeline = Pipeline(stages=[assembler, kmeans])\n",
    "    model = pipeline.fit(rfm_df)\n",
    "    \n",
    "    # Évaluation\n",
    "    predictions = model.transform(rfm_df)\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    logger.info(f\"Silhouette Score pour RFM amélioré: {silhouette}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "### 3. Intégration dans le flux principal ###\n",
    "# Après l'entraînement du modèle ALS et la segmentation RFM initiale\n",
    "\n",
    "# Segmentation RFM améliorée\n",
    "enhanced_rfm = enhanced_rfm_segmentation(user_behavior_df)\n",
    "enhanced_rfm.select(\"user_id\", \"rfm_score\", \"prediction\").show(5)\n",
    "\n",
    "# Exemple d'utilisation des recommandations hybrides\n",
    "sample_user = str(user_behavior_df.first().user_id)\n",
    "\n",
    "hybrid_recs = hybrid_recommendations(\n",
    "    user_id=sample_user,\n",
    "    als_model=als_model,\n",
    "    product_features=product_features,\n",
    "    num_recs=10\n",
    ")\n",
    "\n",
    "logger.info(\"Recommandations hybrides pour l'utilisateur %s:\", sample_user)\n",
    "hybrid_recs.show(10)\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "hybrid_recs.write.mode(\"overwrite\").parquet(f\"{RESULTS_DIR}/hybrid_recommendations\")\n",
    "enhanced_rfm.write.mode(\"overwrite\").parquet(f\"{RESULTS_DIR}/enhanced_rfm_segments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
