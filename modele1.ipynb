{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28e686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:42:35,920 - INFO - Session Spark initialisée\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, when, desc, avg, min, udf, percent_rank\n",
    ")\n",
    "from pyspark.sql.types import ( StringType, FloatType\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, StandardScaler, StringIndexer, Bucketizer\n",
    ")\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator, RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.window import Window\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"modelisation.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paramètres globaux\n",
    "DATA_DIR = \"./data/processed/parquet\"\n",
    "MODELS_DIR = \"./models\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "\n",
    "# Création des répertoires de sortie s'ils n'existent pas\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Création de la session Spark\n",
    "def create_spark_session():\n",
    "    \"\"\"Crée et retourne une session Spark configurée\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"E-commerce Models\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"20\") \\\n",
    "        .config(\"spark.default.parallelism\", \"20\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Initialisation de la session Spark\n",
    "spark = create_spark_session()\n",
    "logger.info(\"Session Spark initialisée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceb6f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:00:46,162 - INFO - Chargement des données prétraitées\n",
      "2025-05-18 16:00:50,443 - INFO - Données nettoyées chargées: 1000000 lignes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- second: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- hour_bucket: string (nullable = true)\n",
      "\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+----+------+------+---+-----+---------+----------+-------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|hour|minute|second|day|month|dayofweek|      date|        hour_bucket|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+----+------+------+---+-----+---------+----------+-------------------+\n",
      "|2019-10-01 00:00:00|      view|  44600062|2103807459595387724|             unknown|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|   0|     0|     0|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "|2019-10-01 00:00:00|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|   0|     0|     0|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "|2019-10-01 00:00:01|      view|  17200506|2053013559792632471|furniture.living_...| unknown|  543.1|519107250|566511c2-e2e3-422...|   0|     0|     1|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "|2019-10-01 00:00:01|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|   0|     0|     1|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "|2019-10-01 00:00:04|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|   0|     0|     4|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+----+------+------+---+-----+---------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:00:52,623 - INFO - Comportements utilisateurs chargés: 163024 lignes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- nb_events: long (nullable = true)\n",
      " |-- nb_views: long (nullable = true)\n",
      " |-- nb_carts: long (nullable = true)\n",
      " |-- nb_purchases: long (nullable = true)\n",
      " |-- nb_removes: long (nullable = true)\n",
      " |-- avg_price_viewed: double (nullable = true)\n",
      " |-- avg_price_purchased: double (nullable = true)\n",
      " |-- nb_sessions: long (nullable = true)\n",
      " |-- first_seen: timestamp (nullable = true)\n",
      " |-- last_seen: timestamp (nullable = true)\n",
      " |-- recency: integer (nullable = true)\n",
      " |-- frequency: long (nullable = true)\n",
      " |-- monetary: double (nullable = true)\n",
      " |-- viewed_categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- viewed_brands: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- conversion_rate: double (nullable = true)\n",
      " |-- cart_abandonment: double (nullable = true)\n",
      " |-- engagement_days: integer (nullable = true)\n",
      "\n",
      "+---------+---------+--------+--------+------------+----------+------------------+-------------------+-----------+-------------------+-------------------+-------+---------+--------+--------------------+--------------------+---------------+----------------+---------------+\n",
      "|  user_id|nb_events|nb_views|nb_carts|nb_purchases|nb_removes|  avg_price_viewed|avg_price_purchased|nb_sessions|         first_seen|          last_seen|recency|frequency|monetary|   viewed_categories|       viewed_brands|conversion_rate|cart_abandonment|engagement_days|\n",
      "+---------+---------+--------+--------+------------+----------+------------------+-------------------+-----------+-------------------+-------------------+-------+---------+--------+--------------------+--------------------+---------------+----------------+---------------+\n",
      "|337535108|        9|       9|       0|           0|         0| 731.0644444444443|               NULL|          2|2019-10-01 13:51:12|2019-10-01 14:04:32|     30|        0|     0.0|[computers.notebo...|[hp, hp, hp, sams...|            0.0|             0.0|              1|\n",
      "|410824220|        4|       4|       0|           0|         0|             73.92|               NULL|          1|2019-10-01 08:21:19|2019-10-01 08:22:07|     30|        0|     0.0|[furniture.living...|[zlatek, zlatek, ...|            0.0|             0.0|              1|\n",
      "|418592979|        1|       1|       0|           0|         0|            178.89|               NULL|          1|2019-10-01 07:27:22|2019-10-01 07:27:22|     30|        0|     0.0|           [unknown]|            [metabo]|            0.0|             0.0|              1|\n",
      "|420339201|        1|       1|       0|           0|         0|            102.96|               NULL|          1|2019-10-01 03:48:48|2019-10-01 03:48:48|     30|        0|     0.0|           [unknown]|           [montale]|            0.0|             0.0|              1|\n",
      "|430276841|       22|      22|       0|           0|         0|130.81590909090912|               NULL|          6|2019-10-01 08:44:51|2019-10-01 13:28:58|     30|        0|     0.0|[auto.accessories...|[sho-me, starline...|            0.0|             0.0|              1|\n",
      "+---------+---------+--------+--------+------------+----------+------------------+-------------------+-----------+-------------------+-------------------+-------+---------+--------+--------------------+--------------------+---------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:00:53,325 - INFO - Données de recommandation chargées: 1000000 lignes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- interaction_score: integer (nullable = true)\n",
      "\n",
      "+---------+----------+----------+-------+-------------------+-----------------+\n",
      "|  user_id|product_id|event_type|  price|         event_time|interaction_score|\n",
      "+---------+----------+----------+-------+-------------------+-----------------+\n",
      "|541312140|  44600062|      view|  35.79|2019-10-01 00:00:00|                1|\n",
      "|554748717|   3900821|      view|   33.2|2019-10-01 00:00:00|                1|\n",
      "|519107250|  17200506|      view|  543.1|2019-10-01 00:00:01|                1|\n",
      "|550050854|   1307067|      view| 251.74|2019-10-01 00:00:01|                1|\n",
      "|535871217|   1004237|      view|1081.98|2019-10-01 00:00:04|                1|\n",
      "+---------+----------+----------+-------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:00:54,152 - INFO - Données produits chargées: 66603 lignes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+----------+-------------------+--------------------+--------+-------+\n",
      "|product_id|        category_id|       category_code|   brand|  price|\n",
      "+----------+-------------------+--------------------+--------+-------+\n",
      "|   1004235|2053013555631882655|electronics.smart...|   apple|1169.25|\n",
      "|   5400748|2053013552989470973|             unknown|     svc|  88.17|\n",
      "|  13200019|2053013557192163841|furniture.bedroom...|      sv| 386.08|\n",
      "|   3800422|2053013566176363511|     appliances.iron| polaris|  23.14|\n",
      "|   6000267|2053013560807654091|auto.accessories....|starline| 420.09|\n",
      "+----------+-------------------+--------------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:00:54,882 - INFO - Données temporelles chargées: 17 lignes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hour_bucket: string (nullable = true)\n",
      " |-- total_events: long (nullable = true)\n",
      " |-- unique_users: long (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      " |-- carts: long (nullable = true)\n",
      " |-- purchases: long (nullable = true)\n",
      " |-- removes: long (nullable = true)\n",
      " |-- avg_price: double (nullable = true)\n",
      "\n",
      "+-------------------+------------+------------+-----+-----+---------+-------+------------------+\n",
      "|        hour_bucket|total_events|unique_users|views|carts|purchases|removes|         avg_price|\n",
      "+-------------------+------------+------------+-----+-----+---------+-------+------------------+\n",
      "|2019-10-01 00:00:00|        1083|         383| 1070|    3|       10|      0| 303.2282086795937|\n",
      "|2019-10-01 01:00:00|         121|         102|  121|    0|        0|      0|327.07438016528926|\n",
      "|2019-10-01 02:00:00|       22886|        5378|22326|  244|      316|      0| 289.5163129346923|\n",
      "|2019-10-01 03:00:00|       49409|       10514|47951|  613|      845|      0|283.44011712259385|\n",
      "|2019-10-01 04:00:00|       55290|       11933|53390|  879|     1021|      0| 288.2792449345431|\n",
      "+-------------------+------------+------------+-----+-----+---------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_latest_file(directory, pattern):\n",
    "    \"\"\"Trouve le fichier le plus récent dans le répertoire correspondant au pattern\"\"\"\n",
    "    files = [f for f in os.listdir(directory) if pattern in f]\n",
    "    if not files:\n",
    "        return None\n",
    "    \n",
    "    latest_file = None\n",
    "    latest_time = 0\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        file_time = os.path.getmtime(file_path)\n",
    "        if file_time > latest_time:\n",
    "            latest_time = file_time\n",
    "            latest_file = file\n",
    "            \n",
    "    return latest_file\n",
    "\n",
    "# Trouver les fichiers les plus récents\n",
    "latest_cleaned = find_latest_file(DATA_DIR, \"cleaned_data\")\n",
    "latest_user_behavior = find_latest_file(DATA_DIR, \"user_behavior\")\n",
    "latest_recommendation = find_latest_file(DATA_DIR, \"recommendation_data\")\n",
    "latest_product = find_latest_file(DATA_DIR, \"product_data\")\n",
    "latest_time_series = find_latest_file(DATA_DIR, \"time_series_data\")\n",
    "\n",
    "# Chargement des données\n",
    "logger.info(\"Chargement des données prétraitées\")\n",
    "\n",
    "# Données nettoyées\n",
    "cleaned_df = spark.read.parquet(os.path.join(DATA_DIR, latest_cleaned))\n",
    "logger.info(f\"Données nettoyées chargées: {cleaned_df.count()} lignes\")\n",
    "cleaned_df.printSchema()\n",
    "cleaned_df.show(5)\n",
    "\n",
    "# Comportements utilisateurs\n",
    "user_behavior_df = spark.read.parquet(os.path.join(DATA_DIR, latest_user_behavior))\n",
    "logger.info(f\"Comportements utilisateurs chargés: {user_behavior_df.count()} lignes\")\n",
    "user_behavior_df.printSchema()\n",
    "user_behavior_df.show(5)\n",
    "\n",
    "# Données de recommandation\n",
    "recommendation_df = spark.read.parquet(os.path.join(DATA_DIR, latest_recommendation))\n",
    "logger.info(f\"Données de recommandation chargées: {recommendation_df.count()} lignes\")\n",
    "recommendation_df.printSchema()\n",
    "recommendation_df.show(5)\n",
    "\n",
    "# Données produits\n",
    "product_df = spark.read.parquet(os.path.join(DATA_DIR, latest_product))\n",
    "logger.info(f\"Données produits chargées: {product_df.count()} lignes\")\n",
    "product_df.printSchema()\n",
    "product_df.show(5)\n",
    "\n",
    "# Données temporelles\n",
    "time_series_df = spark.read.parquet(os.path.join(DATA_DIR, latest_time_series))\n",
    "logger.info(f\"Données temporelles chargées: {time_series_df.count()} lignes\")\n",
    "time_series_df.printSchema()\n",
    "time_series_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26935f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:02:09,427 - INFO - Préparation des données pour segmentation RFM\n",
      "2025-05-18 16:02:10,396 - INFO - Quantiles récence: [0, 30.0, 31]\n",
      "2025-05-18 16:02:10,397 - INFO - Quantiles fréquence: [0, 1.0, inf]\n",
      "2025-05-18 16:02:10,398 - INFO - Quantiles monétaire: [0, 1.0, inf]\n",
      "2025-05-18 16:02:10,665 - INFO - Distribution des segments RFM:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|        rfm_segment| count|\n",
      "+-------------------+------+\n",
      "|Potential Loyalists|163024|\n",
      "+-------------------+------+\n",
      "\n",
      "+---------+-------+---------+--------+-------------+---------------+--------------+---------+-------------------+\n",
      "|  user_id|recency|frequency|monetary|recency_score|frequency_score|monetary_score|rfm_score|        rfm_segment|\n",
      "+---------+-------+---------+--------+-------------+---------------+--------------+---------+-------------------+\n",
      "|337535108|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "|410824220|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "|418592979|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "|420339201|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "|430276841|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "|431759282|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "|437675804|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "|439469760|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "|439663576|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "|443458419|     30|        0|     0.0|          5.0|            1.0|           1.0|    511.0|Potential Loyalists|\n",
      "+---------+-------+---------+--------+-------------+---------------+--------------+---------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_rfm_segmentation(user_df):\n",
    "    \"\"\"Prépare les données pour la segmentation RFM\"\"\"\n",
    "    logger.info(\"Préparation des données pour segmentation RFM\")\n",
    "    \n",
    "    # Filtrer les utilisateurs avec au moins une interaction\n",
    "    rfm_df = user_df.filter(col(\"nb_events\") > 0)\n",
    "    \n",
    "    # Convertir les valeurs manquantes/nulles en 0 pour les métriques RFM\n",
    "    rfm_df = rfm_df.fillna({\n",
    "        \"recency\": 30,  # Valeur max si jamais vu\n",
    "        \"frequency\": 0,  # Pas d'achats\n",
    "        \"monetary\": 0    # Pas de dépenses\n",
    "    })\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"is_active\",\n",
    "        when(col(\"frequency\") > 0, 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Créer des buckets pour les métriques RFM\n",
    "    # 1 = meilleur, 5 = pire\n",
    "    \n",
    "    # Récence (inversée: plus petit = meilleur)\n",
    "    recency_quantiles = rfm_df.approxQuantile(\"recency\", [0.2, 0.4, 0.6, 0.8], 0.01)\n",
    "    recency_quantiles = sorted(list(set([0] + recency_quantiles + [31])))  # Ajouter min et max, enlever les doublons\n",
    "    \n",
    "    # Fréquence\n",
    "    frequency_quantiles = rfm_df.approxQuantile(\"frequency\", [0.2, 0.4, 0.6, 0.8], 0.01)\n",
    "    frequency_quantiles = sorted(list(set([0] + frequency_quantiles)))  # Min et max, enlever les doublons\n",
    "    if len(frequency_quantiles) <= 1:\n",
    "        frequency_quantiles.append(1.0)\n",
    "    frequency_quantiles.append(float('inf'))\n",
    "    \n",
    "    # Valeur monétaire\n",
    "    monetary_quantiles = rfm_df.approxQuantile(\"monetary\", [0.2, 0.4, 0.6, 0.8], 0.01)\n",
    "    monetary_quantiles = sorted(list(set([0] + monetary_quantiles)))  # Min et max, enlever les doublons\n",
    "    if len(monetary_quantiles) <= 1:\n",
    "        monetary_quantiles.append(1.0)\n",
    "    monetary_quantiles.append(float('inf'))\n",
    "    \n",
    "    logger.info(f\"Quantiles récence: {recency_quantiles}\")\n",
    "    logger.info(f\"Quantiles fréquence: {frequency_quantiles}\")\n",
    "    logger.info(f\"Quantiles monétaire: {monetary_quantiles}\")\n",
    "    \n",
    "    # Inversée pour récence (plus bas = meilleur score)\n",
    "    recency_bucketizer = Bucketizer(\n",
    "        splits=recency_quantiles, \n",
    "        inputCol=\"recency\", \n",
    "        outputCol=\"recency_score\"\n",
    "    )\n",
    "    \n",
    "    # Pas inversée (plus haut = meilleur score)\n",
    "    frequency_bucketizer = Bucketizer(\n",
    "        splits=frequency_quantiles, \n",
    "        inputCol=\"frequency\", \n",
    "        outputCol=\"frequency_score\"\n",
    "    )\n",
    "    \n",
    "    # Pas inversée (plus haut = meilleur score)\n",
    "    monetary_bucketizer = Bucketizer(\n",
    "        splits=monetary_quantiles, \n",
    "        inputCol=\"monetary\", \n",
    "        outputCol=\"monetary_score\"\n",
    "    )\n",
    "    \n",
    "    # Appliquer les bucketizers\n",
    "    rfm_df = recency_bucketizer.transform(rfm_df)\n",
    "    rfm_df = frequency_bucketizer.transform(rfm_df)\n",
    "    rfm_df = monetary_bucketizer.transform(rfm_df)\n",
    "    \n",
    "    # Inverser le score de récence (5 - score) pour qu'un score élevé soit meilleur\n",
    "    rfm_df = rfm_df.withColumn(\"recency_score\", 5.0 - col(\"recency_score\"))\n",
    "    \n",
    "    # Ajouter 1 aux scores pour qu'ils commencent à 1 (et pas à 0)\n",
    "    rfm_df = rfm_df.withColumn(\"recency_score\", col(\"recency_score\") + 1)\n",
    "    rfm_df = rfm_df.withColumn(\"frequency_score\", col(\"frequency_score\") + 1)\n",
    "    rfm_df = rfm_df.withColumn(\"monetary_score\", col(\"monetary_score\") + 1)\n",
    "    \n",
    "    # Calcul du score RFM global\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"rfm_score\", \n",
    "        col(\"recency_score\") * 100 + col(\"frequency_score\") * 10 + col(\"monetary_score\")\n",
    "    )\n",
    "    \n",
    "    # Segmentation RFM basique\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"rfm_segment\",\n",
    "        when((col(\"recency_score\") >= 4) & (col(\"frequency_score\") >= 4) & (col(\"monetary_score\") >= 4), \"Champions\")\n",
    "        .when((col(\"recency_score\") >= 3) & (col(\"frequency_score\") >= 3) & (col(\"monetary_score\") >= 3), \"Loyal Customers\")\n",
    "        .when((col(\"recency_score\") >= 3) & (col(\"frequency_score\") <= 2) & (col(\"monetary_score\") <= 2), \"Potential Loyalists\")\n",
    "        .when((col(\"recency_score\") <= 2) & (col(\"frequency_score\") >= 3) & (col(\"monetary_score\") >= 3), \"At Risk\")\n",
    "        .when((col(\"recency_score\") <= 2) & (col(\"frequency_score\") <= 2) & (col(\"monetary_score\") <= 2), \"Hibernating\")\n",
    "        .when((col(\"recency_score\") >= 4) & (col(\"frequency_score\") <= 2), \"New Customers\")\n",
    "        .when((col(\"recency_score\") <= 2) & (col(\"frequency_score\") >= 4) & (col(\"monetary_score\") >= 4), \"Cannot Lose Them\")\n",
    "        .when((col(\"recency_score\") >= 4) & (col(\"frequency_score\") >= 3) & (col(\"monetary_score\") < 3), \"Promising\")\n",
    "        .when((col(\"recency_score\") >= 3) & (col(\"frequency_score\") <= 2) & (col(\"monetary_score\") >= 3), \"Need Attention\")\n",
    "        .otherwise(\"Others\")\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Distribution des segments RFM:\")\n",
    "    rfm_df.groupBy(\"rfm_segment\").count().orderBy(desc(\"count\")).show()\n",
    "    \n",
    "    return rfm_df\n",
    "\n",
    "# Exécuter la segmentation RFM\n",
    "rfm_segmentation = prepare_rfm_segmentation(user_behavior_df)\n",
    "rfm_segmentation.select(\"user_id\", \"recency\", \"frequency\", \"monetary\", \n",
    "                       \"recency_score\", \"frequency_score\", \"monetary_score\", \n",
    "                       \"rfm_score\", \"rfm_segment\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:11:09,939 - INFO - Préparation des données pour clustering comportemental\n",
      "2025-05-18 16:11:10,107 - INFO - Utilisateurs avec au moins 2 événements: 117529\n",
      "2025-05-18 16:11:10,615 - INFO - Statistiques sur les features pour le clustering:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------+\n",
      "|    avg_nb_events|min_nb_events|max_nb_events|\n",
      "+-----------------+-------------+-------------+\n",
      "|8.121442367415701|            2|          339|\n",
      "+-----------------+-------------+-------------+\n",
      "\n",
      "+------------------+------------+------------+\n",
      "|      avg_nb_views|min_nb_views|max_nb_views|\n",
      "+------------------+------------+------------+\n",
      "|7.8537467348484205|           0|         339|\n",
      "+------------------+------------+------------+\n",
      "\n",
      "+------------------+------------+------------+\n",
      "|      avg_nb_carts|min_nb_carts|max_nb_carts|\n",
      "+------------------+------------+------------+\n",
      "|0.1245479839018455|           0|         116|\n",
      "+------------------+------------+------------+\n",
      "\n",
      "+-------------------+----------------+----------------+\n",
      "|   avg_nb_purchases|min_nb_purchases|max_nb_purchases|\n",
      "+-------------------+----------------+----------------+\n",
      "|0.14314764866543578|               0|              26|\n",
      "+-------------------+----------------+----------------+\n",
      "\n",
      "+--------------+--------------+--------------+\n",
      "|avg_nb_removes|min_nb_removes|max_nb_removes|\n",
      "+--------------+--------------+--------------+\n",
      "|           0.0|             0|             0|\n",
      "+--------------+--------------+--------------+\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|avg_avg_price_viewed|min_avg_price_viewed|max_avg_price_viewed|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|   325.1943000860289|                 0.0|             2574.07|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "+-----------------------+-----------------------+-----------------------+\n",
      "|avg_avg_price_purchased|min_avg_price_purchased|max_avg_price_purchased|\n",
      "+-----------------------+-----------------------+-----------------------+\n",
      "|      32.99910169084103|                    0.0|                2573.79|\n",
      "+-----------------------+-----------------------+-----------------------+\n",
      "\n",
      "+------------------+---------------+---------------+\n",
      "|   avg_nb_sessions|min_nb_sessions|max_nb_sessions|\n",
      "+------------------+---------------+---------------+\n",
      "|1.5391009878412987|              1|            120|\n",
      "+------------------+---------------+---------------+\n",
      "\n",
      "+-------------------+-------------------+-------------------+\n",
      "|avg_conversion_rate|min_conversion_rate|max_conversion_rate|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|0.03365774592905649|                0.0|                2.0|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|avg_cart_abandonment|min_cart_abandonment|max_cart_abandonment|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|0.027186163121027358|               -10.0|                 1.0|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "+-------------------+-------------------+-------------------+\n",
      "|avg_engagement_days|min_engagement_days|max_engagement_days|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|                1.0|                  1|                  1|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n",
      "+---------+---------+--------+--------+------------+----------+------------------+-------------------+-----------+---------------+----------------+---------------+--------------------+\n",
      "|  user_id|nb_events|nb_views|nb_carts|nb_purchases|nb_removes|  avg_price_viewed|avg_price_purchased|nb_sessions|conversion_rate|cart_abandonment|engagement_days|            features|\n",
      "+---------+---------+--------+--------+------------+----------+------------------+-------------------+-----------+---------------+----------------+---------------+--------------------+\n",
      "|337535108|        9|       9|       0|           0|         0| 731.0644444444443|                0.0|          2|            0.0|             0.0|              1|[0.08314385594760...|\n",
      "|410824220|        4|       4|       0|           0|         0|             73.92|                0.0|          1|            0.0|             0.0|              1|[-0.3900399903018...|\n",
      "|430276841|       22|      22|       0|           0|         0|130.81590909090912|                0.0|          6|            0.0|             0.0|              1|[1.31342185619613...|\n",
      "|431759282|        3|       3|       0|           0|         0|153.39333333333332|                0.0|          1|            0.0|             0.0|              1|[-0.4846767595517...|\n",
      "|437675804|        8|       8|       0|           0|         0|         138.34875|                0.0|          1|            0.0|             0.0|              1|[-0.0114929133022...|\n",
      "+---------+---------+--------+--------+------------+----------+------------------+-------------------+-----------+---------------+----------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_behavioral_clustering(user_df):\n",
    "    \"\"\"Prépare les données pour le clustering comportemental\"\"\"\n",
    "    logger.info(\"Préparation des données pour clustering comportemental\")\n",
    "    \n",
    "    # Sélection des features comportementales pertinentes\n",
    "    behavior_features = [\n",
    "        \"nb_events\", \"nb_views\", \"nb_carts\", \"nb_purchases\", \"nb_removes\",\n",
    "        \"avg_price_viewed\", \"avg_price_purchased\", \"nb_sessions\",\n",
    "        \"conversion_rate\", \"cart_abandonment\", \"engagement_days\"\n",
    "    ]\n",
    "    \n",
    "    # Filtrer uniquement les utilisateurs avec suffisamment d'interactions\n",
    "    clustering_df = user_df.filter(col(\"nb_events\") >= 2)\n",
    "    logger.info(f\"Utilisateurs avec au moins 2 événements: {clustering_df.count()}\")\n",
    "    \n",
    "    # Remplacer les valeurs nulles par des zéros ou moyennes selon le cas\n",
    "    clustering_df = clustering_df.na.fill({\n",
    "        \"avg_price_viewed\": 0,\n",
    "        \"avg_price_purchased\": 0,\n",
    "        \"conversion_rate\": 0,\n",
    "        \"cart_abandonment\": 0,\n",
    "        \"engagement_days\": 1\n",
    "    })\n",
    "    \n",
    "    # Assembler les features en vecteurs\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=behavior_features,\n",
    "        outputCol=\"features_raw\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    clustering_df = assembler.transform(clustering_df)\n",
    "    \n",
    "    # Standardiser les features pour le clustering\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features_raw\", \n",
    "        outputCol=\"features\",\n",
    "        withStd=True, \n",
    "        withMean=True\n",
    "    )\n",
    "    \n",
    "    # Appliquer le scaling\n",
    "    clustering_pipeline = Pipeline(stages=[scaler])\n",
    "    clustering_model = clustering_pipeline.fit(clustering_df)\n",
    "    clustering_df = clustering_model.transform(clustering_df)\n",
    "    \n",
    "    # Afficher quelques statistiques sur les features\n",
    "    logger.info(\"Statistiques sur les features pour le clustering:\")\n",
    "    for feature in behavior_features:\n",
    "        clustering_df.select(\n",
    "            avg(feature).alias(f\"avg_{feature}\"),\n",
    "            min(feature).alias(f\"min_{feature}\"),\n",
    "            max(feature).alias(f\"max_{feature}\")\n",
    "        ).show()\n",
    "    \n",
    "    return clustering_df, behavior_features\n",
    "\n",
    "# Exécuter la préparation pour le clustering\n",
    "behavior_clustering_df, behavior_features = prepare_behavioral_clustering(user_behavior_df)\n",
    "behavior_clustering_df.select(\"user_id\", *behavior_features, \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4c7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:39:21,321 - INFO - Entraînement des modèles K-means\n",
      "2025-05-18 16:39:21,328 - INFO - Essai avec k=2\n",
      "2025-05-18 16:39:25,929 - INFO - Silhouette pour k=2: 0.7834441765805065\n",
      "2025-05-18 16:39:25,930 - INFO - Essai avec k=3\n",
      "2025-05-18 16:39:29,654 - INFO - Silhouette pour k=3: 0.7244368921023959\n",
      "2025-05-18 16:39:29,655 - INFO - Essai avec k=4\n",
      "2025-05-18 16:39:33,403 - INFO - Silhouette pour k=4: 0.7282655495178941\n",
      "2025-05-18 16:39:33,404 - INFO - Essai avec k=5\n",
      "2025-05-18 16:39:37,075 - INFO - Silhouette pour k=5: 0.5213942468016235\n",
      "2025-05-18 16:39:37,076 - INFO - Essai avec k=6\n",
      "2025-05-18 16:39:40,669 - INFO - Silhouette pour k=6: 0.5486512480531769\n",
      "2025-05-18 16:39:40,670 - INFO - Essai avec k=7\n",
      "2025-05-18 16:39:44,318 - INFO - Silhouette pour k=7: 0.5640408649173314\n",
      "2025-05-18 16:39:44,319 - INFO - Essai avec k=8\n",
      "2025-05-18 16:39:48,047 - INFO - Silhouette pour k=8: 0.5795547723995433\n",
      "2025-05-18 16:39:48,048 - INFO - Meilleur modèle: k=2 avec silhouette=0.7834441765805065\n",
      "2025-05-18 16:39:48,062 - INFO - Distribution des clusters:\n",
      "2025-05-18 16:39:48,363 - INFO - K=2, Silhouette Score=0.7834441765805065\n",
      "2025-05-18 16:39:48,364 - INFO - K=3, Silhouette Score=0.7244368921023959\n",
      "2025-05-18 16:39:48,365 - INFO - K=4, Silhouette Score=0.7282655495178941\n",
      "2025-05-18 16:39:48,366 - INFO - K=5, Silhouette Score=0.5213942468016235\n",
      "2025-05-18 16:39:48,368 - INFO - K=6, Silhouette Score=0.5486512480531769\n",
      "2025-05-18 16:39:48,369 - INFO - K=7, Silhouette Score=0.5640408649173314\n",
      "2025-05-18 16:39:48,370 - INFO - K=8, Silhouette Score=0.5795547723995433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|         0|107807|\n",
      "|         1|  9722|\n",
      "+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:39:48,719 - INFO - Modèle K-means sauvegardé: ./models/kmeans_behavioral_2_clusters_20250518_163948\n",
      "2025-05-18 16:39:48,741 - INFO - Analyse des caractéristiques des clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+-----------------+--------------------+--------------------+--------------+--------------------+-----------------------+------------------+---------------------+--------------------+-------------------+\n",
      "|prediction|cluster_size|avg_nb_events     |avg_nb_views     |avg_nb_carts        |avg_nb_purchases    |avg_nb_removes|avg_avg_price_viewed|avg_avg_price_purchased|avg_nb_sessions   |avg_conversion_rate  |avg_cart_abandonment|avg_engagement_days|\n",
      "+----------+------------+------------------+-----------------+--------------------+--------------------+--------------+--------------------+-----------------------+------------------+---------------------+--------------------+-------------------+\n",
      "|0         |107807      |7.872633502462734 |7.803556355338707|0.043587151112636474|0.025489996011390726|0.0           |320.807282761595    |2.2345311528935974     |1.5037613513037187|0.0027813540821527696|0.02762498415378099 |1.0                |\n",
      "|1         |9722        |10.880477268051841|8.410306521291915|1.02232051018309    |1.447850236576836   |0.0           |373.84181877511037  |374.1466079636757      |1.9309812795721044|0.37604523572942206  |0.02232008833414497 |1.0                |\n",
      "+----------+------------+------------------+-----------------+--------------------+--------------------+--------------+--------------------+-----------------------+------------------+---------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_kmeans_model(df, feature_col=\"features\", k_values=range(2, 11)):\n",
    "    \"\"\"Entraîne et évalue plusieurs modèles K-means avec différentes valeurs de k\"\"\"\n",
    "    logger.info(\"Entraînement des modèles K-means\")\n",
    "    \n",
    "    # Liste pour stocker les résultats\n",
    "    silhouette_scores = []\n",
    "    models = {}\n",
    "    \n",
    "    # Évaluateur pour le clustering\n",
    "    evaluator = ClusteringEvaluator(\n",
    "        predictionCol=\"prediction\", \n",
    "        featuresCol=feature_col,\n",
    "        metricName=\"silhouette\"\n",
    "    )\n",
    "    \n",
    "    # Tester différentes valeurs de k\n",
    "    for k in k_values:\n",
    "        logger.info(f\"Essai avec k={k}\")\n",
    "        \n",
    "        # Créer et entraîner le modèle\n",
    "        kmeans = KMeans(\n",
    "            k=k, \n",
    "            seed=42, \n",
    "            featuresCol=feature_col,\n",
    "            maxIter=20,\n",
    "            tol=1e-4\n",
    "        )\n",
    "        model = kmeans.fit(df)\n",
    "        \n",
    "        # Faire des prédictions\n",
    "        predictions = model.transform(df)\n",
    "        \n",
    "        # Évaluer le modèle\n",
    "        silhouette = evaluator.evaluate(predictions)\n",
    "        logger.info(f\"Silhouette pour k={k}: {silhouette}\")\n",
    "        \n",
    "        # Stocker les résultats\n",
    "        silhouette_scores.append(silhouette)\n",
    "        models[k] = model\n",
    "    \n",
    "    # Trouver la meilleure valeur de k\n",
    "    best_k = k_values[np.argmax(silhouette_scores)]\n",
    "    best_score = np.max(silhouette_scores)\n",
    "    best_model = models[best_k]\n",
    "    \n",
    "    logger.info(f\"Meilleur modèle: k={best_k} avec silhouette={best_score}\")\n",
    "    \n",
    "    # Générer les prédictions avec le meilleur modèle\n",
    "    results = best_model.transform(df)\n",
    "    \n",
    "    # Afficher la distribution des clusters\n",
    "    logger.info(\"Distribution des clusters:\")\n",
    "    results.groupBy(\"prediction\").count().orderBy(\"prediction\").show()\n",
    "    \n",
    "    return best_model, results, best_k, silhouette_scores\n",
    "\n",
    "# Exécuter l'entraînement du modèle K-means\n",
    "kmeans_model, cluster_results, best_k, silhouette_scores = train_kmeans_model(\n",
    "    behavior_clustering_df, feature_col=\"features\", k_values=range(2, 9)\n",
    ")\n",
    "\n",
    "# Visualiser les scores silhouette (à travers le logging)\n",
    "for k, score in zip(range(2, 9), silhouette_scores):\n",
    "    logger.info(f\"K={k}, Silhouette Score={score}\")\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f\"{MODELS_DIR}/kmeans_behavioral_{best_k}_clusters_{timestamp_str}\"\n",
    "kmeans_model.save(model_path)\n",
    "logger.info(f\"Modèle K-means sauvegardé: {model_path}\")\n",
    "\n",
    "# Analyser les caractéristiques des clusters\n",
    "def analyze_clusters(df, cluster_col=\"prediction\", feature_cols=None):\n",
    "    \"\"\"Analyse les caractéristiques des clusters\"\"\"\n",
    "    logger.info(\"Analyse des caractéristiques des clusters\")\n",
    "    \n",
    "    # Calculer les moyennes par cluster\n",
    "    agg_exprs = [count(\"*\").alias(\"cluster_size\")]\n",
    "    for col_name in feature_cols:\n",
    "        agg_exprs.append(avg(col(col_name)).alias(f\"avg_{col_name}\"))\n",
    "\n",
    "    cluster_stats = df.groupBy(cluster_col).agg(*agg_exprs).orderBy(cluster_col)\n",
    "\n",
    "    # Afficher les statistiques par cluster\n",
    "    cluster_stats.show(truncate=False)\n",
    "    return cluster_stats\n",
    "\n",
    "# Analyser les clusters obtenus\n",
    "cluster_stats = analyze_clusters(\n",
    "    cluster_results.select(\"user_id\", \"prediction\", *behavior_features),\n",
    "    cluster_col=\"prediction\", \n",
    "    feature_cols=behavior_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4a807ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:55:02,073 - INFO - Combinaison des segmentations RFM et clustering\n",
      "2025-05-18 16:55:02,138 - INFO - Distribution conjointe des segments RFM et clusters comportementaux:\n",
      "2025-05-18 16:55:04,632 - INFO - Distribution des segments comportementaux:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|behavior_cluster|Potential Loyalists|\n",
      "+----------------+-------------------+\n",
      "|1               |9722               |\n",
      "|0               |107807             |\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o12821.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 12012.0 failed 1 times, most recent failure: Lost task 5.0 in stage 12012.0 (TID 79299) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m user_clusters = cluster_results.select(\u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Combiner avec la segmentation RFM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m combined_segments = \u001b[43mcombine_segmentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrfm_segmentation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Sauvegarder les segmentations combinées\u001b[39;00m\n\u001b[32m     70\u001b[39m combined_output_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRESULTS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/combined_segmentation_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mcombine_segmentations\u001b[39m\u001b[34m(cluster_df, rfm_df)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Afficher la distribution des segments comportementaux\u001b[39;00m\n\u001b[32m     54\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mDistribution des segments comportementaux:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbehavior_segment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Calculer l'affinité entre segments RFM et comportementaux\u001b[39;00m\n\u001b[32m     58\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mAffinité entre segments RFM et comportementaux:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\sql\\dataframe.py:978\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    971\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    972\u001b[39m         message_parameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m    975\u001b[39m         },\n\u001b[32m    976\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o12821.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 12012.0 failed 1 times, most recent failure: Lost task 5.0 in stage 12012.0 (TID 79299) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "def combine_segmentations(cluster_df, rfm_df):\n",
    "    \"\"\"Combine les segmentations RFM et clustering comportemental\"\"\"\n",
    "    logger.info(\"Combinaison des segmentations RFM et clustering\")\n",
    "    \n",
    "    # Joindre les deux dataframes sur user_id\n",
    "    combined_df = cluster_df.select(\n",
    "        \"user_id\", \"prediction\"\n",
    "    ).join(\n",
    "        rfm_df.select(\"user_id\", \"rfm_segment\", \"rfm_score\", \n",
    "                      \"recency_score\", \"frequency_score\", \"monetary_score\"),\n",
    "        on=\"user_id\", \n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    # Renommer les colonnes pour plus de clarté\n",
    "    combined_df = combined_df.withColumnRenamed(\"prediction\", \"behavior_cluster\")\n",
    "    \n",
    "    # Analyser la distribution conjointe\n",
    "    logger.info(\"Distribution conjointe des segments RFM et clusters comportementaux:\")\n",
    "    pivot_table = combined_df.groupBy(\"behavior_cluster\") \\\n",
    "        .pivot(\"rfm_segment\") \\\n",
    "        .agg(count(\"*\")) \\\n",
    "        .na.fill(0)\n",
    "    \n",
    "    pivot_table.show(truncate=False)\n",
    "    \n",
    "    # Nommer les clusters comportementaux en fonction de leurs caractéristiques\n",
    "    # Cette partie nécessite une analyse manuelle des statistiques des clusters\n",
    "    \n",
    "    # Étiquettes des clusters en fonction des statistiques précédemment calculées\n",
    "    cluster_labels = {\n",
    "        # Ces étiquettes sont des exemples et doivent être adaptées en fonction des résultats réels\n",
    "        0: \"Explorateurs Occasionnels\",\n",
    "        1: \"Acheteurs Fidèles\",\n",
    "        2: \"Visiteurs Fréquents\",\n",
    "        3: \"Acheteurs à Fort Panier\",\n",
    "        4: \"Visiteurs Uniques\",\n",
    "        5: \"Convertisseurs Efficaces\",\n",
    "        6: \"Indécis (Abandon Panier)\",\n",
    "        7: \"Browsers Passifs\",\n",
    "        8: \"Acheteurs Impulsifs\"\n",
    "    }\n",
    "    \n",
    "    # Créer une fonction UDF pour appliquer les étiquettes\n",
    "    cluster_label_udf = udf(lambda cluster_id: cluster_labels.get(cluster_id, f\"Cluster {cluster_id}\"), StringType())\n",
    "    \n",
    "    # Appliquer les étiquettes aux clusters\n",
    "    combined_df = combined_df.withColumn(\n",
    "        \"behavior_segment\", \n",
    "        cluster_label_udf(col(\"behavior_cluster\"))\n",
    "    )\n",
    "    \n",
    "    # Afficher la distribution des segments comportementaux\n",
    "    logger.info(\"Distribution des segments comportementaux:\")\n",
    "    combined_df.groupBy(\"behavior_segment\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
    "    \n",
    "    # Calculer l'affinité entre segments RFM et comportementaux\n",
    "    logger.info(\"Affinité entre segments RFM et comportementaux:\")\n",
    "    combined_df.groupBy(\"behavior_segment\", \"rfm_segment\").count().orderBy(desc(\"count\")).show(20, truncate=False)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Joindre les résultats de clustering avec les utilisateurs d'origine\n",
    "user_clusters = cluster_results.select(\"user_id\", \"prediction\")\n",
    "\n",
    "# Combiner avec la segmentation RFM\n",
    "combined_segments = combine_segmentations(user_clusters, rfm_segmentation)\n",
    "\n",
    "# Sauvegarder les segmentations combinées\n",
    "combined_output_path = f\"{RESULTS_DIR}/combined_segmentation_{timestamp_str}.parquet\"\n",
    "combined_segments.write.mode(\"overwrite\").format(\"parquet\").save(combined_output_path)\n",
    "logger.info(f\"Segmentations combinées sauvegardées: {combined_output_path}\")\n",
    "\n",
    "# Créer un dataframe de profils utilisateurs pour les recommandations\n",
    "user_profiles = combined_segments.select(\n",
    "    \"user_id\", \"behavior_cluster\", \"behavior_segment\", \"rfm_segment\", \"rfm_score\"\n",
    ")\n",
    "\n",
    "# Afficher quelques exemples de profils utilisateurs\n",
    "logger.info(\"Exemples de profils utilisateurs:\")\n",
    "user_profiles.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d1ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:10:47,530 - INFO - Préparation des données pour le système de recommandation\n",
      "2025-05-18 16:10:51,730 - INFO - Aperçu des données préparées pour ALS:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+-----------+----------+-----------------+\n",
      "|  user_id|user_idx|product_id|product_idx|event_type|interaction_score|\n",
      "+---------+--------+----------+-----------+----------+-----------------+\n",
      "|541312140|107029.0|  44600062|    28727.0|      view|                1|\n",
      "|554748717| 88872.0|   3900821|     1179.0|      view|                1|\n",
      "|519107250|  8415.0|  17200506|     3015.0|      view|                1|\n",
      "|550050854| 42331.0|   1307067|      117.0|      view|                1|\n",
      "|535871217| 27228.0|   1004237|       40.0|      view|                1|\n",
      "+---------+--------+----------+-----------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:10:56,996 - INFO - Statistiques du dataset de recommandation:\n",
      "2025-05-18 16:10:56,997 - INFO - Utilisateurs uniques: 163024\n",
      "2025-05-18 16:10:56,997 - INFO - Produits uniques: 63322\n",
      "2025-05-18 16:10:56,998 - INFO - Interactions totales: 1000000\n",
      "2025-05-18 16:10:56,999 - INFO - Densité: 0.009687%\n",
      "2025-05-18 16:10:59,244 - INFO - Ensemble d'entraînement: 800279 lignes\n",
      "2025-05-18 16:11:00,726 - INFO - Ensemble de test: 199721 lignes\n"
     ]
    }
   ],
   "source": [
    "def prepare_recommendation_data(recom_df):\n",
    "    \"\"\"Prépare les données pour le système de recommandation\"\"\"\n",
    "    logger.info(\"Préparation des données pour le système de recommandation\")\n",
    "    \n",
    "    # Indexer les utilisateurs et produits pour ALS\n",
    "    user_indexer = StringIndexer(\n",
    "        inputCol=\"user_id\", \n",
    "        outputCol=\"user_idx\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    product_indexer = StringIndexer(\n",
    "        inputCol=\"product_id\", \n",
    "        outputCol=\"product_idx\",\n",
    "        handleInvalid=\"skip\"\n",
    "    )\n",
    "    \n",
    "    # Créer le pipeline de préparation\n",
    "    pipeline = Pipeline(stages=[user_indexer, product_indexer])\n",
    "    pipeline_model = pipeline.fit(recom_df)\n",
    "    als_df = pipeline_model.transform(recom_df)\n",
    "    \n",
    "    # Afficher un aperçu des données préparées\n",
    "    logger.info(\"Aperçu des données préparées pour ALS:\")\n",
    "    als_df.select(\"user_id\", \"user_idx\", \"product_id\", \"product_idx\", \n",
    "                 \"event_type\", \"interaction_score\").show(5)\n",
    "    \n",
    "    # Calculer quelques statistiques utiles\n",
    "    unique_users = als_df.select(\"user_id\").distinct().count()\n",
    "    unique_products = als_df.select(\"product_id\").distinct().count()\n",
    "    total_interactions = als_df.count()\n",
    "    \n",
    "    logger.info(f\"Statistiques du dataset de recommandation:\")\n",
    "    logger.info(f\"Utilisateurs uniques: {unique_users}\")\n",
    "    logger.info(f\"Produits uniques: {unique_products}\")\n",
    "    logger.info(f\"Interactions totales: {total_interactions}\")\n",
    "    logger.info(f\"Densité: {total_interactions / (unique_users * unique_products) * 100:.6f}%\")\n",
    "    \n",
    "    # Diviser les données en ensembles d'entraînement et de test\n",
    "    train_df, test_df = als_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    logger.info(f\"Ensemble d'entraînement: {train_df.count()} lignes\")\n",
    "    logger.info(f\"Ensemble de test: {test_df.count()} lignes\")\n",
    "    \n",
    "    return train_df, test_df, pipeline_model\n",
    "\n",
    "# Préparer les données pour les recommandations\n",
    "als_train_df, als_test_df, als_pipeline = prepare_recommendation_data(recommendation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe34f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:24:25,139 - INFO - Entraînement du modèle ALS\n",
      "2025-05-18 16:24:25,141 - INFO - Essai avec rank=10, regParam=0.01\n",
      "2025-05-18 16:25:32,342 - INFO - RMSE pour rank=10, regParam=0.01: 1.687387446788512\n",
      "2025-05-18 16:25:32,343 - INFO - Essai avec rank=10, regParam=0.1\n",
      "2025-05-18 16:26:20,404 - INFO - RMSE pour rank=10, regParam=0.1: 1.6891501157680544\n",
      "2025-05-18 16:26:20,405 - INFO - Essai avec rank=10, regParam=1.0\n",
      "2025-05-18 16:27:09,275 - INFO - RMSE pour rank=10, regParam=1.0: 1.730507037766797\n",
      "2025-05-18 16:27:09,277 - INFO - Essai avec rank=20, regParam=0.01\n",
      "2025-05-18 16:28:06,149 - INFO - RMSE pour rank=20, regParam=0.01: 1.6673586953989568\n",
      "2025-05-18 16:28:06,151 - INFO - Essai avec rank=20, regParam=0.1\n",
      "2025-05-18 16:28:56,647 - INFO - RMSE pour rank=20, regParam=0.1: 1.669009329707226\n",
      "2025-05-18 16:28:56,648 - INFO - Essai avec rank=20, regParam=1.0\n",
      "2025-05-18 16:29:52,043 - INFO - RMSE pour rank=20, regParam=1.0: 1.7204041942327433\n",
      "2025-05-18 16:29:52,045 - INFO - Essai avec rank=30, regParam=0.01\n",
      "2025-05-18 16:30:57,164 - INFO - RMSE pour rank=30, regParam=0.01: 1.6531043709586275\n",
      "2025-05-18 16:30:57,165 - INFO - Essai avec rank=30, regParam=0.1\n",
      "2025-05-18 16:32:03,114 - INFO - RMSE pour rank=30, regParam=0.1: 1.6552328092844721\n",
      "2025-05-18 16:32:03,115 - INFO - Essai avec rank=30, regParam=1.0\n",
      "2025-05-18 16:33:02,363 - INFO - RMSE pour rank=30, regParam=1.0: 1.71459763361837\n",
      "2025-05-18 16:33:02,364 - INFO - Meilleur modèle - RMSE: 1.6531043709586275\n",
      "2025-05-18 16:33:05,185 - INFO - Modèle ALS sauvegardé: ./models/als_model_20250518_163302\n"
     ]
    }
   ],
   "source": [
    "def train_als_model(train_df, test_df):\n",
    "    \"\"\"Entraîne et évalue le modèle ALS\"\"\"\n",
    "    logger.info(\"Entraînement du modèle ALS\")\n",
    "    \n",
    "    # Hyperparamètres à tester\n",
    "    als_models = {}\n",
    "    ranks = [10, 20, 30]\n",
    "    reg_params = [0.01, 0.1, 1.0]\n",
    "    \n",
    "    best_model = None\n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    for rank in ranks:\n",
    "        for reg_param in reg_params:\n",
    "            logger.info(f\"Essai avec rank={rank}, regParam={reg_param}\")\n",
    "            \n",
    "            als = ALS(\n",
    "                rank=rank,\n",
    "                maxIter=15,\n",
    "                regParam=reg_param,\n",
    "                userCol=\"user_idx\",\n",
    "                itemCol=\"product_idx\",\n",
    "                ratingCol=\"interaction_score\",\n",
    "                coldStartStrategy=\"drop\",\n",
    "                nonnegative=True,\n",
    "                implicitPrefs=True\n",
    "            )\n",
    "            \n",
    "            model = als.fit(train_df)\n",
    "            predictions = model.transform(test_df)\n",
    "            \n",
    "            # Évaluation\n",
    "            evaluator = RegressionEvaluator(\n",
    "                metricName=\"rmse\",\n",
    "                labelCol=\"interaction_score\",\n",
    "                predictionCol=\"prediction\"\n",
    "            )\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            \n",
    "            logger.info(f\"RMSE pour rank={rank}, regParam={reg_param}: {rmse}\")\n",
    "            \n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_model = model\n",
    "    \n",
    "    logger.info(f\"Meilleur modèle - RMSE: {best_rmse}\")\n",
    "    \n",
    "    # Sauvegarde du modèle\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = f\"{MODELS_DIR}/als_model_{timestamp_str}\"\n",
    "    best_model.save(model_path)\n",
    "    logger.info(f\"Modèle ALS sauvegardé: {model_path}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Exécuter l'entraînement ALS\n",
    "als_model = train_als_model(als_train_df, als_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11eddbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:40:42,419 - INFO - Préparation des caractéristiques produits\n"
     ]
    }
   ],
   "source": [
    "def prepare_content_based_features(product_df):\n",
    "    \"\"\"Prépare les caractéristiques produits pour le content-based filtering\"\"\"\n",
    "    logger.info(\"Préparation des caractéristiques produits\")\n",
    "    \n",
    "    # Combiner les métadonnées\n",
    "    product_features = product_df.withColumn(\n",
    "        \"product_features\",\n",
    "        concat_ws(\" \", \n",
    "            col(\"category_code\"), \n",
    "            col(\"brand\"), \n",
    "            col(\"price\").cast(\"string\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # TF-IDF\n",
    "    tokenizer = Tokenizer(inputCol=\"product_features\", outputCol=\"tokens\")\n",
    "    hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=[tokenizer, hashing_tf, idf])\n",
    "    model = pipeline.fit(product_features)\n",
    "    product_features = model.transform(product_features)\n",
    "    \n",
    "    return product_features\n",
    "\n",
    "# Utilisation\n",
    "product_features = prepare_content_based_features(product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311226f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:54:03,738 - INFO - Silhouette Score pour RFM amélioré: 0.9795136175414187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+\n",
      "|  user_id|rfm_score|prediction|\n",
      "+---------+---------+----------+\n",
      "|337535108|      0.0|         0|\n",
      "|410824220|      0.0|         0|\n",
      "|418592979|      0.0|         0|\n",
      "|420339201|      0.0|         0|\n",
      "|430276841|      0.0|         0|\n",
      "+---------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_idx` cannot be resolved. Did you mean one of the following? [`user_id`].;\n'Project ['user_idx]\n+- LogicalRDD [user_id#22230], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Exemple d'utilisation des recommandations hybrides\u001b[39;00m\n\u001b[32m    105\u001b[39m sample_user = \u001b[38;5;28mstr\u001b[39m(user_behavior_df.first().user_id)  \u001b[38;5;66;03m# Conversion explicite en string\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m hybrid_recs = \u001b[43mhybrid_recommendations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Doit être un type primitif (str/int)\u001b[39;49;00m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mals_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mals_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproduct_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproduct_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_recs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m    112\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mRecommandations hybrides pour l\u001b[39m\u001b[33m'\u001b[39m\u001b[33mutilisateur \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m, sample_user)\n\u001b[32m    115\u001b[39m hybrid_recs.show(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mhybrid_recommendations\u001b[39m\u001b[34m(user_id, als_model, product_features, num_recs)\u001b[39m\n\u001b[32m     25\u001b[39m user_df = spark.createDataFrame([(user_id,)], [\u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Récupération des recommandations ALS (avec gestion de la sérialisation)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m als_recs = \u001b[43mals_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecommendForUserSubset\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_recs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Extraction du vecteur utilisateur sous forme de liste Python\u001b[39;00m\n\u001b[32m     31\u001b[39m user_vector = get_user_preferences(user_id, als_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\ml\\recommendation.py:695\u001b[39m, in \u001b[36mALSModel.recommendForUserSubset\u001b[39m\u001b[34m(self, dataset, numItems)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecommendForUserSubset\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame, numItems: \u001b[38;5;28mint\u001b[39m) -> DataFrame:\n\u001b[32m    675\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    676\u001b[39m \u001b[33;03m    Returns top `numItems` items recommended for each user id in the input data set. Note that\u001b[39;00m\n\u001b[32m    677\u001b[39m \u001b[33;03m    if there are duplicate ids in the input dataset, only one set of recommendations per unique\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    693\u001b[39m \u001b[33;03m        stored as an array of (itemCol, rating) Rows.\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_java\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecommendForUserSubset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumItems\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\ml\\wrapper.py:72\u001b[39m, in \u001b[36mJavaWrapper._call_java\u001b[39m\u001b[34m(self, name, *args)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m java_args = [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_idx` cannot be resolved. Did you mean one of the following? [`user_id`].;\n'Project ['user_idx]\n+- LogicalRDD [user_id#22230], false\n"
     ]
    }
   ],
   "source": [
    "def get_user_preferences(user_id, als_model):\n",
    "    \"\"\"Récupère le vecteur latent sous forme de liste\"\"\"\n",
    "    user_factors = als_model.userFactors.filter(col(\"id\") == user_id)\n",
    "    if user_factors.count() == 0:\n",
    "        return None\n",
    "    return user_factors.first().features.tolist()\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calcule la similarité cosinus entre deux vecteurs\"\"\"\n",
    "    return float(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "def combine_recommendations(als_recs, content_recs, alpha=0.7):\n",
    "    \"\"\"Combine les recommandations avec pondération\"\"\"\n",
    "    combined = als_recs.union(content_recs).groupBy(\"product_id\").agg(\n",
    "        (alpha * max(\"rating\")).alias(\"als_score\"),\n",
    "        ((1 - alpha) * max(\"similarity\")).alias(\"content_score\"),\n",
    "        (alpha * max(\"rating\") + (1 - alpha) * max(\"similarity\")).alias(\"combined_score\")\n",
    "    ).orderBy(desc(\"combined_score\"))\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def hybrid_recommendations(user_id, als_model, product_features, num_recs=10):\n",
    "    \"\"\"Combine les recommandations collaboratives et basées sur le contenu\"\"\"\n",
    "    # Conversion préalable du user_id en DataFrame\n",
    "    user_df = spark.createDataFrame([(user_id,)], [\"user_id\"])\n",
    "    \n",
    "    # Récupération des recommandations ALS (avec gestion de la sérialisation)\n",
    "    als_recs = als_model.recommendForUserSubset(user_df, num_recs)\n",
    "    \n",
    "    # Extraction du vecteur utilisateur sous forme de liste Python\n",
    "    user_vector = get_user_preferences(user_id, als_model)\n",
    "    if user_vector is None:\n",
    "        return als_recs\n",
    "    \n",
    "    user_array = user_vector.tolist()  # Conversion en liste sérialisable\n",
    "    \n",
    "    # Définition de l'UDF avec capture de la liste Python\n",
    "    def similarity_udf_wrapper(features):\n",
    "        user_vec = np.array(user_array)\n",
    "        product_vec = np.array(features.toArray())\n",
    "        dot_product = np.dot(user_vec, product_vec)\n",
    "        norm_product = np.linalg.norm(product_vec)\n",
    "        return float(dot_product / (np.linalg.norm(user_vec) * norm_product))\n",
    "    \n",
    "    similarity_udf = udf(similarity_udf_wrapper, FloatType())\n",
    "    \n",
    "    # Calcul des similarités\n",
    "    content_recs = product_features.withColumn(\n",
    "        \"similarity\", similarity_udf(col(\"features\"))\n",
    "    ).select(\"product_id\", \"similarity\").orderBy(desc(\"similarity\")).limit(num_recs)\n",
    "    \n",
    "    # Combinaison des résultats\n",
    "    als_df = als_recs.selectExpr(\n",
    "        \"product_id\", \n",
    "        \"explode(recommendations).rating\"\n",
    "    )\n",
    "    \n",
    "    return combine_recommendations(als_df, content_recs)    \n",
    "\n",
    "### 2. Implémentation complète de enhanced_rfm_segmentation ###\n",
    "def enhanced_rfm_segmentation(user_df):\n",
    "    \"\"\"Amélioration de la segmentation RFM avec validation\"\"\"\n",
    "    # Calcul des percentiles dynamiques\n",
    "    window_r = Window.orderBy(desc(\"recency\"))\n",
    "    window_fm = Window.orderBy(col(\"frequency\"), col(\"monetary\"))\n",
    "    \n",
    "    rfm_df = user_df.withColumn(\n",
    "        \"r_percentile\", percent_rank().over(window_r)\n",
    "    ).withColumn(\n",
    "        \"f_percentile\", percent_rank().over(window_fm)\n",
    "    ).withColumn(\n",
    "        \"m_percentile\", percent_rank().over(window_fm)\n",
    "    )\n",
    "    \n",
    "    # Calcul du score pondéré\n",
    "    rfm_df = rfm_df.withColumn(\n",
    "        \"rfm_score\",\n",
    "        (col(\"r_percentile\") * 0.4 + \n",
    "         col(\"f_percentile\") * 0.3 + \n",
    "         col(\"m_percentile\") * 0.3)\n",
    "    )\n",
    "    \n",
    "    # Clustering avec validation\n",
    "    assembler = VectorAssembler(inputCols=[\"rfm_score\"], outputCol=\"features\")\n",
    "    kmeans = KMeans(k=5, seed=42)\n",
    "    pipeline = Pipeline(stages=[assembler, kmeans])\n",
    "    model = pipeline.fit(rfm_df)\n",
    "    \n",
    "    # Évaluation\n",
    "    predictions = model.transform(rfm_df)\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    logger.info(f\"Silhouette Score pour RFM amélioré: {silhouette}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "### 3. Intégration dans le flux principal ###\n",
    "# Après l'entraînement du modèle ALS et la segmentation RFM initiale\n",
    "\n",
    "# Segmentation RFM améliorée\n",
    "enhanced_rfm = enhanced_rfm_segmentation(user_behavior_df)\n",
    "enhanced_rfm.select(\"user_id\", \"rfm_score\", \"prediction\").show(5)\n",
    "\n",
    "# Exemple d'utilisation des recommandations hybrides\n",
    "sample_user = str(user_behavior_df.first().user_id)\n",
    "\n",
    "hybrid_recs = hybrid_recommendations(\n",
    "    user_id=sample_user,\n",
    "    als_model=als_model,\n",
    "    product_features=product_features,\n",
    "    num_recs=10\n",
    ")\n",
    "\n",
    "logger.info(\"Recommandations hybrides pour l'utilisateur %s:\", sample_user)\n",
    "hybrid_recs.show(10)\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "hybrid_recs.write.mode(\"overwrite\").parquet(f\"{RESULTS_DIR}/hybrid_recommendations\")\n",
    "enhanced_rfm.write.mode(\"overwrite\").parquet(f\"{RESULTS_DIR}/enhanced_rfm_segments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
