{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfc8b7a",
   "metadata": {},
   "source": [
    "# Script de prétraitement des données e-commerce pour l'analyse comportementale et le système de recommandation Ce script structure le traitement en phases distinctes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03c449",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques et configuration initiale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef5522",
   "metadata": {},
   "source": [
    "## importations et la configuration initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52cdb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, when, month, dayofweek, hour, minute, second, dayofmonth, countDistinct,\n",
    "    desc, sum, avg, min, max, datediff, lit, to_date, date_format, \n",
    "    collect_list, struct,unix_timestamp\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, TimestampType, IntegerType\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"preprocessing.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paramètres globaux\n",
    "input_file = \"../data/2019-Oct_reduit.csv\"\n",
    "# input_file = \"../data/2019-Oct.csv\"\n",
    "output_directory = \"./data/processed\"\n",
    "\n",
    "# Création du répertoire de sortie s'il n'existe pas\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Définition du schéma pour les données\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"category_id\", StringType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Crée et retourne une session Spark avec la configuration appropriée\n",
    "    \n",
    "    Returns:\n",
    "        SparkSession: Session Spark configurée\n",
    "    \"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"E-commerce Data Analysis\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f08699",
   "metadata": {},
   "source": [
    "## Chargement et exploration des données e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df1cfca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark_session=None, input_path=input_file, schema=schema):\n",
    "    \"\"\"\n",
    "    Charge les données depuis le fichier CSV avec le schéma défini\n",
    "    \n",
    "    Args:\n",
    "        spark_session: Session Spark à utiliser (crée une nouvelle si None)\n",
    "        input_path: Chemin du fichier à charger\n",
    "        schema: Schéma à appliquer\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame Spark contenant les données chargées\n",
    "    \"\"\"\n",
    "    if spark_session is None:\n",
    "        spark_session = create_spark_session()\n",
    "    \n",
    "    logger.info(f\"Chargement des données depuis {input_path}\")\n",
    "    try:\n",
    "        df = spark_session.read.csv(\n",
    "            input_path,\n",
    "            header=True,\n",
    "            schema=schema\n",
    "        )\n",
    "        logger.info(f\"Données chargées : {df.count()} lignes\")\n",
    "        \n",
    "        # Afficher un aperçu des données\n",
    "        logger.info(\"Aperçu des données:\")\n",
    "        df.show(5)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors du chargement des données : {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def explore_data(df):\n",
    "    \"\"\"\n",
    "    Exploration initiale des données\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame Spark à explorer\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        logger.error(\"Les données n'ont pas été chargées\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Exploration initiale des données\")\n",
    "    logger.info(f\"Nombre de lignes: {df.count()}\")\n",
    "    logger.info(f\"Nombre de colonnes: {len(df.columns)}\")\n",
    "    \n",
    "    # Afficher le schéma\n",
    "    logger.info(\"Schéma du DataFrame:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Calculer les statistiques descriptives \n",
    "    logger.info(\"Statistiques descriptives:\")\n",
    "    df.describe().show()\n",
    "    \n",
    "    # Calculer le nombre de valeurs manquantes par colonne\n",
    "    logger.info(\"Valeurs manquantes par colonne:\")\n",
    "    for column in df.columns:\n",
    "        missing_count = df.filter(col(column).isNull()).count()\n",
    "        missing_percentage = (missing_count / df.count()) * 100\n",
    "        logger.info(f\"{column}: {missing_count} ({missing_percentage:.2f}%)\")\n",
    "    \n",
    "    # Distribution des types d'événements\n",
    "    logger.info(\"Distribution des types d'événements:\")\n",
    "    df.groupBy(\"event_type\").count().orderBy(desc(\"count\")).show()\n",
    "    \n",
    "    # Distribution des catégories principales\n",
    "    logger.info(\"Top 10 des catégories:\")\n",
    "    df.groupBy(\"category_code\").count().orderBy(desc(\"count\")).limit(10).show()\n",
    "    \n",
    "    # Distribution des marques principales\n",
    "    logger.info(\"Top 10 des marques:\")\n",
    "    df.groupBy(\"brand\").count().orderBy(desc(\"count\")).limit(10).show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Exécuter le chargement et l'exploration\n",
    "# if __name__ == \"__main__\":\n",
    "#     spark = create_spark_session()\n",
    "#     df = load_data(spark)\n",
    "#     explore_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3313b65",
   "metadata": {},
   "source": [
    "## Prétraitement des données e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f710c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Prétraitement des données : extraction des caractéristiques temporelles,\n",
    "    traitement des valeurs manquantes, etc.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame Spark à prétraiter\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame Spark prétraité\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        logger.error(\"Les données n'ont pas été chargées\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Prétraitement des données\")\n",
    "    \n",
    "    # 1. Extraction des caractéristiques temporelles\n",
    "    logger.info(\"Extraction des caractéristiques temporelles\")\n",
    "    df_temp = df.withColumn(\"hour\", hour(\"event_time\")) \\\n",
    "        .withColumn(\"minute\", minute(\"event_time\")) \\\n",
    "        .withColumn(\"second\", second(\"event_time\")) \\\n",
    "        .withColumn(\"day\", dayofmonth(\"event_time\")) \\\n",
    "        .withColumn(\"month\", month(\"event_time\")) \\\n",
    "        .withColumn(\"dayofweek\", dayofweek(\"event_time\")) \\\n",
    "        .withColumn(\"date\", date_format(\"event_time\", \"yyyy-MM-dd\")) \\\n",
    "        .withColumn(\"hour_bucket\", date_format(\"event_time\", \"yyyy-MM-dd HH:00:00\"))\n",
    "    \n",
    "    # 2. Traitement des valeurs manquantes\n",
    "    logger.info(\"Traitement des valeurs manquantes\")\n",
    "    df_temp = df_temp.withColumn(\n",
    "        \"category_code\",\n",
    "        when(col(\"category_code\").isNull() | (col(\"category_code\") == \"NaN\"),\n",
    "            \"unknown\").otherwise(col(\"category_code\"))\n",
    "    )\n",
    "    \n",
    "    df_temp = df_temp.withColumn(\n",
    "        \"brand\",\n",
    "        when(col(\"brand\").isNull(), \"unknown\").otherwise(col(\"brand\"))\n",
    "    )\n",
    "    \n",
    "    # 3. Nettoyage des prix (valeurs négatives ou nulles)\n",
    "    logger.info(\"Nettoyage des prix\")\n",
    "    df_temp = df_temp.withColumn(\n",
    "        \"price\",\n",
    "        when(col(\"price\").isNull() | (col(\"price\") <= 0), None).otherwise(col(\"price\"))\n",
    "    )\n",
    "    \n",
    "    # Afficher un aperçu des données prétraitées\n",
    "    logger.info(\"Aperçu des données prétraitées:\")\n",
    "    df_temp.show(5)\n",
    "    \n",
    "    # Afficher un résumé après prétraitement\n",
    "    logger.info(\"Statistiques après prétraitement:\")\n",
    "    df_temp.describe().show()\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "def compute_user_behavior(cleaned_df):\n",
    "    \"\"\"\n",
    "    Calcul des métriques de comportement utilisateur pour la segmentation et l'analyse\n",
    "    \n",
    "    Args:\n",
    "        cleaned_df: DataFrame Spark nettoyé\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame Spark des comportements utilisateur\n",
    "    \"\"\"\n",
    "    if cleaned_df is None:\n",
    "        logger.error(\"Les données n'ont pas été prétraitées\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Calcul des métriques de comportement utilisateur\")\n",
    "    \n",
    "    # Agrégation des comportements par utilisateur avec métriques enrichies pour la segmentation RFM\n",
    "    user_behavior_df = cleaned_df.groupBy(\"user_id\").agg(\n",
    "        count(\"*\").alias(\"nb_events\"),\n",
    "        sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"nb_views\"),\n",
    "        sum(when(col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"nb_carts\"),\n",
    "        sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"nb_purchases\"),\n",
    "        sum(when(col(\"event_type\") == \"remove_from_cart\", 1).otherwise(0)).alias(\"nb_removes\"),\n",
    "        avg(\"price\").alias(\"avg_price_viewed\"),\n",
    "        avg(when(col(\"event_type\") == \"purchase\", col(\"price\")).otherwise(None)).alias(\"avg_price_purchased\"),\n",
    "        countDistinct(\"user_session\").alias(\"nb_sessions\"),\n",
    "        min(\"event_time\").alias(\"first_seen\"),\n",
    "        max(\"event_time\").alias(\"last_seen\"),\n",
    "        # Métriques pour la segmentation RFM\n",
    "        datediff(lit(\"2019-10-31\"), max(\"event_time\")).alias(\"recency\"),\n",
    "        countDistinct(when(col(\"event_type\") == \"purchase\", col(\"date\")).otherwise(None)).alias(\"frequency\"),\n",
    "        sum(when(col(\"event_type\") == \"purchase\", col(\"price\")).otherwise(0)).alias(\"monetary\"),\n",
    "        # Liste des catégories consultées\n",
    "        collect_list(when(col(\"event_type\") == \"view\", col(\"category_code\")).otherwise(None)).alias(\"viewed_categories\"),\n",
    "        # Liste des marques consultées\n",
    "        collect_list(when(col(\"event_type\") == \"view\", col(\"brand\")).otherwise(None)).alias(\"viewed_brands\")\n",
    "    )\n",
    "    \n",
    "    # Calcul des métriques dérivées\n",
    "    user_behavior_df = user_behavior_df.withColumn(\n",
    "        \"conversion_rate\",\n",
    "        when(col(\"nb_views\") > 0, col(\"nb_purchases\") / col(\"nb_views\")).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    user_behavior_df = user_behavior_df.withColumn(\n",
    "        \"cart_abandonment\",\n",
    "        when(col(\"nb_carts\") > 0, (col(\"nb_carts\") - col(\"nb_purchases\")) / col(\"nb_carts\")).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    user_behavior_df = user_behavior_df.withColumn(\n",
    "        \"engagement_days\",\n",
    "        datediff(col(\"last_seen\"), col(\"first_seen\")) + 1\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Métriques de comportement utilisateur calculées\")\n",
    "    logger.info(\"Aperçu des comportements utilisateur:\")\n",
    "    user_behavior_df.show(5)\n",
    "    \n",
    "    return user_behavior_df\n",
    "\n",
    "def prepare_recommendation_data(cleaned_df):\n",
    "    \"\"\"\n",
    "    Prépare les données pour le système de recommandation\n",
    "    \n",
    "    Args:\n",
    "        cleaned_df: DataFrame Spark nettoyé\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (recommandation_df, produit_df): DataFrames pour les systèmes de recommandation\n",
    "    \"\"\"\n",
    "    if cleaned_df is None:\n",
    "        logger.error(\"Les données n'ont pas été prétraitées\")\n",
    "        return None, None\n",
    "    \n",
    "    logger.info(\"Préparation des données pour le système de recommandation\")\n",
    "    \n",
    "    # DataFrame pour le filtrage collaboratif (ALS)\n",
    "    recommandation_df = cleaned_df.filter(col(\"event_type\").isin([\"view\", \"cart\", \"purchase\"])).select(\n",
    "        \"user_id\",\n",
    "        \"product_id\",\n",
    "        \"event_type\",\n",
    "        \"price\",\n",
    "        \"event_time\",\n",
    "        # Score implicite basé sur l'interaction\n",
    "        when(col(\"event_type\") == \"view\", 1)\n",
    "        .when(col(\"event_type\") == \"cart\", 5)\n",
    "        .when(col(\"event_type\") == \"purchase\", 10)\n",
    "        .otherwise(0).alias(\"interaction_score\")\n",
    "    )\n",
    "    \n",
    "    # DataFrame des produits pour les recommandations basées sur le contenu\n",
    "    produit_df = cleaned_df.select(\n",
    "        \"product_id\", \n",
    "        \"category_id\", \n",
    "        \"category_code\", \n",
    "        \"brand\", \n",
    "        \"price\"\n",
    "    ).distinct()\n",
    "    \n",
    "    logger.info(\"Aperçu des données de recommandation:\")\n",
    "    recommandation_df.show(5)\n",
    "    logger.info(\"Aperçu des données produits:\")\n",
    "    produit_df.show(5)\n",
    "    \n",
    "    return recommandation_df, produit_df\n",
    "\n",
    "def prepare_time_series_data(cleaned_df):\n",
    "    \"\"\"\n",
    "    Prépare les données temporelles pour les analyses et simulations\n",
    "    \n",
    "    Args:\n",
    "        cleaned_df: DataFrame Spark nettoyé\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame agrégé par heures pour analyses temporelles\n",
    "    \"\"\"\n",
    "    if cleaned_df is None:\n",
    "        logger.error(\"Les données n'ont pas été prétraitées\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Préparation des données pour analyses temporelles\")\n",
    "    \n",
    "    # Agrégation horaire des événements\n",
    "    hourly_events = cleaned_df.groupBy(\"hour_bucket\").agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "        sum(when(col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"carts\"),\n",
    "        sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\"),\n",
    "        sum(when(col(\"event_type\") == \"remove_from_cart\", 1).otherwise(0)).alias(\"removes\"),\n",
    "        avg(\"price\").alias(\"avg_price\")\n",
    "    ).orderBy(\"hour_bucket\")\n",
    "    \n",
    "    logger.info(\"Aperçu des données temporelles:\")\n",
    "    hourly_events.show(5)\n",
    "    \n",
    "    return hourly_events\n",
    "\n",
    "# Exécuter le prétraitement\n",
    "# if __name__ == \"__main__\":\n",
    "#     spark = create_spark_session()\n",
    "#     df = load_data(spark)\n",
    "#     cleaned_df = preprocess_data(df)\n",
    "#     user_behavior_df = compute_user_behavior(cleaned_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620bd88",
   "metadata": {},
   "source": [
    "## Analyse et visualisation des données e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05522a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analytics(cleaned_df):\n",
    "    \"\"\"\n",
    "    Génère des statistiques agrégées pour l'analyse\n",
    "    \n",
    "    Args:\n",
    "        cleaned_df: DataFrame Spark nettoyé    \n",
    "    Returns:\n",
    "        Tuples de données pour la visualisation\n",
    "    \"\"\"\n",
    "    if cleaned_df is None:\n",
    "        logger.error(\"Les données n'ont pas été prétraitées\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Génération des statistiques pour analyse\")\n",
    "    \n",
    "    # 1. Distribution des types d'événements\n",
    "    event_counts = cleaned_df.groupBy(\"event_type\") \\\n",
    "        .count() \\\n",
    "        .orderBy(desc(\"count\"))\n",
    "    \n",
    "    # Conversion en liste Python pour la visualisation\n",
    "    event_counts_list = [(row['event_type'], row['count']) \n",
    "                         for row in event_counts.collect()]\n",
    "    \n",
    "    # 2. Distribution horaire des événements\n",
    "    hourly_events = cleaned_df.groupBy(\"hour\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"hour\")\n",
    "    \n",
    "    hourly_events_list = [(row['hour'], row['count']) \n",
    "                          for row in hourly_events.collect()]\n",
    "    \n",
    "    # 3. Top catégories\n",
    "    top_categories = cleaned_df.groupBy(\"category_code\") \\\n",
    "        .count() \\\n",
    "        .orderBy(desc(\"count\")) \\\n",
    "        .limit(10)\n",
    "    \n",
    "    top_categories_list = [(row['category_code'], row['count']) \n",
    "                           for row in top_categories.collect()]\n",
    "    \n",
    "    # 4. Top marques\n",
    "    top_brands = cleaned_df.groupBy(\"brand\") \\\n",
    "        .count() \\\n",
    "        .orderBy(desc(\"count\")) \\\n",
    "        .limit(10)\n",
    "    \n",
    "    top_brands_list = [(row['brand'], row['count']) \n",
    "                       for row in top_brands.collect()]\n",
    "    \n",
    "    # 5. Statistiques de prix\n",
    "    price_stats = cleaned_df.select(\"price\").summary(\n",
    "        \"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"\n",
    "    )\n",
    "    \n",
    "    price_stats_list = [(row['summary'], row['price']) \n",
    "                         for row in price_stats.collect()]\n",
    "    \n",
    "    logger.info(\"Statistiques d'analyse générées\")\n",
    "    \n",
    "    return (event_counts_list, hourly_events_list, \n",
    "            top_categories_list, top_brands_list, price_stats_list)\n",
    "\n",
    "def visualize_data(analytics_data, output_dir=output_directory, show_plots=True):\n",
    "    \"\"\"\n",
    "    Crée des visualisations à partir des données agrégées\n",
    "    \n",
    "    Args:\n",
    "        analytics_data: Tuple de données d'analyse\n",
    "            (event_counts, hourly_events, top_categories, top_brands, price_stats)\n",
    "        output_dir: Répertoire de sortie pour les visualisations\n",
    "    \"\"\"\n",
    "    # Déballage des données d'analyse\n",
    "    event_counts, hourly_events, top_categories, top_brands, price_stats = analytics_data\n",
    "    \n",
    "    # Création du répertoire de visualisation\n",
    "    vis_dir = os.path.join(output_dir, \"visualizations\")\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    \n",
    "    # Configuration du style des graphiques\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "    \n",
    "    # 1. Distribution des types d'événements\n",
    "    plt.figure()\n",
    "    ax = sns.barplot(x=[x[0] for x in event_counts], y=[x[1] for x in event_counts], palette=\"viridis\")\n",
    "    plt.title('Distribution des Types d\\'Événements', fontsize=15)\n",
    "    plt.ylabel('Nombre d\\'événements', fontsize=12)\n",
    "    plt.xlabel('Type d\\'événement', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{int(p.get_height()):,}', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha = 'center', va = 'bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, \"event_types_distribution.png\"), dpi=300)\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    logger.info(f\"Graphique de distribution des types d'événements sauvegardé\")\n",
    "    \n",
    "    # 2. Distribution horaire des événements\n",
    "    plt.figure()\n",
    "    ax = sns.lineplot(x=[x[0] for x in hourly_events], y=[x[1] for x in hourly_events], marker='o', linewidth=2.5)\n",
    "    plt.title('Distribution Horaire des Événements', fontsize=15)\n",
    "    plt.xlabel('Heure de la journée', fontsize=12)\n",
    "    plt.ylabel('Nombre d\\'événements', fontsize=12)\n",
    "    \n",
    "    # Formater l'axe des x pour afficher toutes les heures\n",
    "    plt.xticks(range(0, 24))\n",
    "    \n",
    "    # Ajouter une grille\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, \"hourly_distribution.png\"), dpi=300)\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    logger.info(\"Graphique de distribution horaire sauvegardé\")\n",
    "    \n",
    "    # 3. Top 10 des catégories\n",
    "    plt.figure()\n",
    "    category_names = [x[0] if len(str(x[0])) < 20 else str(x[0])[:17]+'...' for x in top_categories]\n",
    "    ax = sns.barplot(x=[x[1] for x in top_categories], y=category_names, palette=\"viridis\")\n",
    "    plt.title('Top 10 des Catégories les Plus Consultées', fontsize=15)\n",
    "    plt.xlabel('Nombre d\\'événements', fontsize=12)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{int(p.get_width()):,}', \n",
    "                   (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
    "                   ha = 'left', va = 'center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, \"top_categories.png\"), dpi=300)\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    logger.info(\"Graphique des top catégories sauvegardé\")\n",
    "    \n",
    "    # 4. Top 10 des marques\n",
    "    plt.figure()\n",
    "    ax = sns.barplot(x=[x[1] for x in top_brands], y=[x[0] for x in top_brands], palette=\"viridis\")\n",
    "    plt.title('Top 10 des Marques les Plus Consultées', fontsize=15)\n",
    "    plt.xlabel('Nombre d\\'événements', fontsize=12)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{int(p.get_width()):,}', \n",
    "                   (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
    "                   ha = 'left', va = 'center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, \"top_brands.png\"), dpi=300)\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    logger.info(\"Graphique des top marques sauvegardé\")\n",
    "    \n",
    "    # 5. Distribution des prix (boîte à moustaches)\n",
    "    plt.figure()    \n",
    "    logger.info(f\"Toutes les visualisations ont été sauvegardées dans {vis_dir}\")\n",
    "    \n",
    "    return vis_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed4428eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_purchase_funnel(cleaned_df):\n",
    "    \"\"\"\n",
    "    Analyse le funnel de conversion des utilisateurs (parcours d'achat)\n",
    "    \n",
    "    Args:\n",
    "        cleaned_df: DataFrame Spark nettoyé\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame contenant les métriques du funnel de conversion\n",
    "    \"\"\"\n",
    "    if cleaned_df is None:\n",
    "        logger.error(\"Les données n'ont pas été prétraitées\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Analyse du funnel de conversion\")\n",
    "    \n",
    "    # Comptage des événements par type pour le funnel global\n",
    "    funnel_global = cleaned_df.groupBy(\"event_type\") \\\n",
    "        .count() \\\n",
    "        .orderBy(desc(\"count\"))\n",
    "    \n",
    "    # Analyse du funnel par utilisateur (combien atteignent chaque étape)\n",
    "    user_funnel = cleaned_df.groupBy(\"user_id\").agg(\n",
    "        sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "        sum(when(col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"cart_additions\"),\n",
    "        sum(when(col(\"event_type\") == \"remove_from_cart\", 1).otherwise(0)).alias(\"cart_removals\"),\n",
    "        sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "    )\n",
    "    \n",
    "    # Calcul des métriques du funnel\n",
    "    funnel_metrics = user_funnel.agg(\n",
    "        count(\"user_id\").alias(\"total_users\"),\n",
    "        sum(when(col(\"views\") > 0, 1).otherwise(0)).alias(\"users_with_views\"),\n",
    "        sum(when(col(\"cart_additions\") > 0, 1).otherwise(0)).alias(\"users_with_cart\"),\n",
    "        sum(when(col(\"purchases\") > 0, 1).otherwise(0)).alias(\"users_with_purchase\")\n",
    "    )\n",
    "    \n",
    "    # Ajout des taux de conversion entre étapes\n",
    "    funnel_metrics = funnel_metrics.withColumn(\n",
    "        \"view_to_cart_rate\", \n",
    "        col(\"users_with_cart\") / col(\"users_with_views\") * 100\n",
    "    ).withColumn(\n",
    "        \"cart_to_purchase_rate\", \n",
    "        col(\"users_with_purchase\") / col(\"users_with_cart\") * 100\n",
    "    ).withColumn(\n",
    "        \"view_to_purchase_rate\", \n",
    "        col(\"users_with_purchase\") / col(\"users_with_views\") * 100\n",
    "    )\n",
    "    \n",
    "    # Analyse du funnel par catégorie\n",
    "    funnel_by_category = cleaned_df.filter(col(\"category_code\") != \"unknown\") \\\n",
    "        .groupBy(\"category_code\").agg(\n",
    "            sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "            sum(when(col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"cart_additions\"),\n",
    "            sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "        ).withColumn(\n",
    "            \"view_to_purchase_rate\",\n",
    "            when(col(\"views\") > 0, col(\"purchases\") / col(\"views\") * 100).otherwise(0)\n",
    "        ).orderBy(desc(\"view_to_purchase_rate\"))\n",
    "    \n",
    "    logger.info(\"Funnel de conversion analysé avec succès\")\n",
    "    logger.info(\"Aperçu des métriques du funnel:\")\n",
    "    funnel_metrics.show()\n",
    "    \n",
    "    logger.info(\"Top catégories par taux de conversion:\")\n",
    "    funnel_by_category.filter(col(\"views\") > 100).select(\n",
    "        \"category_code\", \"views\", \"purchases\", \"view_to_purchase_rate\"\n",
    "    ).orderBy(desc(\"view_to_purchase_rate\")).limit(10).show()\n",
    "    \n",
    "    return funnel_metrics, funnel_by_category\n",
    "\n",
    "def analyze_session_patterns(cleaned_df):\n",
    "    \"\"\"\n",
    "    Analyse les patterns de session des utilisateurs\n",
    "    \n",
    "    Args:\n",
    "        cleaned_df: DataFrame Spark nettoyé\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame contenant les métriques de session\n",
    "    \"\"\"\n",
    "    if cleaned_df is None:\n",
    "        logger.error(\"Les données n'ont pas été prétraitées\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Analyse des patterns de session utilisateurs\")\n",
    "    \n",
    "    # Statistiques par session\n",
    "    session_stats = cleaned_df.groupBy(\"user_id\", \"user_session\").agg(\n",
    "        count(\"*\").alias(\"session_events\"),\n",
    "        min(\"event_time\").alias(\"session_start\"),\n",
    "        max(\"event_time\").alias(\"session_end\"),\n",
    "        sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"session_views\"),\n",
    "        sum(when(col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"session_carts\"),\n",
    "        sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"session_purchases\"),\n",
    "        countDistinct(\"product_id\").alias(\"unique_products_viewed\"),\n",
    "        countDistinct(\"category_code\").alias(\"unique_categories_viewed\")\n",
    "    )\n",
    "    \n",
    "    # Calculer la durée des sessions en minutes\n",
    "    session_stats = session_stats.withColumn(\n",
    "        \"session_duration_minutes\", \n",
    "        (unix_timestamp(\"session_end\") - unix_timestamp(\"session_start\")) / 60\n",
    "    )\n",
    "    \n",
    "    # Calculer si la session a abouti à une conversion\n",
    "    session_stats = session_stats.withColumn(\n",
    "        \"converted\", \n",
    "        when(col(\"session_purchases\") > 0, 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Métrique d'engagement (score simple)\n",
    "    session_stats = session_stats.withColumn(\n",
    "        \"engagement_score\",\n",
    "        col(\"session_views\") + col(\"session_carts\") * 2 + col(\"session_purchases\") * 5\n",
    "    )\n",
    "    \n",
    "    # Statistiques agrégées globales\n",
    "    session_metrics = session_stats.agg(\n",
    "        count(\"*\").alias(\"total_sessions\"),\n",
    "        avg(\"session_events\").alias(\"avg_events_per_session\"),\n",
    "        avg(\"session_duration_minutes\").alias(\"avg_session_duration_minutes\"),\n",
    "        avg(\"unique_products_viewed\").alias(\"avg_products_per_session\"),\n",
    "        avg(\"unique_categories_viewed\").alias(\"avg_categories_per_session\"),\n",
    "        sum(\"converted\") / count(\"*\") * 100 .alias(\"session_conversion_rate\"),\n",
    "        avg(\"engagement_score\").alias(\"avg_engagement_score\")\n",
    "    )\n",
    "    \n",
    "    # Distribution des durées de session\n",
    "    session_durations = session_stats.select(\n",
    "        when(col(\"session_duration_minutes\") < 1, \"< 1 min\")\n",
    "        .when(col(\"session_duration_minutes\") < 5, \"1-5 mins\")\n",
    "        .when(col(\"session_duration_minutes\") < 15, \"5-15 mins\")\n",
    "        .when(col(\"session_duration_minutes\") < 30, \"15-30 mins\")\n",
    "        .when(col(\"session_duration_minutes\") < 60, \"30-60 mins\")\n",
    "        .otherwise(\"> 60 mins\").alias(\"duration_bucket\")\n",
    "    ).groupBy(\"duration_bucket\").count().orderBy(\"duration_bucket\")\n",
    "    \n",
    "    logger.info(\"Patterns de session analysés avec succès\")\n",
    "    logger.info(\"Métriques des sessions:\")\n",
    "    session_metrics.show()\n",
    "    \n",
    "    logger.info(\"Distribution des durées de session:\")\n",
    "    session_durations.show()\n",
    "    \n",
    "    return session_metrics, session_durations, session_stats\n",
    "\n",
    "def visualize_funnel_data(funnel_metrics, funnel_by_category, vis_dir):\n",
    "    \"\"\"\n",
    "    Création de visualisations pour le funnel de conversion\n",
    "    \n",
    "    Args:\n",
    "        funnel_metrics: DataFrame des métriques du funnel\n",
    "        funnel_by_category: DataFrame du funnel par catégorie\n",
    "        vis_dir: Répertoire de sortie pour les visualisations\n",
    "    \"\"\"\n",
    "    # Graphique du funnel de conversion\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    funnel_data = funnel_metrics.collect()[0]\n",
    "    \n",
    "    funnel_stages = ['Users with Views', 'Users with Cart', 'Users with Purchase']\n",
    "    funnel_values = [funnel_data['users_with_views'], funnel_data['users_with_cart'], funnel_data['users_with_purchase']]\n",
    "    \n",
    "    colors = [\"#2ca02c\", \"#ff7f0e\", \"#1f77b4\"]\n",
    "    \n",
    "    # Créer le graphique en entonnoir\n",
    "    plt.bar(funnel_stages, funnel_values, width=0.5, color=colors, edgecolor='black')\n",
    "    \n",
    "    # Ajouter les valeurs et taux de conversion sur les barres\n",
    "    for i, (stage, value) in enumerate(zip(funnel_stages, funnel_values)):\n",
    "        plt.text(i, value + (max(funnel_values) * 0.01), f\"{int(value):,}\", \n",
    "                 ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        if i > 0:\n",
    "            conversion_rate = funnel_values[i] / funnel_values[i-1] * 100\n",
    "            plt.text(i - 0.5, (funnel_values[i-1] + funnel_values[i]) / 2, \n",
    "                     f\"{conversion_rate:.1f}%\", ha='center', va='center',\n",
    "                     fontsize=12, fontweight='bold', bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    plt.title('Funnel de Conversion', fontsize=16)\n",
    "    plt.ylabel('Nombre d\\'utilisateurs', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, \"conversion_funnel.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    logger.info(\"Graphique du funnel de conversion sauvegardé\")\n",
    "    \n",
    "    # Top catégories par taux de conversion\n",
    "    top_categories = funnel_by_category.filter(col(\"views\") > 100) \\\n",
    "        .select(\"category_code\", \"view_to_purchase_rate\") \\\n",
    "        .orderBy(desc(\"view_to_purchase_rate\")) \\\n",
    "        .limit(10).collect()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    category_names = [row['category_code'] if len(str(row['category_code'])) < 20 \n",
    "                      else str(row['category_code'])[:17]+'...' for row in top_categories]\n",
    "    conversion_rates = [row['view_to_purchase_rate'] for row in top_categories]\n",
    "    \n",
    "    ax = sns.barplot(x=conversion_rates, y=category_names, palette=\"viridis\")\n",
    "    plt.title('Top 10 des Catégories par Taux de Conversion (Vue → Achat)', fontsize=15)\n",
    "    plt.xlabel('Taux de Conversion (%)', fontsize=12)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{p.get_width():.2f}%', \n",
    "                   (p.get_width(), p.get_y() + p.get_height() / 2.),\n",
    "                   ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, \"top_categories_conversion.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    logger.info(\"Graphique des taux de conversion par catégorie sauvegardé\")\n",
    "\n",
    "def visualize_session_data(session_metrics, session_durations, session_stats, vis_dir):\n",
    "    \"\"\"\n",
    "    Création de visualisations pour les patterns de session\n",
    "    \n",
    "    Args:\n",
    "        session_metrics: DataFrame des métriques de session agrégées\n",
    "        session_durations: DataFrame de distribution des durées de session\n",
    "        session_stats: DataFrame des statistiques détaillées par session\n",
    "        vis_dir: Répertoire de sortie pour les visualisations\n",
    "    \"\"\"\n",
    "    # Distribution des durées de session\n",
    "    durations_data = session_durations.orderBy(\"duration_bucket\").collect()\n",
    "    \n",
    "    # Définir l'ordre correct des buckets de durée\n",
    "    duration_order = [\"< 1 min\", \"1-5 mins\", \"5-15 mins\", \"15-30 mins\", \"30-60 mins\", \"> 60 mins\"]\n",
    "    \n",
    "    # Préparer les données pour le graphique\n",
    "    durations_dict = {row['duration_bucket']: row['count'] for row in durations_data}\n",
    "    ordered_durations = [durations_dict.get(bucket, 0) for bucket in duration_order]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=duration_order, y=ordered_durations, palette=\"viridis\")\n",
    "    plt.title('Distribution des Durées de Session', fontsize=15)\n",
    "    plt.xlabel('Durée de Session', fontsize=12)\n",
    "    plt.ylabel('Nombre de Sessions', fontsize=12)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for i, p in enumerate(ax.patches):\n",
    "        ax.annotate(f'{int(p.get_height()):,}', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, \"session_duration_distribution.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    logger.info(\"Graphique de distribution des durées de session sauvegardé\")\n",
    "    \n",
    "    # Relation entre engagement et conversion\n",
    "    engagement_data = session_stats.select(\n",
    "        \"engagement_score\", \"converted\", \"session_duration_minutes\"\n",
    "    ).filter(col(\"session_duration_minutes\") < 60).collect()  # Filtrer les sessions extrêmement longues\n",
    "    \n",
    "    engagement_scores = [row['engagement_score'] for row in engagement_data]\n",
    "    conversion_status = [row['converted'] for row in engagement_data]\n",
    "    durations = [row['session_duration_minutes'] for row in engagement_data]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(engagement_scores, durations, c=conversion_status, cmap='viridis', \n",
    "                alpha=0.5, edgecolors='none')\n",
    "    \n",
    "    plt.colorbar(label='Conversion (1=Oui, 0=Non)')\n",
    "    plt.title('Relation entre Engagement, Durée de Session et Conversion', fontsize=15)\n",
    "    plt.xlabel('Score d\\'Engagement', fontsize=12)\n",
    "    plt.ylabel('Durée de Session (minutes)', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, \"engagement_vs_conversion.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    logger.info(\"Graphique de relation engagement-conversion sauvegardé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af40d9",
   "metadata": {},
   "source": [
    "## Sauvegarde des données traitées avec Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d24b0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(cleaned_df, user_behavior_df=None, recommendation_df=None, \n",
    "                        product_df=None, time_series_df=None, output_dir=output_directory):\n",
    "    \"\"\"\n",
    "    Sauvegarde des données prétraitées dans différents formats\n",
    "    \n",
    "    Args:\n",
    "        cleaned_df: DataFrame Spark principal prétraité\n",
    "        user_behavior_df: DataFrame des comportements utilisateur (optionnel)\n",
    "        recommendation_df: DataFrame pour recommandations (optionnel)\n",
    "        product_df: DataFrame produits (optionnel)\n",
    "        time_series_df: DataFrame séries temporelles (optionnel)\n",
    "        output_dir: Répertoire de sortie pour les données traitées\n",
    "    \n",
    "    Returns:\n",
    "        Dictionnaire des chemins de fichiers sauvegardés\n",
    "    \"\"\"\n",
    "    if cleaned_df is None:\n",
    "        logger.error(\"Les données n'ont pas été prétraitées\")\n",
    "        return None\n",
    "    \n",
    "    # Convertir en chemins absolus avec slashes normaux\n",
    "    output_dir = os.path.abspath(output_dir).replace(\"\\\\\", \"/\")\n",
    "    \n",
    "    # Création des sous-répertoires\n",
    "    parquet_dir = os.path.join(output_dir, \"parquet\").replace(\"\\\\\", \"/\")\n",
    "    os.makedirs(parquet_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Chemins des fichiers avec slashes normaux\n",
    "    output_paths = {\n",
    "        \"cleaned_data\": f\"{parquet_dir}/cleaned_data_{timestamp_str}.parquet\",\n",
    "        \"user_behavior\": f\"{parquet_dir}/user_behavior_{timestamp_str}.parquet\" if user_behavior_df is not None else None,\n",
    "        \"recommendation_data\": f\"{parquet_dir}/recommendation_data_{timestamp_str}.parquet\" if recommendation_df is not None else None,\n",
    "        \"product_data\": f\"{parquet_dir}/product_data_{timestamp_str}.parquet\" if product_df is not None else None,\n",
    "        \"time_series_data\": f\"{parquet_dir}/time_series_data_{timestamp_str}.parquet\" if time_series_df is not None else None\n",
    "    }\n",
    "\n",
    "    # Sauvegarde des données nettoyées principales\n",
    "    logger.info(f\"Sauvegarde des données nettoyées: {output_paths['cleaned_data']}\")\n",
    "    try:\n",
    "        cleaned_df.write.mode(\"overwrite\").format(\"parquet\").save(output_paths[\"cleaned_data\"])\n",
    "        logger.info(\"Données nettoyées sauvegardées\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la sauvegarde des données nettoyées: {str(e)}\")\n",
    "    \n",
    "    # Sauvegarde des comportements utilisateur\n",
    "    if user_behavior_df is not None:\n",
    "        logger.info(f\"Sauvegarde des comportements utilisateur: {output_paths['user_behavior']}\")\n",
    "        try:\n",
    "            user_behavior_df.write.mode(\"overwrite\").format(\"parquet\").save(output_paths[\"user_behavior\"])\n",
    "            logger.info(\"Comportements utilisateur sauvegardés\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la sauvegarde des comportements utilisateur: {str(e)}\")\n",
    "    \n",
    "    # Sauvegarde des données de recommandation\n",
    "    if recommendation_df is not None:\n",
    "        logger.info(f\"Sauvegarde des données de recommandation: {output_paths['recommendation_data']}\")\n",
    "        try:\n",
    "            recommendation_df.write.mode(\"overwrite\").format(\"parquet\").save(output_paths[\"recommendation_data\"])\n",
    "            logger.info(\"Données de recommandation sauvegardées\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la sauvegarde des données de recommandation: {str(e)}\")\n",
    "    \n",
    "    # Sauvegarde des données produits\n",
    "    if product_df is not None:\n",
    "        logger.info(f\"Sauvegarde des données produits: {output_paths['product_data']}\")\n",
    "        try:\n",
    "            product_df.write.mode(\"overwrite\").format(\"parquet\").save(output_paths[\"product_data\"])\n",
    "            logger.info(\"Données produits sauvegardées\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la sauvegarde des données produits: {str(e)}\")\n",
    "    \n",
    "    # Sauvegarde des données temporelles\n",
    "    if time_series_df is not None:\n",
    "        logger.info(f\"Sauvegarde des données temporelles: {output_paths['time_series_data']}\")\n",
    "        try:\n",
    "            time_series_df.write.mode(\"overwrite\").format(\"parquet\").save(output_paths[\"time_series_data\"])\n",
    "            logger.info(\"Données temporelles sauvegardées\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de la sauvegarde des données temporelles: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"Toutes les données ont été sauvegardées dans {output_dir}\")\n",
    "    return output_paths\n",
    "\n",
    "# Exécuter la sauvegarde des données\n",
    "# save_processed_data(cleaned_df, user_behavior_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ad9dc",
   "metadata": {},
   "source": [
    "## pipeline d'execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b4c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:57:24,468 - INFO - Chargement des données depuis ../data/2019-Oct_reduit.csv\n",
      "2025-05-18 15:57:24,843 - INFO - Données chargées : 1000000 lignes\n",
      "2025-05-18 15:57:24,844 - INFO - Aperçu des données:\n",
      "2025-05-18 15:57:24,931 - INFO - Exploration initiale des données\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|2019-10-01 00:00:00|      view|  44600062|2103807459595387724|                NULL|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 00:00:00|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|\n",
      "|2019-10-01 00:00:01|      view|  17200506|2053013559792632471|furniture.living_...|    NULL|  543.1|519107250|566511c2-e2e3-422...|\n",
      "|2019-10-01 00:00:01|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n",
      "|2019-10-01 00:00:04|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:57:25,250 - INFO - Nombre de lignes: 1000000\n",
      "2025-05-18 15:57:25,254 - INFO - Nombre de colonnes: 9\n",
      "2025-05-18 15:57:25,255 - INFO - Schéma du DataFrame:\n",
      "2025-05-18 15:57:25,256 - INFO - Statistiques descriptives:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:57:30,136 - INFO - Valeurs manquantes par colonne:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+--------------------+-------------------+--------+------------------+--------------------+--------------------+\n",
      "|summary|event_type|          product_id|         category_id|      category_code|   brand|             price|             user_id|        user_session|\n",
      "+-------+----------+--------------------+--------------------+-------------------+--------+------------------+--------------------+--------------------+\n",
      "|  count|   1000000|             1000000|             1000000|             681869|  852440|           1000000|             1000000|             1000000|\n",
      "|   mean|      NULL|   1.0347992085779E7|2.056347726779049...|               NULL|Infinity|295.98247050999464|   5.3127631576035E8|                NULL|\n",
      "| stddev|      NULL|1.1238269023409404E7|1.579787912319246...|               NULL|     NaN| 368.2165155182077|1.6673329959351553E7|                NULL|\n",
      "|    min|      cart|             1001588| 2053013552226107603|    accessories.bag|  a-case|               0.0|           244951053|00012d23-c857-40a...|\n",
      "|    max|      view|             9900454| 2175419595093967522|stationery.cartrige|   zyxel|           2574.07|           555717509|fffe439b-c02e-4eb...|\n",
      "+-------+----------+--------------------+--------------------+-------------------+--------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:57:31,699 - INFO - event_time: 0 (0.00%)\n",
      "2025-05-18 15:57:32,523 - INFO - event_type: 0 (0.00%)\n",
      "2025-05-18 15:57:33,271 - INFO - product_id: 0 (0.00%)\n",
      "2025-05-18 15:57:33,913 - INFO - category_id: 0 (0.00%)\n",
      "2025-05-18 15:57:34,541 - INFO - category_code: 318131 (31.81%)\n",
      "2025-05-18 15:57:35,108 - INFO - brand: 147560 (14.76%)\n",
      "2025-05-18 15:57:35,869 - INFO - price: 0 (0.00%)\n",
      "2025-05-18 15:57:36,422 - INFO - user_id: 0 (0.00%)\n",
      "2025-05-18 15:57:37,079 - INFO - user_session: 0 (0.00%)\n",
      "2025-05-18 15:57:37,080 - INFO - Distribution des types d'événements:\n",
      "2025-05-18 15:57:37,999 - INFO - Top 10 des catégories:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|event_type| count|\n",
      "+----------+------+\n",
      "|      view|968513|\n",
      "|  purchase| 16848|\n",
      "|      cart| 14639|\n",
      "+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:57:39,220 - INFO - Top 10 des marques:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|       category_code| count|\n",
      "+--------------------+------+\n",
      "|                NULL|318131|\n",
      "|electronics.smart...|274622|\n",
      "|  electronics.clocks| 35394|\n",
      "|  computers.notebook| 28913|\n",
      "|electronics.audio...| 26397|\n",
      "|electronics.video.tv| 22019|\n",
      "|appliances.kitche...| 17964|\n",
      "|appliances.enviro...| 17285|\n",
      "|       apparel.shoes| 16116|\n",
      "|appliances.kitche...| 16098|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:57:40,998 - INFO - Prétraitement des données\n",
      "2025-05-18 15:57:40,999 - INFO - Extraction des caractéristiques temporelles\n",
      "2025-05-18 15:57:41,074 - INFO - Traitement des valeurs manquantes\n",
      "2025-05-18 15:57:41,095 - INFO - Nettoyage des prix\n",
      "2025-05-18 15:57:41,107 - INFO - Aperçu des données prétraitées:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  brand| count|\n",
      "+-------+------+\n",
      "|   NULL|147560|\n",
      "|samsung|120715|\n",
      "|  apple|103067|\n",
      "| xiaomi| 69246|\n",
      "| huawei| 28013|\n",
      "|lucente| 16948|\n",
      "|  bosch| 12402|\n",
      "|     lg| 10921|\n",
      "|   acer|  9802|\n",
      "|   sony|  9561|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:57:41,266 - INFO - Statistiques après prétraitement:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+----+------+------+---+-----+---------+----------+-------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|hour|minute|second|day|month|dayofweek|      date|        hour_bucket|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+----+------+------+---+-----+---------+----------+-------------------+\n",
      "|2019-10-01 00:00:00|      view|  44600062|2103807459595387724|             unknown|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|   0|     0|     0|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "|2019-10-01 00:00:00|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|   0|     0|     0|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "|2019-10-01 00:00:01|      view|  17200506|2053013559792632471|furniture.living_...| unknown|  543.1|519107250|566511c2-e2e3-422...|   0|     0|     1|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "|2019-10-01 00:00:01|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|   0|     0|     1|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "|2019-10-01 00:00:04|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|   0|     0|     4|  1|   10|        3|2019-10-01|2019-10-01 00:00:00|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+----+------+------+---+-----+---------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:57:51,966 - INFO - Calcul des métriques de comportement utilisateur\n",
      "2025-05-18 15:57:52,098 - INFO - Métriques de comportement utilisateur calculées\n",
      "2025-05-18 15:57:52,099 - INFO - Aperçu des comportements utilisateur:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+--------------------+---------------+--------+-----------------+--------------------+--------------------+-----------------+------------------+------------------+-------+-------+--------------------+----------+-------------------+\n",
      "|summary|event_type|          product_id|         category_id|  category_code|   brand|            price|             user_id|        user_session|             hour|            minute|            second|    day|  month|           dayofweek|      date|        hour_bucket|\n",
      "+-------+----------+--------------------+--------------------+---------------+--------+-----------------+--------------------+--------------------+-----------------+------------------+------------------+-------+-------+--------------------+----------+-------------------+\n",
      "|  count|   1000000|             1000000|             1000000|        1000000| 1000000|           998393|             1000000|             1000000|          1000000|           1000000|           1000000|1000000|1000000|             1000000|   1000000|            1000000|\n",
      "|   mean|      NULL|   1.0347992085779E7|2.056347726779049...|           NULL|Infinity|296.4588799300422|   5.3127631576035E8|                NULL|          9.84265|         29.879386|          29.50199|    1.0|   10.0|                 3.0|      NULL|               NULL|\n",
      "| stddev|      NULL|1.1238269023409404E7|1.579787912319246...|           NULL|     NaN|368.3210553345152|1.6673329959351553E7|                NULL|4.181281676784811|17.152649780031048|17.324520489148018|    0.0|    0.0|9.203666424105919...|      NULL|               NULL|\n",
      "|    min|      cart|             1001588| 2053013552226107603|accessories.bag|  a-case|             0.79|           244951053|00012d23-c857-40a...|                0|                 0|                 0|      1|     10|                   3|2019-10-01|2019-10-01 00:00:00|\n",
      "|    max|      view|             9900454| 2175419595093967522|        unknown|   zyxel|          2574.07|           555717509|fffe439b-c02e-4eb...|               16|                59|                59|      1|     10|                   3|2019-10-01|2019-10-01 16:00:00|\n",
      "+-------+----------+--------------------+--------------------+---------------+--------+-----------------+--------------------+--------------------+-----------------+------------------+------------------+-------+-------+--------------------+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:58:01,572 - INFO - Préparation des données pour le système de recommandation\n",
      "2025-05-18 15:58:01,618 - INFO - Aperçu des données de recommandation:\n",
      "2025-05-18 15:58:01,715 - INFO - Aperçu des données produits:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+--------+------------+----------+----------------+-------------------+-----------+-------------------+-------------------+-------+---------+--------+--------------------+--------------------+---------------+----------------+---------------+\n",
      "|  user_id|nb_events|nb_views|nb_carts|nb_purchases|nb_removes|avg_price_viewed|avg_price_purchased|nb_sessions|         first_seen|          last_seen|recency|frequency|monetary|   viewed_categories|       viewed_brands|conversion_rate|cart_abandonment|engagement_days|\n",
      "+---------+---------+--------+--------+------------+----------+----------------+-------------------+-----------+-------------------+-------------------+-------+---------+--------+--------------------+--------------------+---------------+----------------+---------------+\n",
      "|318145786|        2|       2|       0|           0|         0|          35.075|               NULL|          1|2019-10-01 10:32:50|2019-10-01 10:33:18|     30|        0|     0.0|  [unknown, unknown]|     [arua, unknown]|            0.0|             0.0|              1|\n",
      "|379865057|        4|       4|       0|           0|         0|           391.0|               NULL|          1|2019-10-01 15:21:50|2019-10-01 15:25:57|     30|        0|     0.0|[electronics.smar...|[samsung, samsung...|            0.0|             0.0|              1|\n",
      "|403442482|        3|       3|       0|           0|         0|          136.34|               NULL|          1|2019-10-01 05:26:50|2019-10-01 05:27:24|     30|        0|     0.0|[unknown, unknown...|[unknown, unknown...|            0.0|             0.0|              1|\n",
      "|414289411|        2|       2|       0|           0|         0|         1413.16|               NULL|          1|2019-10-01 14:20:36|2019-10-01 14:22:14|     30|        0|     0.0|[electronics.smar...|      [apple, apple]|            0.0|             0.0|              1|\n",
      "|417354230|        1|       1|       0|           0|         0|          218.77|               NULL|          1|2019-10-01 12:25:31|2019-10-01 12:25:31|     30|        0|     0.0|[electronics.smar...|           [samsung]|            0.0|             0.0|              1|\n",
      "+---------+---------+--------+--------+------------+----------+----------------+-------------------+-----------+-------------------+-------------------+-------+---------+--------+--------------------+--------------------+---------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+----------+----------+-------+-------------------+-----------------+\n",
      "|  user_id|product_id|event_type|  price|         event_time|interaction_score|\n",
      "+---------+----------+----------+-------+-------------------+-----------------+\n",
      "|541312140|  44600062|      view|  35.79|2019-10-01 00:00:00|                1|\n",
      "|554748717|   3900821|      view|   33.2|2019-10-01 00:00:00|                1|\n",
      "|519107250|  17200506|      view|  543.1|2019-10-01 00:00:01|                1|\n",
      "|550050854|   1307067|      view| 251.74|2019-10-01 00:00:01|                1|\n",
      "|535871217|   1004237|      view|1081.98|2019-10-01 00:00:04|                1|\n",
      "+---------+----------+----------+-------+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:58:03,959 - INFO - Préparation des données pour analyses temporelles\n",
      "2025-05-18 15:58:04,035 - INFO - Aperçu des données temporelles:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+-------+------+\n",
      "|product_id|        category_id|       category_code|  brand| price|\n",
      "+----------+-------------------+--------------------+-------+------+\n",
      "|  26200210|2053013563693335403|             unknown|unknown| 98.84|\n",
      "|  21402696|2053013561579406073|  electronics.clocks|unknown| 24.97|\n",
      "|   3100871|2053013555262783879|appliances.kitche...|  vitek| 23.14|\n",
      "|   1480608|2053013561092866779|   computers.desktop| pulser|380.94|\n",
      "|  37800028|2078957461921858354|             unknown|carters| 31.02|\n",
      "+----------+-------------------+--------------------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:58:07,050 - INFO - Sauvegarde des données nettoyées: c:/Users/Acer_M/Documents/DIC2/bigdata/projet/customer_recommendation_segmentation/data/processed/parquet/cleaned_data_20250518_155807.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+------------+-----+-----+---------+-------+------------------+\n",
      "|        hour_bucket|total_events|unique_users|views|carts|purchases|removes|         avg_price|\n",
      "+-------------------+------------+------------+-----+-----+---------+-------+------------------+\n",
      "|2019-10-01 00:00:00|        1083|         383| 1070|    3|       10|      0| 303.2282086795937|\n",
      "|2019-10-01 01:00:00|         121|         102|  121|    0|        0|      0|327.07438016528926|\n",
      "|2019-10-01 02:00:00|       22886|        5378|22326|  244|      316|      0| 289.5163129346923|\n",
      "|2019-10-01 03:00:00|       49409|       10514|47951|  613|      845|      0|283.44011712259385|\n",
      "|2019-10-01 04:00:00|       55290|       11933|53390|  879|     1021|      0| 288.2792449345431|\n",
      "+-------------------+------------+------------+-----+-----+---------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:58:10,919 - INFO - Données nettoyées sauvegardées\n",
      "2025-05-18 15:58:10,920 - INFO - Sauvegarde des comportements utilisateur: c:/Users/Acer_M/Documents/DIC2/bigdata/projet/customer_recommendation_segmentation/data/processed/parquet/user_behavior_20250518_155807.parquet\n",
      "2025-05-18 15:58:19,563 - INFO - Comportements utilisateur sauvegardés\n",
      "2025-05-18 15:58:19,565 - INFO - Sauvegarde des données de recommandation: c:/Users/Acer_M/Documents/DIC2/bigdata/projet/customer_recommendation_segmentation/data/processed/parquet/recommendation_data_20250518_155807.parquet\n",
      "2025-05-18 15:58:21,273 - INFO - Données de recommandation sauvegardées\n",
      "2025-05-18 15:58:21,275 - INFO - Sauvegarde des données produits: c:/Users/Acer_M/Documents/DIC2/bigdata/projet/customer_recommendation_segmentation/data/processed/parquet/product_data_20250518_155807.parquet\n",
      "2025-05-18 15:58:23,077 - INFO - Données produits sauvegardées\n",
      "2025-05-18 15:58:23,078 - INFO - Sauvegarde des données temporelles: c:/Users/Acer_M/Documents/DIC2/bigdata/projet/customer_recommendation_segmentation/data/processed/parquet/time_series_data_20250518_155807.parquet\n",
      "2025-05-18 15:58:25,417 - INFO - Données temporelles sauvegardées\n",
      "2025-05-18 15:58:25,417 - INFO - Toutes les données ont été sauvegardées dans c:/Users/Acer_M/Documents/DIC2/bigdata/projet/customer_recommendation_segmentation/data/processed\n",
      "2025-05-18 15:58:25,418 - INFO - Pipeline de prétraitement terminé avec succès\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale exécutant tout le pipeline de prétraitement\n",
    "    \"\"\"\n",
    "    # Création de la session Spark\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    # Chargement des données\n",
    "    raw_df = load_data(spark)\n",
    "    \n",
    "    #visualisation des données\n",
    "    explore_data(raw_df)\n",
    "\n",
    "    # Nettoyage et prétraitement\n",
    "    cleaned_df = preprocess_data(raw_df)\n",
    "    \n",
    "    # Calcul des métriques utilisateur pour segmentation\n",
    "    user_behavior_df = compute_user_behavior(cleaned_df)\n",
    "    \n",
    "    # Préparation des données pour recommandation\n",
    "    recommendation_df, product_df = prepare_recommendation_data(cleaned_df)\n",
    "    \n",
    "    # Préparation des données temporelles pour simulation\n",
    "    time_series_df = prepare_time_series_data(cleaned_df)\n",
    "    \n",
    "    # Analyse du funnel de conversion\n",
    "    # funnel_metrics, funnel_by_category = analyze_purchase_funnel(cleaned_df)\n",
    "\n",
    "    # Analyse des patterns de session\n",
    "    # session_metrics, session_durations, session_stats = analyze_session_patterns(cleaned_df)\n",
    "\n",
    "    # # Modification de la partie visualize_data\n",
    "    # analytics_data = generate_analytics(cleaned_df)\n",
    "    # if analytics_data:\n",
    "    #     vis_path = visualize_data(analytics_data, show_plots=True)\n",
    "    #     # Ajout des nouvelles visualisations\n",
    "    #     visualize_funnel_data(funnel_metrics, funnel_by_category, vis_path)\n",
    "    #     visualize_session_data(session_metrics, session_durations, session_stats, vis_path)\n",
    "    #     print(f\"Visualisations générées dans : {vis_path}\")\n",
    "    # Sauvegarde de tous les DataFrames générés\n",
    "    save_processed_data(\n",
    "        cleaned_df=cleaned_df,\n",
    "        user_behavior_df=user_behavior_df,\n",
    "        recommendation_df=recommendation_df,\n",
    "        product_df=product_df,\n",
    "        time_series_df=time_series_df\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Pipeline de prétraitement terminé avec succès\")\n",
    "    \n",
    "    # Arrêt de la session Spark\n",
    "    spark.stop()\n",
    "\n",
    "# Si exécuté comme script principal\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
