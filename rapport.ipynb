{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Rapport projet : Analyse Comportementale et Recommandation Produit sur un Site E-commerce avec Spark \n",
    "\n",
    "## Pr√©sent√© par :\n",
    "- mouhamadou diouf ciss√©\n",
    "- ndeye fatou niassy\n",
    "\n",
    "## plan du projet\n",
    "\n",
    "1. Introduction\n",
    "2. Contexte du projet\n",
    "3. Objectifs du projet\n",
    "4. Analyse exploratoire des donn√©es\n",
    "5. Pr√©traitement des donn√©es\n",
    "\n",
    "\n",
    "# 1. Pr√©sentation du Projet\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Le pr√©sent projet vise √† concevoir une solution d‚Äôanalyse comportementale des utilisateurs d‚Äôune plateforme e-commerce, avec pour finalit√© l‚Äô√©laboration d‚Äôun syst√®me de recommandation intelligent. Ce projet, men√© dans un cadre acad√©mique, s‚Äôappuie sur des technologies Big Data et des approches d‚Äôintelligence artificielle afin de traiter efficacement des volumes importants de donn√©es g√©n√©r√©s par les utilisateurs.\n",
    "\n",
    "## Objectifs du Projet\n",
    "\n",
    "Le projet vise √† concevoir une solution compl√®te d‚Äôanalyse comportementale et de recommandation intelligente dans un contexte e-commerce. Pour cela, plusieurs objectifs principaux ont √©t√© d√©finis :\n",
    "\n",
    "### ‚Ä¢ Compr√©hension et mod√©lisation des comportements utilisateurs\n",
    "\n",
    "L‚Äôobjectif initial est d‚Äôanalyser en profondeur les comportements de navigation et d‚Äôachat des utilisateurs √† partir des donn√©es de logs collect√©es sur une plateforme e-commerce. Il s‚Äôagit notamment d‚Äôidentifier les actions r√©alis√©es par les utilisateurs (consultations, ajouts au panier, achats), d‚Äô√©tudier la chronologie et la fr√©quence de ces actions, et d‚Äôen extraire des tendances comportementales significatives. Ces analyses doivent permettre d‚Äô√©tablir un mod√®le repr√©sentatif des interactions typiques entre un utilisateur et la plateforme.\n",
    "\n",
    "### ‚Ä¢ Identification des patterns d‚Äôinteractions et des profils types\n",
    "\n",
    "√Ä partir des donn√©es collect√©es, il est essentiel d‚Äôidentifier des sch√©mas d‚Äôinteractions r√©currents, appel√©s patterns, permettant de regrouper les utilisateurs selon des comportements similaires. Cette √©tape vise √† faire √©merger des profils types de clients (par exemple : acheteurs impulsifs, visiteurs r√©guliers non acheteurs, utilisateurs sensibles au prix, etc.), facilitant ainsi une segmentation fine de la client√®le et une personnalisation plus pertinente des actions marketing ou commerciales.\n",
    "\n",
    "### ‚Ä¢ Mise en place d‚Äôun syst√®me de recommandation dynamique et personnalis√©\n",
    "\n",
    "Sur la base des pr√©f√©rences observ√©es (historique de navigation, fr√©quence d‚Äôachat, cat√©gories pr√©f√©r√©es, marques consult√©es), un syst√®me de recommandation doit √™tre d√©velopp√©. Ce syst√®me devra √™tre capable de proposer de mani√®re dynamique des produits pertinents √† chaque utilisateur, en tenant compte √† la fois de ses habitudes pass√©es et de son profil comportemental. Les recommandations devront s‚Äôappuyer sur des techniques hybrides combinant filtrage collaboratif, analyse de contenu, et segmentation client.\n",
    "\n",
    "### ‚Ä¢ Simulation d‚Äôun environnement temps r√©el pour √©valuation des performances\n",
    "\n",
    "Enfin, une infrastructure de simulation en temps r√©el sera mise en place afin de tester la robustesse et la r√©activit√© de l‚Äôensemble du syst√®me. Cette simulation permettra de reproduire l‚Äôarriv√©e continue des donn√©es, de d√©clencher le traitement de ces flux par les modules analytiques et de mesurer les performances en conditions quasi-r√©elles.\n",
    "\n",
    "## Donn√©es\n",
    "Les jeux de donn√©es fournis sont au format CSV et couvrent les √©v√©nements de navigation et d‚Äôachat sur les mois d‚Äôoctobre et novembre. Chaque enregistrement contient des informations sur le type d‚Äô√©v√©nement, les produits concern√©s, les cat√©gories, la marque, le prix, l‚Äôutilisateur et sa session. Le volume de donn√©es est estim√© √† plusieurs dizaines de millions de lignes, ce qui justifie l‚Äôadoption d‚Äôune architecture Big Data.\n",
    "\n",
    "# 2. Analyse exploratoire des donn√©es\n",
    "## üîç Aper√ßu des donn√©es\n",
    "\n",
    "### üìä √âchantillon des premi√®res lignes du jeu de donn√©es :\n",
    "\n",
    "| event_time         | event_type | product_id | category_id         | category_code             | brand    | price  | user_id  | user_session                      |\n",
    "|--------------------|------------|------------|----------------------|----------------------------|----------|--------|----------|-----------------------------------|\n",
    "| 2019-10-01 00:00:00| view       | 44600062   | 2103807459595387724 | NULL                       | shiseido | 35.79  | 541312140| 72d76fde-8bb3-4e0...              |\n",
    "| 2019-10-01 00:00:00| view       | 3900821    | 2053013552326770905 | appliances.environment    | aqua     | 33.20  | 554748717| 9333dfbd-b87a-470...              |\n",
    "| 2019-10-01 00:00:01| view       | 17200506   | 2053013559792632471 | furniture.living_room     | NULL     | 543.10 | 519107250| 566511c2-e2e3-422...              |\n",
    "| 2019-10-01 00:00:01| view       | 1307067    | 2053013558920217191 | computers.notebook        | lenovo   | 251.74 | 550050854| 7c90fc70-0e80-459...              |\n",
    "| 2019-10-01 00:00:04| view       | 1004237    | 2053013555631882655 | electronics.smartphone    | apple    |1081.98 | 535871217| c6bd7419-2748-4c5...              |\n",
    "\n",
    "> *Remarque* : seules les 5 premi√®res lignes sont affich√©es.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà R√©partition des types d'√©v√©nements :\n",
    "\n",
    "| event_type | count     |\n",
    "|------------|-----------|\n",
    "| view       | 40,779,399|\n",
    "| cart       |   926,516 |\n",
    "| purchase   |   742,849 |\n",
    "\n",
    "Le jeu de donn√©es contient plusieurs colonnes cl√©s relatives au comportement des utilisateurs sur une plateforme e-commerce :\n",
    "\n",
    "- *event_time* : Date et heure pr√©cises de l‚Äô√©v√©nement.\n",
    "- *event_type* : Type d‚Äôaction effectu√©e par l‚Äôutilisateur (view, cart, purchase).\n",
    "- *product_id* : Identifiant unique du produit concern√©.\n",
    "- *category_id* et *category_code* : Informations sur la cat√©gorie du produit.\n",
    "- *brand* : Marque du produit (des valeurs peuvent √™tre manquantes).\n",
    "- *price* : Prix du produit.\n",
    "- *user_id* : Identifiant de l‚Äôutilisateur.\n",
    "- *user_session* : Identifiant unique de la session utilisateur (utile pour reconstituer les parcours utilisateurs).\n",
    "\n",
    " üìä Analyse Exploratoire des Donn√©es d'√âv√©nements eCommerce\n",
    "\n",
    "## üîç Aper√ßu des Donn√©es\n",
    "\n",
    "| event_time         | event_type | product_id | category_id        | category_code              | brand    | price   | user_id   | user_session                   |\n",
    "|--------------------|------------|------------|---------------------|-----------------------------|----------|---------|-----------|--------------------------------|\n",
    "| 2019-10-01 00:00:00| view       | 44600062   | 2103807459595387724| NULL                        | shiseido | 35.79   | 541312140 | 72d76fde-8bb3-4e0...           |\n",
    "| 2019-10-01 00:00:00| view       | 3900821    | 2053013552326770905| appliances.environment      | aqua     | 33.2    | 554748717 | 9333dfbd-b87a-470...           |\n",
    "| 2019-10-01 00:00:01| view       | 17200506   | 2053013559792632471| furniture.living_room       | NULL     | 543.1   | 519107250 | 566511c2-e2e3-422...           |\n",
    "| 2019-10-01 00:00:01| view       | 1307067    | 2053013558920217191| computers.notebook          | lenovo   | 251.74  | 550050854 | 7c90fc70-0e80-459...           |\n",
    "| 2019-10-01 00:00:04| view       | 1004237    | 2053013555631882655| electronics.smartphone      | apple    | 1081.98 | 535871217 | c6bd7419-2748-4c5...           |\n",
    "\n",
    "> Affichage des 5 premi√®res lignes du dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Analyse\n",
    "\n",
    "- *View* est de loin l‚Äô√©v√©nement le plus fr√©quent, repr√©sentant environ *96.1%* de toutes les interactions.  \n",
    "- Les √©v√©nements *cart* (ajouts au panier) sont environ *2.2%, tandis que les **purchases* (achats) ne repr√©sentent que *1.7%*.  \n",
    "- Ce d√©s√©quilibre est *typique dans les donn√©es eCommerce* : la majorit√© des utilisateurs consultent les produits sans n√©cessairement acheter.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Visualisation de la Distribution des √âv√©nements\n",
    "\n",
    "### Histogramme et Camembert\n",
    "![image.jpg](images%2Fimage.jpg)\n",
    "\n",
    "*Figure : Diagramme en barres et camembert repr√©sentant la distribution et la proportion des √©v√©nements (view, cart, purchase).*\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Interpr√©tation\n",
    "\n",
    "- *Taux de conversion faible* : Seuls une faible proportion des utilisateurs passent du view au purchase.\n",
    "- *Potentiel d'optimisation* :\n",
    "  - Travailler sur la r√©tention des paniers (cart abandonment).\n",
    "  - Analyser les produits les plus vus vs. ceux les plus achet√©s.\n",
    "  - Am√©liorer le parcours utilisateur pour favoriser l‚Äôachat\n",
    "\n",
    "## üßπ Analyse des Valeurs Manquantes\n",
    "\n",
    "L'examen des valeurs manquantes permet de mieux comprendre la qualit√© et la compl√©tude des donn√©es avant tout traitement ou mod√©lisation.\n",
    "\n",
    "### üìã Tableau des Valeurs Manquantes\n",
    "\n",
    "| Colonne         | Valeurs Manquantes | Pourcentage (%) |\n",
    "|-----------------|--------------------|------------------|\n",
    "| category_code | 13,515,609         | 31.84%           |\n",
    "| brand         | 6,113,008          | 14.40%           |\n",
    "| user_session  | 2                  | 0.000005%        |\n",
    "| event_time    | 0                  | 0.00%            |\n",
    "| event_type    | 0                  | 0.00%            |\n",
    "| product_id    | 0                  | 0.00%            |\n",
    "| category_id   | 0                  | 0.00%            |\n",
    "| price         | 0                  | 0.00%            |\n",
    "| user_id       | 0                  | 0.00%            |\n",
    "\n",
    "### üîç Interpr√©tation\n",
    "\n",
    "- ‚úÖ *Colonnes sans valeurs manquantes* : event_time, event_type, product_id, category_id, price, et user_id. Ces colonnes sont enti√®rement compl√®tes.\n",
    "- ‚ö† *Colonnes avec valeurs manquantes notables* :\n",
    "  - category_code : *31.84%* de valeurs manquantes, ce qui est √©lev√©. Cela peut poser probl√®me pour la cat√©gorisation ou les analyses li√©es √† la classification de produits.\n",
    "  - brand : *14.40%* de valeurs manquantes. Cela pourrait affecter les analyses de fid√©lit√© √† la marque ou les √©tudes de pr√©f√©rences.\n",
    "- üü¢ *Colonne avec peu de valeurs manquantes* : user_session ne contient que 2 valeurs manquantes, ce qui est n√©gligeable et facilement nettoyable.\n",
    "\n",
    "## ‚è∞ Analyse Temporelle\n",
    "\n",
    "L‚Äôanalyse temporelle permet de comprendre quand les utilisateurs interagissent le plus avec la plateforme, en examinant les tendances par *jour* et par *heure*.\n",
    "\n",
    "### üìÖ Distribution des √âv√©nements par Jour du Mois (Octobre 2019)\n",
    "\n",
    "| Jour du Mois | Nombre d'√âv√©nements |\n",
    "|--------------|---------------------|\n",
    "| 1            | 1 244 245           |\n",
    "| 2            | 1 191 328           |\n",
    "| 3            | 1 127 303           |\n",
    "| 4            | 1 417 190           |\n",
    "| 5            | 1 330 339           |\n",
    "| 6            | 1 318 379           |\n",
    "| 7            | 1 200 531           |\n",
    "| 8            | 1 370 633           |\n",
    "| 9            | 1 347 543           |\n",
    "| 10           | 1 284 077           |\n",
    "\n",
    "> üîç *Observation* : Les √©v√©nements sont globalement bien r√©partis sur le mois d‚Äôoctobre, avec un pic autour du *15 au 20 octobre*. Il n‚Äôy a pas de chute brutale, ce qui sugg√®re une activit√© r√©guli√®re des utilisateurs.\n",
    "\n",
    "### üïí Distribution des √âv√©nements par Heure de la Journ√©e\n",
    "\n",
    "| Heure | Nombre d'√âv√©nements |\n",
    "|-------|---------------------|\n",
    "| 0     | 306 805             |\n",
    "| 1     | 559 027             |\n",
    "| 2     | 1 069 047           |\n",
    "| 3     | 1 550 285           |\n",
    "| 4     | 1 915 643           |\n",
    "| 5     | 2 125 633           |\n",
    "| 6     | 2 269 092           |\n",
    "| 7     | 2 335 718           |\n",
    "| 8     | 2 390 127           |\n",
    "| 9     | 2 351 683           |\n",
    "\n",
    "> üîç *Observation* :  \n",
    "- L'activit√© commence √† monter √† partir de *3h du matin, atteint son pic entre **8h et 15h*, puis redescend progressivement.\n",
    "- Le *pic horaire* se situe vers *15h*, sugg√©rant un comportement d‚Äôachat ou de navigation intensif en journ√©e.\n",
    "\n",
    "## üí∞ Analyse des Prix\n",
    "\n",
    "L‚Äôanalyse des prix permet de comparer les comportements des utilisateurs selon le type d‚Äô√©v√©nement (vue, ajout au panier, achat), en se basant sur les statistiques suivantes : prix moyen, minimum, maximum et le nombre de valeurs non nulles.\n",
    "\n",
    "### üî¢ Statistiques des Prix par Type d'√âv√©nement\n",
    "\n",
    "| Type d'√âv√©nement | Prix Moyen (en $) | Prix Min | Prix Max      | Valeurs Non Nulles |\n",
    "|------------------|-------------------|----------|---------------|---------------------|\n",
    "| Purchase         | 309.56            | 0.77     | 2 574.07      | 742 849             |\n",
    "| View             | 288.98            | 0.00     | 2 574.07      | 40 779 399          |\n",
    "| Cart             | 333.83            | 0.00     | 2 574.04      | 926 516             |\n",
    "\n",
    "### üßê Observations\n",
    "\n",
    "- üìå *Prix Moyen* :\n",
    "  - Les produits ajout√©s au panier (*cart) ont un prix moyen plus √©lev√© (333,83 $) que ceux seulement consult√©s (288,98 $) ou achet√©s (309,56 $*).\n",
    "  - Cela peut indiquer une intention d‚Äôachat pour des produits de plus grande valeur qui ne sont pas toujours concr√©tis√©e.\n",
    "\n",
    "- üìâ *Prix Min = 0.00* :\n",
    "  - Le prix minimal est nul pour les √©v√©nements view et cart, ce qui peut signaler la pr√©sence d‚Äôarticles gratuits ou une donn√©e erron√©e.\n",
    "\n",
    "- üìà *Prix Max similaire* :\n",
    "  - Le prix maximal est tr√®s proche pour tous les types d‚Äô√©v√©nements (‚âà 2 574 $), ce qui montre qu‚Äôil existe des produits haut de gamme qui suscitent √† la fois de la consultation, de l‚Äôajout au panier et des achats.\n",
    "\n",
    "- üßÆ *Volume de donn√©es* :\n",
    "  - Les √©v√©nements de type view sont les plus nombreux, ce qui est logique car les utilisateurs consultent bien plus qu‚Äôils n‚Äôach√®tent ou ajoutent au panier.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Analyse des Sessions\n",
    "\n",
    "L‚Äôanalyse des sessions permet de mieux comprendre le comportement utilisateur sur la plateforme, en mesurant le nombre d‚Äô√©v√©nements par session et la dur√©e des sessions.\n",
    "\n",
    "### üìä Distribution du Nombre d‚Äô√âv√©nements par Session\n",
    "\n",
    "| Statistique        | Valeur        |\n",
    "|--------------------|---------------|\n",
    "| Nombre total       | 9 244 422     |\n",
    "| Moyenne            | 4.59 √©v√©nements/session |\n",
    "| √âcart-type         | 6.77          |\n",
    "| Minimum            | 1 √©v√©nement   |\n",
    "| 25e percentile     | 1 √©v√©nement   |\n",
    "| M√©diane (50e)      | 2 √©v√©nements  |\n",
    "| 75e percentile     | 5 √©v√©nements  |\n",
    "| Maximum            | 1 159 √©v√©nements |\n",
    "\n",
    "### ‚è± Dur√©e des Sessions\n",
    "\n",
    "- *Dur√©e moyenne* : 17,37 minutes  \n",
    "- *Dur√©e m√©diane* : 1,05 minute\n",
    "\n",
    "### üßê Observations\n",
    "\n",
    "- üìå *Sessions courtes* :\n",
    "  - La m√©diane tr√®s basse (1,05 minute) indique que la majorit√© des sessions sont *tr√®s br√®ves*, avec peu d‚Äôinteractions.\n",
    "  - Cela sugg√®re une navigation rapide ou peu engageante pour de nombreux utilisateurs.\n",
    "\n",
    "- üîù *Sessions longues mais rares* :\n",
    "  - La valeur maximale de *1 159 √©v√©nements* montre qu‚Äôil existe des sessions tr√®s actives, mais ce sont des *cas exceptionnels*.\n",
    "\n",
    "- üéØ *Comportement utilisateur* :\n",
    "  - 75 % des sessions contiennent *5 √©v√©nements ou moins*, ce qui renforce l‚Äôid√©e que la plateforme est souvent utilis√©e de mani√®re ponctuelle.\n",
    "  - Ces statistiques peuvent √™tre utiles pour *segmenter les utilisateurs* selon leur engagement ou ajuster l‚Äôinterface utilisateur (UX).\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Pr√©traitement des donn√©es\n",
    "\n",
    "# T√¢ches de pr√©traitement r√©alis√©es\n",
    "\n",
    "La fonction preprocess_data effectue plusieurs op√©rations de pr√©traitement sur les donn√©es brutes. Voici les √©tapes principales :\n",
    "\n",
    "## 1. Extraction des caract√©ristiques temporelles et nettoyage des donn√©e\n",
    "\n",
    "√Ä partir de la colonne event_time, plusieurs nouvelles colonnes sont cr√©√©es pour enrichir les donn√©es temporelles :\n",
    "\n",
    "- hour : heure de l'√©v√©nement\n",
    "- minute : minute de l'√©v√©nement\n",
    "- second : seconde de l'√©v√©nement\n",
    "- day : jour du mois\n",
    "- month : mois\n",
    "- dayofweek : jour de la semaine\n",
    "- date : date au format yyyy-MM-dd\n",
    "- hour_bucket : regroupement horaire (date et heure arrondie √† l'heure, format yyyy-MM-dd HH:00:00)\n",
    "\n",
    "| event_time          | event_type | product_id | category_id          | category_code       | brand    | price  | user_id   | user_session                        | hour | minute | second | day | month | dayofweek | date       | hour_bucket           |\n",
    "|---------------------|------------|------------|---------------------|---------------------|----------|--------|-----------|-----------------------------------|------|--------|--------|-----|-------|-----------|------------|-----------------------|\n",
    "| 2019-10-01 00:00:00 | view       | 44600062   | 2103807459595387724 | unknown             | shiseido | 35.79  | 541312140 | 72d76fde-8bb3-4e0...              | 0    | 0      | 0      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |\n",
    "| 2019-10-01 00:00:00 | view       | 3900821    | 2053013552326770905 | appliances.enviro... | aqua     | 33.2   | 554748717 | 9333dfbd-b87a-470...              | 0    | 0      | 0      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |\n",
    "| 2019-10-01 00:00:01 | view       | 17200506   | 2053013559792632471 | furniture.living_... | unknown  | 543.1  | 519107250 | 566511c2-e2e3-422...              | 0    | 0      | 1      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |\n",
    "| 2019-10-01 00:00:01 | view       | 1307067    | 2053013558920217191 | computers.notebook   | lenovo   | 251.74 | 550050854 | 7c90fc70-0e80-459...              | 0    | 0      | 1      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |\n",
    "| 2019-10-01 00:00:04 | view       | 1004237    | 2053013555631882655 | electronics.smart... | apple    | 1081.98| 535871217 | c6bd7419-2748-4c5...              | 0    | 0      | 4      | 1   | 10    | 3         | 2019-10-01 | 2019-10-01 00:00:00   |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Traitement des valeurs manquantes\n",
    "\n",
    "Pour garantir la qualit√© des donn√©es, certains champs cat√©goriels sont nettoy√©s :\n",
    "\n",
    "- category_code :  \n",
    "  - Remplacement des valeurs null ou \"NaN\" par \"unknown\"\n",
    "- brand :  \n",
    "  - Remplacement des valeurs null par \"unknown\"\n",
    "\n",
    "\n",
    "\n",
    "## 3. Nettoyage des prix\n",
    "\n",
    "- La colonne price est corrig√©e en rempla√ßant les valeurs null ou inf√©rieures ou √©gales √† 0 par null (valeur manquante)\n",
    "# Entra√Ænement des Mod√®les d'IA avec Apache Spark\n",
    "\n",
    "## Vue d'ensemble\n",
    "\n",
    "Cette section pr√©sente l'impl√©mentation de l'entra√Ænement des mod√®les d'intelligence artificielle pour le syst√®me de recommandation e-commerce utilisant Apache Spark. Le processus comprend deux composants principaux : un mod√®le de segmentation client bas√© sur l'approche RFM (Recency, Frequency, Monetary) et un syst√®me de recommandation collaborative utilisant l'algorithme ALS (Alternating Least Squares).\n",
    "\n",
    "## Architecture du syst√®me d'entra√Ænement\n",
    "\n",
    "### Classe principale : EcommerceModelTrainer\n",
    "\n",
    "Le syst√®me d'entra√Ænement est encapsul√© dans la classe `EcommerceModelTrainer` qui orchestre l'ensemble du processus. Cette classe centralise la gestion de la session Spark, la pr√©paration des donn√©es et l'entra√Ænement des mod√®les.\n",
    "\n",
    "### Configuration Spark\n",
    "\n",
    "La m√©thode `_create_spark_session()` initialise la session Spark avec une configuration optimis√©e :\n",
    "\n",
    "```python\n",
    "SparkSession.builder \\\n",
    "    .appName(\"E-commerce Model Training\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .master(\"local[*]\")\n",
    "```\n",
    "\n",
    "Cette configuration alloue 8 Go de m√©moire aux processus driver et executor, active l'optimisation adaptative des requ√™tes SQL et utilise la s√©rialisation Kryo pour am√©liorer les performances.\n",
    "\n",
    "## Fonctions de pr√©paration des donn√©es\n",
    "\n",
    "### prepare_user_features()\n",
    "**But** : Calculer les caract√©ristiques comportementales et RFM pour chaque utilisateur\n",
    "\n",
    "Cette fonction transforme les donn√©es brutes d'√©v√©nements en m√©triques utilisateur exploitables :\n",
    "\n",
    "```python\n",
    "# Calcul de la date de r√©f√©rence (date maximale dans les donn√©es)\n",
    "max_date = cleaned_df.select(max(\"event_time\")).first()[0]\n",
    "\n",
    "# Calcul des m√©triques RFM et comportementales\n",
    "user_features = cleaned_df.groupBy(\"user_id\").agg(\n",
    "    # Recency: nombre de jours depuis la derni√®re activit√©\n",
    "    datediff(lit(max_date), max(\"event_time\")).alias(\"recency\"),\n",
    "    \n",
    "    # Frequency: nombre de jours distincts d'activit√©\n",
    "    countDistinct(col(\"event_time\").cast(\"date\")).alias(\"frequency\"),\n",
    "    \n",
    "    # Monetary: montant total des achats\n",
    "    sum(when(col(\"event_type\") == \"purchase\", col(\"price\")).otherwise(0)).alias(\"monetary\"),\n",
    "    \n",
    "    # M√©triques comportementales\n",
    "    count(\"*\").alias(\"total_events\"),\n",
    "    sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "    sum(when(col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"carts\"),\n",
    "    sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    ")\n",
    "\n",
    "# Calcul des taux de conversion\n",
    "user_features = user_features.withColumn(\n",
    "    \"conversion_rate\",\n",
    "    when(col(\"views\") > 0, col(\"purchases\") / col(\"views\")).otherwise(0)\n",
    ").withColumn(\n",
    "    \"cart_abandonment\",\n",
    "    when(col(\"carts\") > 0, (col(\"carts\") - col(\"purchases\")) / col(\"carts\")).otherwise(0)\n",
    ")\n",
    "```\n",
    "\n",
    "**Sortie** : DataFrame avec 15+ caract√©ristiques par utilisateur\n",
    "\n",
    "### prepare_product_features()  \n",
    "**But** : Agr√©ger les m√©triques de performance et popularit√© par produit\n",
    "\n",
    "Cette fonction analyse l'engagement des utilisateurs avec chaque produit :\n",
    "\n",
    "```python\n",
    "# Calcul des m√©triques agr√©g√©es par produit\n",
    "product_features = cleaned_df.groupBy(\"product_id\").agg(\n",
    "    count(\"*\").alias(\"total_events\"),\n",
    "    sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"total_views\"),\n",
    "    sum(when(col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"total_carts\"),\n",
    "    sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"total_purchases\"),\n",
    "    countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    "    first(\"category_code\").alias(\"category_code\"),\n",
    "    first(\"brand\").alias(\"brand\")\n",
    ")\n",
    "\n",
    "# Nettoyage et extraction de la cat√©gorie principale\n",
    "product_features = product_features.withColumn(\n",
    "    \"category\",\n",
    "    when(col(\"category_code\").isNull(), \"unknown\")\n",
    "    .otherwise(split(col(\"category_code\"), r\"\\\\.\")[0])\n",
    ").drop(\"category_code\")\n",
    "\n",
    "# Calcul du score de popularit√© am√©lior√©\n",
    "product_features = product_features.withColumn(\n",
    "    \"enhanced_popularity_score\",\n",
    "    (col(\"total_purchases\") * 3 + col(\"total_carts\") * 2 + col(\"total_views\")) / col(\"unique_users\")\n",
    ")\n",
    "```\n",
    "\n",
    "**Sortie** : DataFrame avec m√©triques d'engagement et m√©tadonn√©es par produit\n",
    "\n",
    "### prepare_recommendation_data()\n",
    "**But** : Pr√©parer et indexer les donn√©es pour l'algorithme ALS\n",
    "\n",
    "Cette fonction transforme les donn√©es pour le syst√®me de recommandation :\n",
    "\n",
    "```python\n",
    "# Indexation des utilisateurs et produits\n",
    "user_indexer = StringIndexer(\n",
    "    inputCol=\"user_id\",\n",
    "    outputCol=\"user_idx\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "product_indexer = StringIndexer(\n",
    "    inputCol=\"product_id\",\n",
    "    outputCol=\"product_idx\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Pipeline d'indexation\n",
    "indexer_pipeline = Pipeline(stages=[user_indexer, product_indexer])\n",
    "indexer_model = indexer_pipeline.fit(cleaned_df)\n",
    "indexed_df = indexer_model.transform(cleaned_df)\n",
    "\n",
    "# Pr√©paration des donn√©es d'interaction avec scores implicites\n",
    "interaction_data = indexed_df.filter(\n",
    "    col(\"event_type\").isin([\"view\", \"cart\", \"purchase\"])\n",
    ").withColumn(\n",
    "    \"rating\",\n",
    "    when(col(\"event_type\") == \"view\", 1.0)\n",
    "    .when(col(\"event_type\") == \"cart\", 3.0)\n",
    "    .when(col(\"event_type\") == \"purchase\", 5.0)\n",
    "    .otherwise(0.0)\n",
    ").groupBy(\"user_idx\", \"product_idx\").agg(\n",
    "    sum(\"rating\").alias(\"rating\"),\n",
    "    count(\"*\").alias(\"interaction_count\")\n",
    ").withColumn(\n",
    "    \"rating\",\n",
    "    # Normalisation du rating bas√©e sur le nombre d'interactions\n",
    "    when(col(\"rating\") > 10, 10.0).otherwise(col(\"rating\"))\n",
    ")\n",
    "```\n",
    "\n",
    "**Sortie** : Donn√©es d'interaction index√©es et pipeline de transformation\n",
    "\n",
    "## Mod√®le de segmentation client (K-Means)\n",
    "\n",
    "### Pr√©paration des caract√©ristiques utilisateur\n",
    "\n",
    "Le syst√®me calcule automatiquement les m√©triques RFM et comportementales pour chaque utilisateur :\n",
    "\n",
    "- **Recency** : Nombre de jours depuis la derni√®re activit√©\n",
    "- **Frequency** : Nombre de jours distincts d'activit√© \n",
    "- **Monetary** : Montant total des achats\n",
    "- **M√©triques comportementales** : Vues, ajouts au panier, achats, suppressions\n",
    "- **Diversit√©** : Nombre de produits, cat√©gories et marques uniques consult√©s\n",
    "- **Taux de conversion** : Ratio achats/vues\n",
    "- **Taux d'abandon panier** : Ratio (paniers - achats)/paniers\n",
    "\n",
    "### train_user_segmentation_model()\n",
    "**But** : Entra√Æner le mod√®le de clustering K-Means pour la segmentation client\n",
    "\n",
    "Cette fonction impl√©mente un pipeline complet de segmentation :\n",
    "\n",
    "```python\n",
    "# S√©lection des caract√©ristiques pour le clustering\n",
    "feature_cols = [\n",
    "    \"recency\", \"frequency\", \"monetary\", \"conversion_rate\",\n",
    "    \"cart_abandonment\", \"unique_products\", \"unique_categories\", \"avg_price\"\n",
    "]\n",
    "\n",
    "# Remplacement des valeurs nulles par 0\n",
    "for col_name in feature_cols:\n",
    "    user_features = user_features.withColumn(col_name,\n",
    "                                           when(col(col_name).isNull(), 0.0).otherwise(\n",
    "                                               col(col_name).cast(DoubleType())))\n",
    "\n",
    "# Assemblage des caract√©ristiques\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Normalisation des caract√©ristiques\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Mod√®le K-means\n",
    "kmeans = KMeans(\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"segment_id\",\n",
    "    k=5,  # 5 segments utilisateur\n",
    "    seed=42,\n",
    "    maxIter=20\n",
    ")\n",
    "\n",
    "# Pipeline de preprocessing et clustering\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "model = pipeline.fit(user_features)\n",
    "\n",
    "# √âvaluation du clustering\n",
    "evaluator = ClusteringEvaluator(\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"segment_id\",\n",
    "    metricName=\"silhouette\"\n",
    ")\n",
    "silhouette_score = evaluator.evaluate(segmented_users)\n",
    "```\n",
    "\n",
    "**Sortie** : Mod√®le K-Means entra√Æn√© et utilisateurs segment√©s avec noms descriptifs\n",
    "\n",
    "### _assign_segment_names()\n",
    "**But** : Attribuer automatiquement des noms m√©tier aux segments num√©riques\n",
    "\n",
    "Cette fonction utilitaire analyse les caract√©ristiques moyennes de chaque segment et applique une logique m√©tier :\n",
    "\n",
    "```python\n",
    "# Calcul des moyennes par segment\n",
    "segment_stats = segmented_users.groupBy(\"segment_id\").agg(\n",
    "    avg(\"recency\").alias(\"avg_recency\"),\n",
    "    avg(\"frequency\").alias(\"avg_frequency\"),\n",
    "    avg(\"monetary\").alias(\"avg_monetary\"),\n",
    "    avg(\"conversion_rate\").alias(\"avg_conversion\"),\n",
    "    count(\"*\").alias(\"segment_size\")\n",
    ").collect()\n",
    "\n",
    "# Logique de nommage bas√©e sur les caract√©ristiques RFM\n",
    "for row in segment_stats:\n",
    "    segment_id = row.segment_id\n",
    "    recency = row.avg_recency\n",
    "    frequency = row.avg_frequency\n",
    "    monetary = row.avg_monetary\n",
    "    conversion = row.avg_conversion\n",
    "\n",
    "    if monetary > 500 and frequency > 10 and recency < 30:\n",
    "        segment_name = \"VIP_Customers\"\n",
    "    elif frequency > 5 and recency < 60:\n",
    "        segment_name = \"Loyal_Customers\"\n",
    "    elif recency < 30 and conversion > 0.1:\n",
    "        segment_name = \"Active_Buyers\"\n",
    "    elif recency > 60 and frequency < 3:\n",
    "        segment_name = \"Dormant_Users\"\n",
    "    else:\n",
    "        segment_name = \"Casual_Browsers\"\n",
    "\n",
    "# Application du mapping\n",
    "mapping_expr = create_map([lit(x) for x in chain.from_iterable(segment_mapping.items())])\n",
    "segmented_users = segmented_users.withColumn(\n",
    "    \"segment\",\n",
    "    mapping_expr[col(\"segment_id\")]\n",
    ")\n",
    "```\n",
    "\n",
    "- **VIP_Customers** : Monetary > 500‚Ç¨, Frequency > 10 jours, Recency < 30 jours\n",
    "- **Loyal_Customers** : Frequency > 5 jours, Recency < 60 jours  \n",
    "- **Active_Buyers** : Recency < 30 jours, Conversion > 10%\n",
    "- **Dormant_Users** : Recency > 60 jours, Frequency < 3 jours\n",
    "- **Casual_Browsers** : Autres profils\n",
    "\n",
    "### √âvaluation du clustering\n",
    "\n",
    "La qualit√© du clustering est mesur√©e par le score de silhouette, qui √©value la coh√©sion intra-cluster et la s√©paration inter-cluster.\n",
    "\n",
    "## Syst√®me de recommandation collaborative (ALS)\n",
    "\n",
    "### Pr√©paration des donn√©es d'interaction\n",
    "\n",
    "Le syst√®me transforme les √©v√©nements utilisateur en scores d'interaction implicites :\n",
    "\n",
    "- **Vues** : Score de 1.0\n",
    "- **Ajouts au panier** : Score de 3.0  \n",
    "- **Achats** : Score de 5.0\n",
    "\n",
    "Les interactions multiples sont agr√©g√©es et normalis√©es avec un plafond √† 10.0 pour √©viter les biais.\n",
    "\n",
    "### Indexation des entit√©s\n",
    "\n",
    "Un pipeline d'indexation convertit les identifiants string en indices num√©riques requis par ALS :\n",
    "\n",
    "```python\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_idx\")\n",
    "product_indexer = StringIndexer(inputCol=\"product_id\", outputCol=\"product_idx\")\n",
    "```\n",
    "\n",
    "### train_als_model()\n",
    "**But** : Entra√Æner le mod√®le de recommandation collaborative ALS\n",
    "\n",
    "Cette fonction impl√©mente l'algorithme de factorisation matricielle :\n",
    "\n",
    "```python\n",
    "# Division train/test\n",
    "train_data, test_data = interaction_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Configuration du mod√®le ALS\n",
    "als = ALS(\n",
    "    userCol=\"user_idx\",\n",
    "    itemCol=\"product_idx\",\n",
    "    ratingCol=\"rating\",\n",
    "    nonnegative=True,\n",
    "    implicitPrefs=True,  # Donn√©es implicites\n",
    "    rank=50,  # Nombre de facteurs latents\n",
    "    maxIter=15,  # Nombre d'it√©rations\n",
    "    regParam=0.1,  # R√©gularisation\n",
    "    alpha=1.0,  # Param√®tre de confiance pour les donn√©es implicites\n",
    "    coldStartStrategy=\"drop\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Entra√Ænement\n",
    "als_model = als.fit(train_data)\n",
    "\n",
    "# √âvaluation sur les donn√©es de test\n",
    "predictions = als_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions.filter(col(\"prediction\").isNotNull()))\n",
    "\n",
    "# G√©n√©ration de recommandations pour tous les utilisateurs (cache)\n",
    "all_users = interaction_data.select(\"user_idx\").distinct()\n",
    "user_recommendations = als_model.recommendForUserSubset(all_users, 10)\n",
    "```\n",
    "\n",
    "**Sortie** : Mod√®le ALS entra√Æn√© et recommandations pr√©-calcul√©es\n",
    "\n",
    "## Fonctions utilitaires et de gestion\n",
    "\n",
    "### _find_latest_parquet_file()\n",
    "**But** : Identifier automatiquement le fichier de donn√©es le plus r√©cent\n",
    "\n",
    "Cette fonction utilitaire parcourt un r√©pertoire pour trouver le fichier avec le timestamp le plus r√©cent :\n",
    "\n",
    "```python\n",
    "def _find_latest_parquet_file(self, directory_path, prefix):\n",
    "    full_directory_path = os.path.abspath(directory_path).replace(\"\\\\\", \"/\")\n",
    "    latest_file = None\n",
    "    latest_timestamp = None\n",
    "    \n",
    "    try:\n",
    "        entries = os.listdir(full_directory_path)\n",
    "        # Filtrer les entr√©es qui sont des r√©pertoires et qui commencent par le pr√©fixe\n",
    "        matching_entries = [entry for entry in entries \n",
    "                          if os.path.isdir(os.path.join(full_directory_path, entry)) \n",
    "                          and entry.startswith(prefix)]\n",
    "\n",
    "        for entry_name in matching_entries:\n",
    "            # Extraire le timestamp du nom du r√©pertoire (format YYYYMMDD_HHMMSS)\n",
    "            timestamp_match = re.search(r'\\d{8}_\\d{6}', entry_name)\n",
    "            if timestamp_match:\n",
    "                timestamp_str = timestamp_match.group(0)\n",
    "                timestamp = datetime.strptime(timestamp_str, \"%Y%m%d_%H%M%S\")\n",
    "                if latest_timestamp is None or timestamp > latest_timestamp:\n",
    "                    latest_timestamp = timestamp\n",
    "                    latest_file = os.path.join(full_directory_path, entry_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la recherche du fichier {prefix}: {e}\")\n",
    "    \n",
    "    return latest_file\n",
    "```\n",
    "\n",
    "**Utilit√©** : Permet la reprise automatique d'entra√Ænement sur les donn√©es les plus fra√Æches\n",
    "\n",
    "### train_all_models()\n",
    "**But** : Orchestrer l'ensemble du processus d'entra√Ænement\n",
    "\n",
    "Cette fonction ma√Ætresse coordonne tout le pipeline d'entra√Ænement :\n",
    "\n",
    "```python\n",
    "def train_all_models(self, cleaned_data_path, processed_data_path=\"./data/processed/parquet/\"):\n",
    "    logger.info(\"D√©but de l'entra√Ænement de tous les mod√®les...\")\n",
    "\n",
    "    # Chargement des donn√©es nettoy√©es\n",
    "    cleaned_df = self.spark.read.parquet(cleaned_data_path)\n",
    "    \n",
    "    # 1. Chargement intelligent des caract√©ristiques utilisateur\n",
    "    user_behavior_path = self._find_latest_parquet_file(processed_data_path, \"user_behavior_\")\n",
    "    if user_behavior_path:\n",
    "        user_features = self.spark.read.parquet(user_behavior_path)\n",
    "    else:\n",
    "        # Fallback vers la pr√©paration si le fichier est manquant\n",
    "        user_features = self.prepare_user_features(cleaned_df)\n",
    "\n",
    "    # 2. Entra√Ænement du mod√®le de segmentation\n",
    "    segmentation_model, segmented_users = self.train_user_segmentation_model(user_features)\n",
    "\n",
    "    # 3. Pr√©paration des donn√©es pour recommandation (Indexation)\n",
    "    interaction_data, indexer_model = self.prepare_recommendation_data(cleaned_df)\n",
    "\n",
    "    # 4. Chargement des caract√©ristiques produits avec fallback\n",
    "    product_features_path = self._find_latest_parquet_file(processed_data_path, \"product_data_\")\n",
    "    if product_features_path:\n",
    "        product_features = self.spark.read.parquet(product_features_path)\n",
    "    else:\n",
    "        product_features = self.prepare_product_features(cleaned_df)\n",
    "\n",
    "    # 5. Entra√Ænement du mod√®le ALS\n",
    "    als_model = self.train_als_model(interaction_data)\n",
    "\n",
    "    # 6. Sauvegarde des m√©tadonn√©es\n",
    "    metadata = {\n",
    "        \"training_date\": datetime.now().isoformat(),\n",
    "        \"total_users\": cleaned_df.select(\"user_id\").distinct().count(),\n",
    "        \"total_products\": cleaned_df.select(\"product_id\").distinct().count(),\n",
    "        \"total_interactions\": cleaned_df.count(),\n",
    "        \"segments_count\": segmented_users.select(\"segment_id\").distinct().count()\n",
    "    }\n",
    "```\n",
    "\n",
    "**Flux d'ex√©cution** :\n",
    "```\n",
    "Donn√©es nettoy√©es ‚Üí Caract√©ristiques utilisateur ‚Üí Segmentation K-Means\n",
    "                 ‚Üì\n",
    "Indexation ‚Üí Donn√©es d'interaction ‚Üí Mod√®le ALS ‚Üí Recommandations\n",
    "                 ‚Üì\n",
    "Sauvegarde des mod√®les et m√©tadonn√©es\n",
    "```\n",
    "\n",
    "**Sortie** : Dictionnaire contenant tous les mod√®les entra√Æn√©s et leurs m√©tadonn√©es\n",
    "\n",
    "## Fonction principale et point d'entr√©e\n",
    "\n",
    "### main()\n",
    "**But** : Point d'entr√©e principal pour l'ex√©cution via spark-submit\n",
    "\n",
    "Cette fonction configure l'environnement d'ex√©cution et lance le processus complet :\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    # Configuration des chemins\n",
    "    CLEANED_DATA_PATH = \"./data/processed/parquet/cleaned_data_*.parquet\" \n",
    "    MODELS_OUTPUT_PATH = \"./models\"\n",
    "    PROCESSED_DATA_PATH = \"./data/processed/parquet/\" \n",
    "    \n",
    "    # Cr√©ation du trainer\n",
    "    trainer = EcommerceModelTrainer(models_output_path=MODELS_OUTPUT_PATH)\n",
    "\n",
    "    try:\n",
    "        # Entra√Ænement de tous les mod√®les\n",
    "        models = trainer.train_all_models(CLEANED_DATA_PATH, PROCESSED_DATA_PATH)\n",
    "        logger.info(\"Tous les mod√®les ont √©t√© entra√Æn√©s et sauvegard√©s avec succ√®s!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'entra√Ænement des mod√®les: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        trainer.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "**Usage** : `spark-submit model_training.py` pour lancement en production\n",
    "\n",
    "### stop()\n",
    "**But** : Fermer proprement la session Spark et lib√©rer les ressources\n",
    "\n",
    "```python\n",
    "def stop(self):\n",
    "    \"\"\"Arr√™te la session Spark\"\"\"\n",
    "    self.spark.stop()\n",
    "```\n",
    "\n",
    "Cette m√©thode garantit la lib√©ration correcte des ressources Spark, √©vitant les fuites m√©moire et les processus orphelins.\n",
    "\n",
    "## Gestion de la persistance et m√©tadonn√©es\n",
    "\n",
    "### Sauvegarde des mod√®les\n",
    "\n",
    "Tous les mod√®les entra√Æn√©s sont sauvegard√©s de mani√®re persistante :\n",
    "\n",
    "- **Mod√®le K-Means** : Pipeline complet incluant le pr√©processing\n",
    "- **Mod√®le ALS** : Mod√®le de factorisation matricielle\n",
    "- **Pipeline d'indexation** : Mappings utilisateur/produit vers indices\n",
    "- **Segments utilisateur** : Assignations de segments avec m√©triques RFM\n",
    "- **Recommandations pr√©-calcul√©es** : Top 10 recommandations par utilisateur\n",
    "\n",
    "### M√©tadonn√©es d'entra√Ænement\n",
    "\n",
    "Le syst√®me sauvegarde automatiquement les m√©tadonn√©es d'entra√Ænement :\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"training_date\": \"2024-12-XX\",\n",
    "  \"total_users\": 123456,\n",
    "  \"total_products\": 7890,\n",
    "  \"total_interactions\": 9876543,\n",
    "  \"segments_count\": 5\n",
    "}\n",
    "```\n",
    "\n",
    "## Optimisations et robustesse\n",
    "\n",
    "### Gestion des donn√©es manquantes\n",
    "\n",
    "Le syst√®me impl√©mente plusieurs strat√©gies de robustesse :\n",
    "\n",
    "- Remplacement des valeurs nulles par 0.0 dans les caract√©ristiques num√©riques\n",
    "- Gestion des cat√©gories manquantes avec une valeur \"unknown\"\n",
    "- Strat√©gie \"handleInvalid=keep\" pour les nouveaux identifiants\n",
    "\n",
    "### Chargement intelligent des donn√©es\n",
    "\n",
    "La fonction `_find_latest_parquet_file()` identifie automatiquement les fichiers de donn√©es les plus r√©cents bas√©s sur des timestamps, permettant une reprise d'entra√Ænement sur les donn√©es les plus √† jour.\n",
    "\n",
    "### Fallback automatique\n",
    "\n",
    "En cas d'√©chec du chargement des donn√©es pr√©trait√©es, le syst√®me bascule automatiquement vers un calcul √† partir des donn√©es nettoy√©es de base, assurant la continuit√© du processus d'entra√Ænement.\n",
    "\n",
    "## M√©triques de performance\n",
    "\n",
    "### Clustering\n",
    "- **Score de silhouette** : Mesure la qualit√© de la s√©paration des segments\n",
    "- **Distribution des segments** : Taille et caract√©ristiques moyennes par segment\n",
    "\n",
    "### Recommandation\n",
    "- **RMSE** : Erreur quadratique moyenne sur l'ensemble de test\n",
    "- **Couverture** : Pourcentage de produits recommand√©s\n",
    "- **Temps d'entra√Ænement** : Performance du processus d'apprentissage\n",
    "\n",
    "## Architecture de d√©ploiement\n",
    "\n",
    "Le syst√®me d'entra√Ænement est con√ßu pour √™tre ex√©cut√© via `spark-submit`, permettant une scalabilit√© horizontale sur un cluster Spark en production. La structure modulaire facilite la maintenance et l'extension avec de nouveaux algorithmes.\n",
    "# Syst√®me de Recommandation en Temps R√©el avec Spark Streaming\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Cette section pr√©sente l'impl√©mentation d'un syst√®me de recommandation capable de traiter des donn√©es clients en temps r√©el et de g√©n√©rer des recommandations instantan√©es. Le syst√®me utilise Apache Spark Structured Streaming pour consommer un flux de donn√©es d'interactions utilisateur-produit et applique un algorithme de recommandation hybride combinant filtrage collaboratif et popularit√©.\n",
    "\n",
    "## Architecture du Streaming\n",
    "\n",
    "### Configuration Spark\n",
    "```python\n",
    "def _create_spark_session(self):\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Streaming Recommendation System\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.sql.streaming.checkpointLocation\", \"./checkpoint\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "```\n",
    "\n",
    "La session Spark est configur√©e avec :\n",
    "- **Checkpointing** : Pour la tol√©rance aux pannes et la reprise apr√®s incident\n",
    "- **M√©moire optimis√©e** : 8GB allou√©s au driver et aux executors pour traiter les donn√©es en m√©moire\n",
    "- **Mode local** : Utilisation de tous les c≈ìurs disponibles\n",
    "\n",
    "### Lecture du Flux de Donn√©es\n",
    "\n",
    "```python\n",
    "stream_df = self.spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(input_path)\n",
    "```\n",
    "\n",
    "**Points cl√©s :**\n",
    "- **Sch√©ma d√©fini** : √âvite l'inf√©rence automatique pour de meilleures performances\n",
    "- **maxFilesPerTrigger** : Contr√¥le le d√©bit de traitement (1 fichier par batch)\n",
    "- **Format CSV** : Simulation de donn√©es temps r√©el √† partir de fichiers\n",
    "\n",
    "### Fen√™trage Temporel\n",
    "\n",
    "```python\n",
    "windowed_activity = stream_df \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\"),\n",
    "        \"user_id\", \"product_id\", \"event_type\", \"category_code\", \"brand\", \"price\"\n",
    "    ).agg(count(\"*\").alias(\"event_count\"))\n",
    "```\n",
    "\n",
    "**M√©canisme de fen√™trage :**\n",
    "- **Watermark** : Tol√®re un retard maximum de 10 minutes pour les √©v√©nements\n",
    "- **Fen√™tre glissante** : Agr√©gation sur des intervalles de 5 minutes\n",
    "- **Agr√©gation** : Comptage des interactions par utilisateur/produit\n",
    "\n",
    "## Algorithme de Recommandation\n",
    "\n",
    "### Analyse des Interactions Utilisateur\n",
    "\n",
    "```python\n",
    "user_interactions = user_activity_df_processed.groupBy(\"user_id\", \"product_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"interaction_count\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"carts\"),\n",
    "        spark_sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\")\n",
    "    )\n",
    "```\n",
    "\n",
    "**Agr√©gation des comportements :**\n",
    "- Comptage total des interactions\n",
    "- Distinction par type d'√©v√©nement (vue, ajout panier, achat)\n",
    "- Base pour le calcul des pr√©f√©rences utilisateur\n",
    "\n",
    "### Extraction des Pr√©f√©rences Cat√©gorielles\n",
    "\n",
    "```python\n",
    "user_categories = user_activity_df_processed.filter(col(\"category_code\").isNotNull()) \\\n",
    "    .groupBy(\"user_id\", \"main_category\") \\\n",
    "    .agg(count(\"*\").alias(\"category_interest\"))\n",
    "\n",
    "top_categories = user_categories.withColumn(\n",
    "    \"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(desc(\"category_interest\")))\n",
    ").filter(col(\"rank\") <= 3)\n",
    "```\n",
    "\n",
    "**Identification des centres d'int√©r√™t :**\n",
    "- Extraction de la cat√©gorie principale depuis le code complet\n",
    "- Classement des cat√©gories par fr√©quence d'interaction\n",
    "- S√©lection des 3 cat√©gories les plus populaires par utilisateur\n",
    "\n",
    "### G√©n√©ration des Recommandations\n",
    "\n",
    "```python\n",
    "recommendations = top_categories.join(\n",
    "    broadcast(self.product_popularity.filter(col(\"total_purchases\") > 0)),\n",
    "    on=top_categories.main_category == self.product_popularity.main_category_code, \n",
    "    how=\"inner\"\n",
    ").withColumn(\n",
    "    \"rec_score\",\n",
    "    col(\"category_interest\") * col(\"total_purchases\") / 100\n",
    ").withColumn(\n",
    "    \"rank\", row_number().over(Window.partitionBy(\"user_id\").orderBy(desc(\"rec_score\")))\n",
    ")\n",
    "```\n",
    "\n",
    "**Algorithme hybride :**\n",
    "- **Filtrage collaboratif** : Bas√© sur les pr√©f√©rences cat√©gorielles de l'utilisateur\n",
    "- **Popularit√© globale** : Pond√©ration par le nombre total d'achats du produit\n",
    "- **Score de recommandation** : Formule combinant int√©r√™t personnel et popularit√©\n",
    "- **Broadcast join** : Optimisation pour les donn√©es de r√©f√©rence (produits populaires)\n",
    "\n",
    "### M√©canisme de Fallback\n",
    "\n",
    "```python\n",
    "if recommendations.isEmpty():\n",
    "    logger.warning(\"Collaborative recommendations are empty. Falling back to popular products.\")\n",
    "    unique_users_df = user_activity_df_processed.select(\"user_id\").distinct()\n",
    "    top_popular_products = self.product_popularity\n",
    "    \n",
    "    fallback_recommendations = unique_users_df.crossJoin(top_popular_products.limit(10))\n",
    "    recommendations = fallback_recommendations.select(\n",
    "        col(unique_users_df.user_id).alias(\"user_id\"),\n",
    "        col(top_popular_products.product_id).alias(\"product_id\"),\n",
    "        lit(0.1).alias(\"rec_score\"),\n",
    "        col(top_popular_products.main_category_code).alias(\"recommended_category_code\"),\n",
    "        col(top_popular_products.main_brand).alias(\"main_brand\"),\n",
    "        col(top_popular_products.avg_price).alias(\"avg_price\")\n",
    "    ).withColumn(\"recommendation_type\", lit(\"streaming_popular_fallback\"))\n",
    "```\n",
    "\n",
    "**Strat√©gie de repli :**\n",
    "- **D√©tection automatique** : V√©rification si les recommandations collaboratives sont vides\n",
    "- **Recommandations populaires** : Suggestion des 10 produits les plus vendus\n",
    "- **Cross-join** : Attribution des m√™mes produits populaires √† tous les nouveaux utilisateurs\n",
    "\n",
    "## Persistance et Monitoring\n",
    "\n",
    "### √âcriture vers InfluxDB\n",
    "\n",
    "```python\n",
    "def _write_to_influxdb(self, df, measurement_name):\n",
    "    def write_batch(batch_df, batch_id):\n",
    "        try:\n",
    "            client = InfluxDBClient(url=INFLUXDB_CONFIG[\"url\"], \n",
    "                                  token=INFLUXDB_CONFIG[\"token\"])\n",
    "            write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "            \n",
    "            points = []\n",
    "            for row in batch_df.collect():\n",
    "                point = Point(measurement_name)\n",
    "                # Ajout des champs et tags selon le type de donn√©es\n",
    "                points.append(point)\n",
    "            \n",
    "            write_api.write(bucket=INFLUXDB_CONFIG[\"bucket\"], record=points)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing to InfluxDB: {e}\")\n",
    "```\n",
    "\n",
    "**Monitoring temps r√©el :**\n",
    "- **M√©triques d'√©v√©nements** : Suivi des interactions utilisateur\n",
    "- **M√©triques de recommandations** : Performance et qualit√© des suggestions\n",
    "- **Base de donn√©es temporelle** : InfluxDB pour l'analyse des tendances\n",
    "\n",
    "### Sauvegarde des R√©sultats\n",
    "\n",
    "```python\n",
    "# Sauvegarde incr√©mentale des recommandations\n",
    "recommendations.write.mode(\"append\").option(\"header\", \"true\").csv(output_csv_path)\n",
    "```\n",
    "\n",
    "**Persistance des recommandations :**\n",
    "- **Mode append** : Accumulation des r√©sultats de chaque batch\n",
    "- **Format CSV** : Facilite l'analyse post-traitement\n",
    "- **Horodatage implicite** : Tra√ßabilit√© temporelle des recommandations\n",
    "\n",
    "## Configuration du Pipeline\n",
    "\n",
    "### Traitement par Micro-Batch\n",
    "\n",
    "```python\n",
    "query = windowed_activity.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .trigger(processingTime='60 seconds') \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Param√®tres de streaming :**\n",
    "- **Mode update** : Seules les lignes modifi√©es sont transmises\n",
    "- **Trigger de 60 secondes** : √âquilibre entre latence et throughput\n",
    "- **ForeachBatch** : Traitement personnalis√© de chaque micro-batch\n",
    "\n",
    "### Gestion d'Erreurs et Tol√©rance aux Pannes\n",
    "\n",
    "```python\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Stopping streaming...\")\n",
    "    query.stop()\n",
    "    self.spark.stop()\n",
    "```\n",
    "\n",
    "**Robustesse du syst√®me :**\n",
    "- **Arr√™t gracieux** : Gestion des interruptions manuelles\n",
    "- **Logging d√©taill√©** : Tra√ßabilit√© des erreurs et performances\n",
    "- **Checkpointing automatique** : Reprise apr√®s panne\n",
    "\n",
    "## R√©sultats et Performance\n",
    "\n",
    "Le syst√®me traite les donn√©es en temps r√©el avec les caract√©ristiques suivantes :\n",
    "\n",
    "- **Latence** : Recommandations g√©n√©r√©es en moins de 60 secondes apr√®s r√©ception des donn√©es\n",
    "- **D√©bit** : Capable de traiter des milliers d'interactions par batch\n",
    "- **Scalabilit√©** : Architecture distribu√©e pr√™te pour le passage √† l'√©chelle\n",
    "- **Qualit√©** : Algorithme hybride combinant personnalisation et popularit√©\n",
    "\n",
    "Cette impl√©mentation d√©montre la capacit√© de Spark Structured Streaming √† alimenter des syst√®mes de recommandation temps r√©el, essentiels pour les applications e-commerce modernes n√©cessitant une personnalisation instantan√©e.\n",
    "# Visualisation des Recommandations √† Froid\n",
    "\n",
    "Cette section pr√©sente l'analyse et la visualisation des recommandations g√©n√©r√©es par notre syst√®me de recommandation Spark. Les recommandations produites sont stock√©es dans deux formats compl√©mentaires pour faciliter l'analyse et le monitoring.\n",
    "\n",
    "## Architecture de Stockage des Recommandations\n",
    "\n",
    "Les recommandations g√©n√©r√©es par le mod√®le Spark sont distribu√©es via deux canaux :\n",
    "\n",
    "- **Export CSV** : Sauvegarde dans un fichier `merged_recommendations.csv` pour analyse batch\n",
    "- **Stockage InfluxDB** : Envoi en temps r√©el pour monitoring et analyse temporelle\n",
    "\n",
    "### Structure de la Table InfluxDB\n",
    "\n",
    "Les recommandations sont stock√©es dans InfluxDB avec la structure suivante :\n",
    "![Capture d'√©cran 2025-05-26 182734.png](images%2FCapture%20d%27%C3%A9cran%202025-05-26%20182734.png)\n",
    "![Capture d'√©cran 2025-05-26 195331.png](images%2FCapture%20d%27%C3%A9cran%202025-05-26%20195331.png)\n",
    "\n",
    "*Capture d'√©cran √† ins√©rer : Structure de la table des recommandations dans InfluxDB*\n",
    "\n",
    "Cette base de donn√©es temporelle permet un suivi en temps r√©el des performances du syst√®me de recommandation et facilite la cr√©ation de dashboards de monitoring.\n",
    "\n",
    "## Analyse des Donn√©es de Recommandation\n",
    "\n",
    "Le script `visualize_recommendations.py` charge et analyse les donn√©es depuis le fichier CSV merg√©. Les donn√©es analys√©es sont identiques √† celles stock√©es dans InfluxDB, garantissant la coh√©rence entre les analyses batch et temps r√©el.\n",
    "\n",
    "### Chargement et Traitement des Donn√©es\n",
    "\n",
    "```python\n",
    "# Chargement du fichier CSV consolid√©\n",
    "df = pd.read_csv(\"./results/merged_recommendations.csv\")\n",
    "\n",
    "# G√©n√©ration automatique de 8 visualisations principales\n",
    "# - Distribution des scores\n",
    "# - Top cat√©gories et marques\n",
    "# - Relations prix/score\n",
    "# - Analyse par utilisateur\n",
    "```\n",
    "\n",
    "## R√©sultats des Visualisations\n",
    "\n",
    "### Distribution des Scores de Recommandation\n",
    "![aed12cf4-5991-415d-9a32-867d28ad3301.png](images%2Faed12cf4-5991-415d-9a32-867d28ad3301.png)\n",
    "\n",
    "La distribution des scores montre une concentration importante des recommandations autour de scores faibles (0-100), avec quelques pics isol√©s √† des scores plus √©lev√©s. Cette distribution sugg√®re que la majorit√© des recommandations sont de qualit√© standard, avec quelques recommandations exceptionnelles.\n",
    "\n",
    "### Top 10 Cat√©gories Recommand√©es\n",
    "![a94eb5fa-7842-40d8-85b0-b79360b4ea29.png](images%2Fa94eb5fa-7842-40d8-85b0-b79360b4ea29.png)\n",
    "L'√©lectronique domine largement avec plus de 400K recommandations, suivie par les appareils √©lectrom√©nagers (~90K). Cette r√©partition refl√®te probablement les habitudes d'achat des utilisateurs et la disponibilit√© des produits dans ces cat√©gories.\n",
    "\n",
    "### Top 10 Marques Recommand√©es\n",
    "\n",
    "![8db3703b-6230-415c-a389-1a9c07a1798f.png](images%2F8db3703b-6230-415c-a389-1a9c07a1798f.png)\n",
    "\n",
    "Apple et Samsung se positionnent comme les marques les plus recommand√©es, avec respectivement ~240K et ~200K recommandations. Cette dominance s'explique par leur pr√©sence forte dans les cat√©gories √©lectroniques populaires.\n",
    "\n",
    "### Relation Score vs Prix Moyen\n",
    "![980174d7-e8fa-413f-8903-020be2726a8b.png](images%2F980174d7-e8fa-413f-8903-020be2726a8b.png)\n",
    "\n",
    "Le graphique de dispersion r√©v√®le une absence de corr√©lation claire entre le prix et le score de recommandation. Les produits √† tous niveaux de prix peuvent obtenir des scores √©lev√©s, sugg√©rant que l'algorithme privil√©gie la pertinence utilisateur plut√¥t que la valeur mon√©taire.\n",
    "\n",
    "### Performance par Cat√©gorie et Marque\n",
    "\n",
    "![f94a8e11-de91-4fbc-ab0e-b0a49bff4448.png](images%2Ff94a8e11-de91-4fbc-ab0e-b0a49bff4448.png)\n",
    "\n",
    "L'√©lectronique obtient le score moyen le plus √©lev√© (~15), confirmant la qualit√© des recommandations dans cette cat√©gorie dominante.\n",
    "\n",
    "![aa35d47d-6b94-49e3-8caa-308e4dca96f4.png](images%2Faa35d47d-6b94-49e3-8caa-308e4dca96f4.png)\n",
    "\n",
    "Samsung pr√©sente le score moyen le plus √©lev√© (~18), suivi d'Apple (~10), sugg√©rant une forte ad√©quation entre ces marques et les pr√©f√©rences utilisateurs.\n",
    "\n",
    "### Distribution des Prix\n",
    "![26d3b002-d87b-4e3d-b9f2-ed718171487b.png](images%2F26d3b002-d87b-4e3d-b9f2-ed718171487b.png)\n",
    "La distribution des prix montre plusieurs pics distincts, sugg√©rant des gammes de prix privil√©gi√©es par les utilisateurs. La pr√©sence de pics autour de 150‚Ç¨, 250‚Ç¨ et 450‚Ç¨ indique des segments de march√© bien d√©finis.\n",
    "\n",
    "## Synth√®se\n",
    "\n",
    "Les visualisations r√©v√®lent un syst√®me de recommandation √©quilibr√© avec :\n",
    "- Une concentration sur l'√©lectronique et les grandes marques\n",
    "- Des scores de recommandation ind√©pendants du prix\n",
    "- Une distribution √©quitable entre utilisateurs\n",
    "- Des segments de prix clairement identifi√©s\n",
    "\n",
    "Ces analyses, disponibles en temps r√©el via InfluxDB et en batch via les exports CSV, permettent un monitoring continu de la qualit√© des recommandations."
   ],
   "id": "19f29830b474884f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87d8add78441b766"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
